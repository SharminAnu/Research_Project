{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/data2/users/ssultana/dataset/N3C_benchmarking/N3C_full_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[1, '1.1'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>78473, 80767, 764599, 4062484, 4149382, 4169954, 4203722, 4223659, 4251903, 4306780+31.0+0+1+0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0</th>\n",
       "      <th>23325, 25844, 30437, 77670, 197381, 198263, 200219, 201061, 315078, 437833, 442077, 4000609, 4209423, 4306292, 40481385, 40482267+32.0+0+1+808+38003564.0+8532.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+1.0+0.0</th>\n",
       "      <th>195083, 315078, 4152351+38.0+0+1+0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0</th>\n",
       "      <th>201826, 254761, 257012, 261687, 320128, 438720, 4166902, 4273307, 37016349+64.0+0+1+0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0</th>\n",
       "      <th>137989, 138717, 255573, 257004, 312437, 313217, 315282, 320128, 433316, 764123, 4154290, 4322024, 42538119+65.0+0+0+601+38003564.0+8532.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0</th>\n",
       "      <th>24134, 372324, 378253, 762296, 4038038, 4166125, 4273307, 4288310+71.0+0+0+460+38003564.0+8532.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0</th>\n",
       "      <th>435950, 436826, 437116, 4115367, 4164959, 40305987+60.0+0+1+801+38003563.0+8532.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0</th>\n",
       "      <th>0, 132412, 132797, 134116, 138384, 139902, 140673, 194133, 197320, 318800, 320128, 377844, 432250, 432791, 433736, 436096, 437191, 437474, 437663, 438485, 439221, 439224, 440276, 440615, 441267, 441488, 443272, 4060985, 4064036, 4117957, 4144111, 4156229, 4169580, 4174262, 4186463, 4214956, 4226022, 4237320, 4240903, 4241527, 4305831, 4314870, 4322175, 4345578, 36712821, 36712860, 37016200, 43021283, 46272450+50.0+0+1+652+38003564.0+8532.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0</th>\n",
       "      <th>75036, 77074, 77965, 78508, 79908, 81175, 81878, 192963, 198678, 198803, 255573, 256451, 257011, 313217, 317002, 320128, 372448, 432867, 436230, 438720, 439297, 439777, 440029, 764123, 4057826, 4110914, 4144111, 4153380, 4154290, 4223659, 4261842, 4330445, 40479625, 40481919, 42537730, 46271022+88.0+0+1+820+38003564.0+8507.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0</th>\n",
       "      <th>27674, 138825, 194133, 196360, 198401, 200174, 200843, 201337, 201620, 201826, 254761, 260123, 320128, 374367, 374375, 377889, 379805, 432867, 436070, 437246, 438699, 439777, 442019, 443211, 443597, 444131, 762289, 4031106, 4032787, 4041285, 4079750, 4084229, 4092743, 4110815, 4115991, 4155909, 4170554, 4171917, 4185946, 4193704, 4223659, 4236484, 4271013, 43531578, 45757656+86.0+0+0+212+38003564.0+8507.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0</th>\n",
       "      <th>...</th>\n",
       "      <th>320128, 320536, 4223659, 4273307+57.0+0+1+460+38003564.0+8507.0+8522.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0</th>\n",
       "      <th>195867, 434005, 436230, 4092538, 4193704, 4282096, 40481920+29.0+0+1+0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0</th>\n",
       "      <th>81239, 133810, 192450, 193782, 194686, 200174, 201826, 254761, 255848, 257004, 312437, 316139, 319835, 320136, 320536, 376065, 432867, 432870, 435515, 436230, 437113, 437663, 439696, 439777, 442019, 442077, 442111, 443729, 443919, 444101, 444459, 764123, 4007310, 4093531, 4162994, 4180628, 4187218, 4209423, 4216644, 4273307, 4310235, 4320635, 37016114, 37017432, 37309626, 40479576, 40481043, 40483287, 42537748, 43530690, 43531578, 44782429, 44784621, 45768812+60.0+0+0+474+38003564.0+8507.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0</th>\n",
       "      <th>77234, 78232, 135618, 198803, 255848, 374009, 377091, 435783, 435796, 436070, 436096, 437113, 437663, 439777, 442077, 443782, 444070, 4133224, 4144111, 4170554, 4224940, 4254485, 4273307, 4286201, 4336254+63.0+0+1+462+38003564.0+8507.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0</th>\n",
       "      <th>27674, 31967, 81902, 139900, 141371, 194081, 197684, 254761, 314754, 378253, 432545, 438028, 439235, 440383, 442077, 4002818, 4041283, 4077577, 4092411, 4152376, 4177206, 4196024, 4224968, 4282096, 4300092+23.0+0+1+522+38003564.0+8532.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0</th>\n",
       "      <th>254761, 37311061+33.0+0+1+0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0</th>\n",
       "      <th>28060, 31967, 79864, 81902, 135473, 197381, 197684, 201340, 201603, 256451, 257012, 433753, 434613, 437663, 440383, 442077, 4025325, 4041283, 4214376, 4280498+40.0+0+1+240+38003564.0+8532.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+1.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0</th>\n",
       "      <th>81902, 442077, 4081648+25.0+0+1+372+38003564.0+8532.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+1.0+0.0</th>\n",
       "      <th>135340, 200485+57.0+1+0+0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0</th>\n",
       "      <th>31967, 133228, 137977, 374375, 433316, 436230, 439658, 443800, 4062790, 4152351, 4170137+26.0+0+1+0+38003563.0+8532.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140214, 254761, 257011, 377889, 4141481+6.0+0+...</td>\n",
       "      <td>78473, 137548, 4236484, 37311338, 37311341+41....</td>\n",
       "      <td>195867, 197684, 201078, 4031128, 4289526, 3701...</td>\n",
       "      <td>133834, 140214, 438134, 438398, 442588, 444131...</td>\n",
       "      <td>25297, 254761+38.0+0+1+0+0.0+0.0+0.0+0.0+0.0+0...</td>\n",
       "      <td>77670, 134222, 137989, 140273, 194133, 254761,...</td>\n",
       "      <td>75860, 81902, 193519, 197320, 199074, 200528, ...</td>\n",
       "      <td>77670, 201826, 254761, 261326, 312437, 435515,...</td>\n",
       "      <td>77734, 78473, 78619, 80242, 136368, 140673, 25...</td>\n",
       "      <td>25297, 30133, 73553, 74728, 80813, 194133, 254...</td>\n",
       "      <td>...</td>\n",
       "      <td>24134, 77670, 194133, 254061, 317002, 321588, ...</td>\n",
       "      <td>75860, 77670, 78474, 134718, 137977, 196168, 3...</td>\n",
       "      <td>73383, 77182, 78508, 133384, 135618, 141475, 3...</td>\n",
       "      <td>197236, 197672, 197684, 200843, 257011, 320128...</td>\n",
       "      <td>27674, 29735, 141323, 196456, 196726, 198263, ...</td>\n",
       "      <td>78832, 136788, 141104, 194133, 376707, 443769+...</td>\n",
       "      <td>23325, 24134, 24818, 25297, 30234, 31967, 7323...</td>\n",
       "      <td>24134, 26662, 27587, 31317, 74125, 77635, 7767...</td>\n",
       "      <td>193322, 194133, 194406, 195596, 197320, 198803...</td>\n",
       "      <td>27674, 30437, 31967, 78232, 80180, 192353, 195...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25297, 30133, 77139, 77421, 80502, 81902, 1369...</td>\n",
       "      <td>200962, 314658, 320128, 321588, 373503, 437113...</td>\n",
       "      <td>80502, 81902, 137809, 201826, 320128, 378253, ...</td>\n",
       "      <td>75909, 77030, 77234, 442111, 444459, 4128329, ...</td>\n",
       "      <td>25297, 31967, 260123, 375415, 375671, 378253, ...</td>\n",
       "      <td>135287, 197050, 197610, 200452, 254761, 433736...</td>\n",
       "      <td>75004, 77670, 201620, 316998, 320128, 320536, ...</td>\n",
       "      <td>27674, 75909, 77670, 134898, 196523, 254761, 3...</td>\n",
       "      <td>30437, 31967, 72711, 73819, 77030, 77635, 7906...</td>\n",
       "      <td>195867, 432545, 4081648, 4195572+21.0+0+1+939+...</td>\n",
       "      <td>...</td>\n",
       "      <td>198492, 432430, 433736, 433864, 435463, 435655...</td>\n",
       "      <td>436070, 442752, 4036803, 4147829, 4223659, 431...</td>\n",
       "      <td>140673, 200962, 254761, 257011, 312437, 320536...</td>\n",
       "      <td>75650, 80813, 136788, 197684, 200843, 254761, ...</td>\n",
       "      <td>75004, 193518, 195585, 197684, 200447, 201618,...</td>\n",
       "      <td>134461, 318736, 374640, 433736, 437541, 439407...</td>\n",
       "      <td>24609, 75650, 77670, 80767, 133810, 138388, 19...</td>\n",
       "      <td>81902, 194847, 200962, 320128, 437827, 442012,...</td>\n",
       "      <td>320128, 4236484+22.0+0+1+0+0.0+8507.0+0.0+0.0+...</td>\n",
       "      <td>26823, 28457, 31317, 75860, 135852, 135930, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78232, 78508, 80502, 134118, 135893, 195771, 1...</td>\n",
       "      <td>81175, 442752, 759852, 765131, 4151985, 432904...</td>\n",
       "      <td>80180, 196715, 201826, 312902, 313459, 319049,...</td>\n",
       "      <td>25297, 196523, 440029, 37311061+45.0+0+1+0+380...</td>\n",
       "      <td>77670, 316866, 437827+83.0+0+0+0+0.0+0.0+0.0+0...</td>\n",
       "      <td>194133, 195562, 377560, 378425, 438531, 404128...</td>\n",
       "      <td>254761, 37311061+75.0+0+1+0+0.0+0.0+0.0+0.0+0....</td>\n",
       "      <td>0, 25297, 193322, 254761, 257011, 260139, 3782...</td>\n",
       "      <td>27674, 75909, 132797, 133810, 136934, 193519, ...</td>\n",
       "      <td>78619, 78786, 141323, 254761, 256451, 257011, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>257011+60.0+0+0+600+38003564.0+8507.0+8527.0+0...</td>\n",
       "      <td>24818, 135930, 140821, 194133, 197440, 372887,...</td>\n",
       "      <td>312437+26.0+0+0+0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+...</td>\n",
       "      <td>75865, 75936, 77670, 78232, 134118, 134460, 13...</td>\n",
       "      <td>23325, 27674, 31967, 73819, 77670, 79864, 8018...</td>\n",
       "      <td>27674, 31317, 73819, 80180, 81902, 135930, 137...</td>\n",
       "      <td>24134, 24818, 25297, 72618, 73560, 74130, 7557...</td>\n",
       "      <td>24134, 79908, 80813, 135777, 135930, 136198, 1...</td>\n",
       "      <td>74104, 435640, 444094, 4061157, 4244438, 36712...</td>\n",
       "      <td>138384, 140673, 196168, 197044, 201909, 317002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79908, 197320, 200338, 200349, 200528, 261600,...</td>\n",
       "      <td>76786, 197381, 320128, 432867, 433753, 434614,...</td>\n",
       "      <td>140821, 198985, 200219, 438485, 442077, 409287...</td>\n",
       "      <td>75909, 77630, 78011, 134736, 135287, 135777, 1...</td>\n",
       "      <td>78232, 4060705, 44783028+16.0+0+1+0+0.0+0.0+0....</td>\n",
       "      <td>25297, 140821, 141323, 194133, 201618, 257011,...</td>\n",
       "      <td>257011, 374948, 443883, 4038037, 4047123, 4066...</td>\n",
       "      <td>4192174, 37311061+45.0+0+1+0+0.0+0.0+0.0+0.0+0...</td>\n",
       "      <td>434170, 4113650, 4171912+24.0+0+1+372+0.0+8532...</td>\n",
       "      <td>25297, 74104, 74698, 75576, 140673, 141253, 19...</td>\n",
       "      <td>...</td>\n",
       "      <td>74777, 78505, 80241, 139099, 257011, 378253, 3...</td>\n",
       "      <td>25297, 27674, 254761, 257011, 380733, 437663, ...</td>\n",
       "      <td>141095, 196360, 196523, 312622, 314666, 320128...</td>\n",
       "      <td>133384, 133655, 4115169, 4115170, 4115367, 413...</td>\n",
       "      <td>434613, 442077+32.0+0+1+0+0.0+0.0+0.0+0.0+0.0+...</td>\n",
       "      <td>195083, 200219, 4103703, 4155909, 4166126, 425...</td>\n",
       "      <td>196158, 4033717, 4193704+50.0+0+0+0+0.0+0.0+0....</td>\n",
       "      <td>257007, 313236, 432586, 436076, 440076, 419159...</td>\n",
       "      <td>23164, 80189, 133857, 136497, 138525, 194133, ...</td>\n",
       "      <td>23653, 26727, 30437, 30753, 31317, 73819, 7847...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24609, 79908, 134460, 134870, 135777, 135930, ...</td>\n",
       "      <td>24134, 73300, 74125, 75036, 80180, 255573, 255...</td>\n",
       "      <td>27674, 77670, 78193, 78232, 79938, 133228, 134...</td>\n",
       "      <td>75860, 140673, 312437, 313217, 314665, 319835,...</td>\n",
       "      <td>25297, 27674, 75860, 77670, 81723, 140214, 194...</td>\n",
       "      <td>312437, 4115171, 4170554, 4273307+61.0+0+0+478...</td>\n",
       "      <td>201620, 260123, 312648, 320128, 373478, 374044...</td>\n",
       "      <td>442588, 4273307+45.0+0+1+479+38003564.0+8507.0...</td>\n",
       "      <td>0, 72995, 78232, 138387, 140821, 194133, 19508...</td>\n",
       "      <td>319835, 320128, 320744, 432867, 439846, 764123...</td>\n",
       "      <td>...</td>\n",
       "      <td>77670, 4273307+60.0+0+0+464+38003564.0+8532.0+...</td>\n",
       "      <td>43021054, 43531419+25.0+0+0+0+0.0+0.0+0.0+0.0+...</td>\n",
       "      <td>77670, 78232, 78508, 257007, 372610, 375819, 4...</td>\n",
       "      <td>25297, 197381, 260123, 318736, 318800, 372328,...</td>\n",
       "      <td>79864, 80813, 134460, 137809, 198846, 254761, ...</td>\n",
       "      <td>80180, 132797, 320128, 433736, 437663, 442588,...</td>\n",
       "      <td>74725, 80189, 320128, 433736, 438720, 442588, ...</td>\n",
       "      <td>197684, 440029, 4103703, 4273307+57.0+0+0+0+0....</td>\n",
       "      <td>254761, 257011, 378427, 379805, 4200056+32.0+0...</td>\n",
       "      <td>77670, 78605, 78619, 138102, 140673, 141932, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>255573, 257011, 37311061+58.0+0+1+0+0.0+0.0+0....</td>\n",
       "      <td>318736, 378253+46.0+0+1+0+0.0+0.0+0.0+0.0+0.0+...</td>\n",
       "      <td>23325, 30437, 73754, 75650, 140673, 195314, 19...</td>\n",
       "      <td>78228, 139900, 312437, 433753, 4180628, 420942...</td>\n",
       "      <td>433864, 435655, 438543, 439658, 442355, 443871...</td>\n",
       "      <td>24134, 73754, 192242, 320128, 435613, 440897, ...</td>\n",
       "      <td>257007, 45766714+21.0+0+1+841+38003564.0+8532....</td>\n",
       "      <td>25297, 28060, 78508, 140641, 312437, 375281, 4...</td>\n",
       "      <td>28457, 139099, 433736, 434005, 436070, 437390,...</td>\n",
       "      <td>197607, 200779, 4095793, 4302555, 37311061+36....</td>\n",
       "      <td>...</td>\n",
       "      <td>437113, 37311061+83.0+0+0+0+0.0+0.0+0.0+0.0+0....</td>\n",
       "      <td>79864, 192450, 194081, 195588, 195590, 320128,...</td>\n",
       "      <td>432867, 433736, 435783, 436070, 4004672, 40853...</td>\n",
       "      <td>30437, 31317, 31967, 196523, 197381, 198678, 2...</td>\n",
       "      <td>25297, 25518, 26711, 27674, 74104, 77340, 7761...</td>\n",
       "      <td>24134, 73571, 75909, 137813, 194133, 195562, 1...</td>\n",
       "      <td>194702, 200588, 441641, 444094, 444114, 404128...</td>\n",
       "      <td>4028474, 37311061+13.0+0+1+019+38003564.0+8532...</td>\n",
       "      <td>378735, 433316, 442752, 4169095, 4211231, 4576...</td>\n",
       "      <td>4008161+17.0+0+1+0+38003564.0+8507.0+0.0+0.0+0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>26662, 31821, 75909, 78232, 80180, 193322, 193...</td>\n",
       "      <td>4273307+0.0+0+0+477+0.0+8532.0+0.0+0.0+0.0+0.0...</td>\n",
       "      <td>23653, 31317, 31610, 77670, 139900, 140214, 14...</td>\n",
       "      <td>23325, 73754, 80951, 141323, 312648, 320128, 4...</td>\n",
       "      <td>45766714+33.0+0+1+600+38003564.0+8507.0+8515.0...</td>\n",
       "      <td>440029, 37311061+43.0+0+0+0+0.0+0.0+0.0+0.0+0....</td>\n",
       "      <td>26662, 312648, 320128, 437827, 4029305, 408670...</td>\n",
       "      <td>434613, 438409, 4039212, 4253962, 46273463+14....</td>\n",
       "      <td>436070, 4041283, 4146209, 4180628, 4223659, 44...</td>\n",
       "      <td>25297, 77670, 134159, 138384, 140673, 317893, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>136788, 192671, 193528, 200219, 312437, 320128...</td>\n",
       "      <td>134438, 140214, 432867, 4273307, 45766714+50.0...</td>\n",
       "      <td>444202, 3655550, 4165681+21.0+0+1+272+38003564...</td>\n",
       "      <td>75860, 75909, 192367, 195562, 201078, 439080, ...</td>\n",
       "      <td>434480, 4014295, 4114164+1.0+0+1+0+0.0+0.0+0.0...</td>\n",
       "      <td>257011+3.0+0+1+430+38003564.0+8532.0+8527.0+0....</td>\n",
       "      <td>24660, 25297+19.0+0+1+0+0.0+0.0+0.0+0.0+0.0+0....</td>\n",
       "      <td>23986, 24134, 75860, 77074, 77670, 132797, 133...</td>\n",
       "      <td>77395, 80242, 198803, 201072, 377889, 378765, ...</td>\n",
       "      <td>23325, 31057, 72418, 134452, 134736, 140214, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>254761, 257011, 378253, 439407, 4328356+8.0+0+...</td>\n",
       "      <td>140821, 4117695+72.0+0+1+0+0.0+0.0+0.0+0.0+0.0...</td>\n",
       "      <td>25297, 77317, 139099, 193322, 195867, 196523, ...</td>\n",
       "      <td>312998, 4113821+28.0+0+1+604+38003564.0+8507.0...</td>\n",
       "      <td>436230, 4041283, 4086701, 4248728, 4305080+32....</td>\n",
       "      <td>77234, 134736, 194133, 200962, 201826, 254761,...</td>\n",
       "      <td>74728, 75667, 78234, 78508, 81151, 201909, 376...</td>\n",
       "      <td>195867, 432545, 432867, 436070, 4081648, 42000...</td>\n",
       "      <td>195083, 255848, 314658, 376208, 437663, 437677...</td>\n",
       "      <td>77234, 138825, 195562, 314962, 320128, 433736,...</td>\n",
       "      <td>...</td>\n",
       "      <td>80502, 140673, 320128, 436070, 438720, 443597,...</td>\n",
       "      <td>31317, 136773, 140214, 141095, 193322, 195083,...</td>\n",
       "      <td>24134, 73553, 74125, 80809, 136368, 141253, 19...</td>\n",
       "      <td>75860, 194133, 195083, 197684, 200219, 201603,...</td>\n",
       "      <td>31967, 78508, 195083, 200219, 201826, 256451, ...</td>\n",
       "      <td>25297, 81144, 197938, 254761, 320136, 375281, ...</td>\n",
       "      <td>77670, 78232, 138825, 194133, 197925, 198520, ...</td>\n",
       "      <td>4193704+40.0+0+1+0+0.0+0.0+0.0+0.0+0.0+0.0+0.0...</td>\n",
       "      <td>254761, 312437, 320128, 436070, 437827, 404128...</td>\n",
       "      <td>30437, 77670, 78508, 137989, 196569, 197381, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>23986, 379822, 443800, 4030186, 4242816, 43133...</td>\n",
       "      <td>27674, 30437, 75909, 80502, 140673, 321319, 37...</td>\n",
       "      <td>27674, 80141, 80180, 141663, 195771, 197320, 1...</td>\n",
       "      <td>75941, 78508, 137820, 140842, 194831, 201728, ...</td>\n",
       "      <td>759853, 4051605, 4115171, 4117695+63.0+0+1+467...</td>\n",
       "      <td>27674, 31967, 75036, 75650, 75860, 75909, 1327...</td>\n",
       "      <td>72737, 73231, 74396, 75311, 77670, 81902, 1398...</td>\n",
       "      <td>75576, 77670, 134460, 135618, 196523, 200219, ...</td>\n",
       "      <td>77647, 138384, 140673, 141825, 256451, 257012,...</td>\n",
       "      <td>77066, 81878, 135930, 138525, 140648, 140673, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>77395, 78508, 81151, 194133, 379805, 444131, 7...</td>\n",
       "      <td>75909, 77630, 77670, 78232, 133228, 201603, 43...</td>\n",
       "      <td>77670, 78232, 80186, 372448+24.0+0+1+0+0.0+0.0...</td>\n",
       "      <td>195259, 442077, 442588, 3661408, 4273307, 3731...</td>\n",
       "      <td>254058+1.0+0+0+207+38003563.0+8532.0+0.0+0.0+0...</td>\n",
       "      <td>73840, 432867, 438720, 440674, 441051, 443211,...</td>\n",
       "      <td>78472, 81878, 195876, 200780, 201627, 257011, ...</td>\n",
       "      <td>24134, 76565, 78272, 81175, 134159, 136788, 19...</td>\n",
       "      <td>438409, 442077, 4113821+20.0+0+1+0+0.0+0.0+0.0...</td>\n",
       "      <td>81902, 192450, 197320, 255573, 316139, 319835,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>25297, 432441, 434089, 436176, 439658, 440029,...</td>\n",
       "      <td>24134, 138387, 140673, 141375, 194984, 195007,...</td>\n",
       "      <td>80180, 80502, 134441, 134736, 135350, 192671, ...</td>\n",
       "      <td>200843, 320128, 321588, 432867, 4115171, 41719...</td>\n",
       "      <td>437116+21.0+0+0+0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+...</td>\n",
       "      <td>195867, 197607, 197684, 380111, 436659, 439777...</td>\n",
       "      <td>136773, 136788, 137053, 196523, 197320, 254761...</td>\n",
       "      <td>80180, 136788, 193402, 196523, 198803, 200843,...</td>\n",
       "      <td>37311061+21.0+0+1+0+0.0+0.0+0.0+0.0+0.0+0.0+0....</td>\n",
       "      <td>133835, 137193, 140214, 257007, 260123, 317009...</td>\n",
       "      <td>...</td>\n",
       "      <td>80180, 197684, 201826, 438720, 764123, 4117695...</td>\n",
       "      <td>25297, 31317, 31610, 75909, 75936, 77670, 1348...</td>\n",
       "      <td>23325, 24134, 27674, 31610, 31967, 72404, 7586...</td>\n",
       "      <td>27674, 73819, 75936, 78473, 80767, 134118, 135...</td>\n",
       "      <td>25297, 201603, 257011, 4025325, 37311061+24.0+...</td>\n",
       "      <td>25297, 28060, 77670, 78272, 80552, 140480, 141...</td>\n",
       "      <td>74722, 75910, 77670, 78232, 80502, 132344, 133...</td>\n",
       "      <td>24134, 75909, 77074, 77670, 78232, 80502, 8187...</td>\n",
       "      <td>140673, 373785, 378425, 4144111, 4265749, 3710...</td>\n",
       "      <td>80180, 193782, 201826, 261326, 320128, 433316,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>443 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    78473, 80767, 764599, 4062484, 4149382, 4169954, 4203722, 4223659, 4251903, 4306780+31.0+0+1+0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0  \\\n",
       "0    140214, 254761, 257011, 377889, 4141481+6.0+0+...                                                                                                                           \n",
       "1    25297, 30133, 77139, 77421, 80502, 81902, 1369...                                                                                                                           \n",
       "2    78232, 78508, 80502, 134118, 135893, 195771, 1...                                                                                                                           \n",
       "3    79908, 197320, 200338, 200349, 200528, 261600,...                                                                                                                           \n",
       "4    24609, 79908, 134460, 134870, 135777, 135930, ...                                                                                                                           \n",
       "..                                                 ...                                                                                                                           \n",
       "438  255573, 257011, 37311061+58.0+0+1+0+0.0+0.0+0....                                                                                                                           \n",
       "439  26662, 31821, 75909, 78232, 80180, 193322, 193...                                                                                                                           \n",
       "440  254761, 257011, 378253, 439407, 4328356+8.0+0+...                                                                                                                           \n",
       "441  23986, 379822, 443800, 4030186, 4242816, 43133...                                                                                                                           \n",
       "442  25297, 432441, 434089, 436176, 439658, 440029,...                                                                                                                           \n",
       "\n",
       "    23325, 25844, 30437, 77670, 197381, 198263, 200219, 201061, 315078, 437833, 442077, 4000609, 4209423, 4306292, 40481385, 40482267+32.0+0+1+808+38003564.0+8532.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+1.0+0.0  \\\n",
       "0    78473, 137548, 4236484, 37311338, 37311341+41....                                                                                                                                                                                        \n",
       "1    200962, 314658, 320128, 321588, 373503, 437113...                                                                                                                                                                                        \n",
       "2    81175, 442752, 759852, 765131, 4151985, 432904...                                                                                                                                                                                        \n",
       "3    76786, 197381, 320128, 432867, 433753, 434614,...                                                                                                                                                                                        \n",
       "4    24134, 73300, 74125, 75036, 80180, 255573, 255...                                                                                                                                                                                        \n",
       "..                                                 ...                                                                                                                                                                                        \n",
       "438  318736, 378253+46.0+0+1+0+0.0+0.0+0.0+0.0+0.0+...                                                                                                                                                                                        \n",
       "439  4273307+0.0+0+0+477+0.0+8532.0+0.0+0.0+0.0+0.0...                                                                                                                                                                                        \n",
       "440  140821, 4117695+72.0+0+1+0+0.0+0.0+0.0+0.0+0.0...                                                                                                                                                                                        \n",
       "441  27674, 30437, 75909, 80502, 140673, 321319, 37...                                                                                                                                                                                        \n",
       "442  24134, 138387, 140673, 141375, 194984, 195007,...                                                                                                                                                                                        \n",
       "\n",
       "    195083, 315078, 4152351+38.0+0+1+0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0  \\\n",
       "0    195867, 197684, 201078, 4031128, 4289526, 3701...                                                               \n",
       "1    80502, 81902, 137809, 201826, 320128, 378253, ...                                                               \n",
       "2    80180, 196715, 201826, 312902, 313459, 319049,...                                                               \n",
       "3    140821, 198985, 200219, 438485, 442077, 409287...                                                               \n",
       "4    27674, 77670, 78193, 78232, 79938, 133228, 134...                                                               \n",
       "..                                                 ...                                                               \n",
       "438  23325, 30437, 73754, 75650, 140673, 195314, 19...                                                               \n",
       "439  23653, 31317, 31610, 77670, 139900, 140214, 14...                                                               \n",
       "440  25297, 77317, 139099, 193322, 195867, 196523, ...                                                               \n",
       "441  27674, 80141, 80180, 141663, 195771, 197320, 1...                                                               \n",
       "442  80180, 80502, 134441, 134736, 135350, 192671, ...                                                               \n",
       "\n",
       "    201826, 254761, 257012, 261687, 320128, 438720, 4166902, 4273307, 37016349+64.0+0+1+0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0  \\\n",
       "0    133834, 140214, 438134, 438398, 442588, 444131...                                                                                                                  \n",
       "1    75909, 77030, 77234, 442111, 444459, 4128329, ...                                                                                                                  \n",
       "2    25297, 196523, 440029, 37311061+45.0+0+1+0+380...                                                                                                                  \n",
       "3    75909, 77630, 78011, 134736, 135287, 135777, 1...                                                                                                                  \n",
       "4    75860, 140673, 312437, 313217, 314665, 319835,...                                                                                                                  \n",
       "..                                                 ...                                                                                                                  \n",
       "438  78228, 139900, 312437, 433753, 4180628, 420942...                                                                                                                  \n",
       "439  23325, 73754, 80951, 141323, 312648, 320128, 4...                                                                                                                  \n",
       "440  312998, 4113821+28.0+0+1+604+38003564.0+8507.0...                                                                                                                  \n",
       "441  75941, 78508, 137820, 140842, 194831, 201728, ...                                                                                                                  \n",
       "442  200843, 320128, 321588, 432867, 4115171, 41719...                                                                                                                  \n",
       "\n",
       "    137989, 138717, 255573, 257004, 312437, 313217, 315282, 320128, 433316, 764123, 4154290, 4322024, 42538119+65.0+0+0+601+38003564.0+8532.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0  \\\n",
       "0    25297, 254761+38.0+0+1+0+0.0+0.0+0.0+0.0+0.0+0...                                                                                                                                                                 \n",
       "1    25297, 31967, 260123, 375415, 375671, 378253, ...                                                                                                                                                                 \n",
       "2    77670, 316866, 437827+83.0+0+0+0+0.0+0.0+0.0+0...                                                                                                                                                                 \n",
       "3    78232, 4060705, 44783028+16.0+0+1+0+0.0+0.0+0....                                                                                                                                                                 \n",
       "4    25297, 27674, 75860, 77670, 81723, 140214, 194...                                                                                                                                                                 \n",
       "..                                                 ...                                                                                                                                                                 \n",
       "438  433864, 435655, 438543, 439658, 442355, 443871...                                                                                                                                                                 \n",
       "439  45766714+33.0+0+1+600+38003564.0+8507.0+8515.0...                                                                                                                                                                 \n",
       "440  436230, 4041283, 4086701, 4248728, 4305080+32....                                                                                                                                                                 \n",
       "441  759853, 4051605, 4115171, 4117695+63.0+0+1+467...                                                                                                                                                                 \n",
       "442  437116+21.0+0+0+0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+...                                                                                                                                                                 \n",
       "\n",
       "    24134, 372324, 378253, 762296, 4038038, 4166125, 4273307, 4288310+71.0+0+0+460+38003564.0+8532.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0  \\\n",
       "0    77670, 134222, 137989, 140273, 194133, 254761,...                                                                                                                        \n",
       "1    135287, 197050, 197610, 200452, 254761, 433736...                                                                                                                        \n",
       "2    194133, 195562, 377560, 378425, 438531, 404128...                                                                                                                        \n",
       "3    25297, 140821, 141323, 194133, 201618, 257011,...                                                                                                                        \n",
       "4    312437, 4115171, 4170554, 4273307+61.0+0+0+478...                                                                                                                        \n",
       "..                                                 ...                                                                                                                        \n",
       "438  24134, 73754, 192242, 320128, 435613, 440897, ...                                                                                                                        \n",
       "439  440029, 37311061+43.0+0+0+0+0.0+0.0+0.0+0.0+0....                                                                                                                        \n",
       "440  77234, 134736, 194133, 200962, 201826, 254761,...                                                                                                                        \n",
       "441  27674, 31967, 75036, 75650, 75860, 75909, 1327...                                                                                                                        \n",
       "442  195867, 197607, 197684, 380111, 436659, 439777...                                                                                                                        \n",
       "\n",
       "    435950, 436826, 437116, 4115367, 4164959, 40305987+60.0+0+1+801+38003563.0+8532.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0  \\\n",
       "0    75860, 81902, 193519, 197320, 199074, 200528, ...                                                                                                      \n",
       "1    75004, 77670, 201620, 316998, 320128, 320536, ...                                                                                                      \n",
       "2    254761, 37311061+75.0+0+1+0+0.0+0.0+0.0+0.0+0....                                                                                                      \n",
       "3    257011, 374948, 443883, 4038037, 4047123, 4066...                                                                                                      \n",
       "4    201620, 260123, 312648, 320128, 373478, 374044...                                                                                                      \n",
       "..                                                 ...                                                                                                      \n",
       "438  257007, 45766714+21.0+0+1+841+38003564.0+8532....                                                                                                      \n",
       "439  26662, 312648, 320128, 437827, 4029305, 408670...                                                                                                      \n",
       "440  74728, 75667, 78234, 78508, 81151, 201909, 376...                                                                                                      \n",
       "441  72737, 73231, 74396, 75311, 77670, 81902, 1398...                                                                                                      \n",
       "442  136773, 136788, 137053, 196523, 197320, 254761...                                                                                                      \n",
       "\n",
       "    0, 132412, 132797, 134116, 138384, 139902, 140673, 194133, 197320, 318800, 320128, 377844, 432250, 432791, 433736, 436096, 437191, 437474, 437663, 438485, 439221, 439224, 440276, 440615, 441267, 441488, 443272, 4060985, 4064036, 4117957, 4144111, 4156229, 4169580, 4174262, 4186463, 4214956, 4226022, 4237320, 4240903, 4241527, 4305831, 4314870, 4322175, 4345578, 36712821, 36712860, 37016200, 43021283, 46272450+50.0+0+1+652+38003564.0+8532.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0  \\\n",
       "0    77670, 201826, 254761, 261326, 312437, 435515,...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "1    27674, 75909, 77670, 134898, 196523, 254761, 3...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "2    0, 25297, 193322, 254761, 257011, 260139, 3782...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "3    4192174, 37311061+45.0+0+1+0+0.0+0.0+0.0+0.0+0...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "4    442588, 4273307+45.0+0+1+479+38003564.0+8507.0...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "..                                                 ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "438  25297, 28060, 78508, 140641, 312437, 375281, 4...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "439  434613, 438409, 4039212, 4253962, 46273463+14....                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "440  195867, 432545, 432867, 436070, 4081648, 42000...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "441  75576, 77670, 134460, 135618, 196523, 200219, ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "442  80180, 136788, 193402, 196523, 198803, 200843,...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "\n",
       "    75036, 77074, 77965, 78508, 79908, 81175, 81878, 192963, 198678, 198803, 255573, 256451, 257011, 313217, 317002, 320128, 372448, 432867, 436230, 438720, 439297, 439777, 440029, 764123, 4057826, 4110914, 4144111, 4153380, 4154290, 4223659, 4261842, 4330445, 40479625, 40481919, 42537730, 46271022+88.0+0+1+820+38003564.0+8507.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0  \\\n",
       "0    77734, 78473, 78619, 80242, 136368, 140673, 25...                                                                                                                                                                                                                                                                                                                                                              \n",
       "1    30437, 31967, 72711, 73819, 77030, 77635, 7906...                                                                                                                                                                                                                                                                                                                                                              \n",
       "2    27674, 75909, 132797, 133810, 136934, 193519, ...                                                                                                                                                                                                                                                                                                                                                              \n",
       "3    434170, 4113650, 4171912+24.0+0+1+372+0.0+8532...                                                                                                                                                                                                                                                                                                                                                              \n",
       "4    0, 72995, 78232, 138387, 140821, 194133, 19508...                                                                                                                                                                                                                                                                                                                                                              \n",
       "..                                                 ...                                                                                                                                                                                                                                                                                                                                                              \n",
       "438  28457, 139099, 433736, 434005, 436070, 437390,...                                                                                                                                                                                                                                                                                                                                                              \n",
       "439  436070, 4041283, 4146209, 4180628, 4223659, 44...                                                                                                                                                                                                                                                                                                                                                              \n",
       "440  195083, 255848, 314658, 376208, 437663, 437677...                                                                                                                                                                                                                                                                                                                                                              \n",
       "441  77647, 138384, 140673, 141825, 256451, 257012,...                                                                                                                                                                                                                                                                                                                                                              \n",
       "442  37311061+21.0+0+1+0+0.0+0.0+0.0+0.0+0.0+0.0+0....                                                                                                                                                                                                                                                                                                                                                              \n",
       "\n",
       "    27674, 138825, 194133, 196360, 198401, 200174, 200843, 201337, 201620, 201826, 254761, 260123, 320128, 374367, 374375, 377889, 379805, 432867, 436070, 437246, 438699, 439777, 442019, 443211, 443597, 444131, 762289, 4031106, 4032787, 4041285, 4079750, 4084229, 4092743, 4110815, 4115991, 4155909, 4170554, 4171917, 4185946, 4193704, 4223659, 4236484, 4271013, 43531578, 45757656+86.0+0+0+212+38003564.0+8507.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0  \\\n",
       "0    25297, 30133, 73553, 74728, 80813, 194133, 254...                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "1    195867, 432545, 4081648, 4195572+21.0+0+1+939+...                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "2    78619, 78786, 141323, 254761, 256451, 257011, ...                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "3    25297, 74104, 74698, 75576, 140673, 141253, 19...                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "4    319835, 320128, 320744, 432867, 439846, 764123...                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "..                                                 ...                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "438  197607, 200779, 4095793, 4302555, 37311061+36....                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "439  25297, 77670, 134159, 138384, 140673, 317893, ...                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "440  77234, 138825, 195562, 314962, 320128, 433736,...                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "441  77066, 81878, 135930, 138525, 140648, 140673, ...                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "442  133835, 137193, 140214, 257007, 260123, 317009...                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "\n",
       "     ...  \\\n",
       "0    ...   \n",
       "1    ...   \n",
       "2    ...   \n",
       "3    ...   \n",
       "4    ...   \n",
       "..   ...   \n",
       "438  ...   \n",
       "439  ...   \n",
       "440  ...   \n",
       "441  ...   \n",
       "442  ...   \n",
       "\n",
       "    320128, 320536, 4223659, 4273307+57.0+0+1+460+38003564.0+8507.0+8522.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0  \\\n",
       "0    24134, 77670, 194133, 254061, 317002, 321588, ...                                                                                       \n",
       "1    198492, 432430, 433736, 433864, 435463, 435655...                                                                                       \n",
       "2    257011+60.0+0+0+600+38003564.0+8507.0+8527.0+0...                                                                                       \n",
       "3    74777, 78505, 80241, 139099, 257011, 378253, 3...                                                                                       \n",
       "4    77670, 4273307+60.0+0+0+464+38003564.0+8532.0+...                                                                                       \n",
       "..                                                 ...                                                                                       \n",
       "438  437113, 37311061+83.0+0+0+0+0.0+0.0+0.0+0.0+0....                                                                                       \n",
       "439  136788, 192671, 193528, 200219, 312437, 320128...                                                                                       \n",
       "440  80502, 140673, 320128, 436070, 438720, 443597,...                                                                                       \n",
       "441  77395, 78508, 81151, 194133, 379805, 444131, 7...                                                                                       \n",
       "442  80180, 197684, 201826, 438720, 764123, 4117695...                                                                                       \n",
       "\n",
       "    195867, 434005, 436230, 4092538, 4193704, 4282096, 40481920+29.0+0+1+0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0  \\\n",
       "0    75860, 77670, 78474, 134718, 137977, 196168, 3...                                                                                                   \n",
       "1    436070, 442752, 4036803, 4147829, 4223659, 431...                                                                                                   \n",
       "2    24818, 135930, 140821, 194133, 197440, 372887,...                                                                                                   \n",
       "3    25297, 27674, 254761, 257011, 380733, 437663, ...                                                                                                   \n",
       "4    43021054, 43531419+25.0+0+0+0+0.0+0.0+0.0+0.0+...                                                                                                   \n",
       "..                                                 ...                                                                                                   \n",
       "438  79864, 192450, 194081, 195588, 195590, 320128,...                                                                                                   \n",
       "439  134438, 140214, 432867, 4273307, 45766714+50.0...                                                                                                   \n",
       "440  31317, 136773, 140214, 141095, 193322, 195083,...                                                                                                   \n",
       "441  75909, 77630, 77670, 78232, 133228, 201603, 43...                                                                                                   \n",
       "442  25297, 31317, 31610, 75909, 75936, 77670, 1348...                                                                                                   \n",
       "\n",
       "    81239, 133810, 192450, 193782, 194686, 200174, 201826, 254761, 255848, 257004, 312437, 316139, 319835, 320136, 320536, 376065, 432867, 432870, 435515, 436230, 437113, 437663, 439696, 439777, 442019, 442077, 442111, 443729, 443919, 444101, 444459, 764123, 4007310, 4093531, 4162994, 4180628, 4187218, 4209423, 4216644, 4273307, 4310235, 4320635, 37016114, 37017432, 37309626, 40479576, 40481043, 40483287, 42537748, 43530690, 43531578, 44782429, 44784621, 45768812+60.0+0+0+474+38003564.0+8507.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0  \\\n",
       "0    73383, 77182, 78508, 133384, 135618, 141475, 3...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "1    140673, 200962, 254761, 257011, 312437, 320536...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "2    312437+26.0+0+0+0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "3    141095, 196360, 196523, 312622, 314666, 320128...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "4    77670, 78232, 78508, 257007, 372610, 375819, 4...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "..                                                 ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "438  432867, 433736, 435783, 436070, 4004672, 40853...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "439  444202, 3655550, 4165681+21.0+0+1+272+38003564...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "440  24134, 73553, 74125, 80809, 136368, 141253, 19...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "441  77670, 78232, 80186, 372448+24.0+0+1+0+0.0+0.0...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "442  23325, 24134, 27674, 31610, 31967, 72404, 7586...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "\n",
       "    77234, 78232, 135618, 198803, 255848, 374009, 377091, 435783, 435796, 436070, 436096, 437113, 437663, 439777, 442077, 443782, 444070, 4133224, 4144111, 4170554, 4224940, 4254485, 4273307, 4286201, 4336254+63.0+0+1+462+38003564.0+8507.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0  \\\n",
       "0    197236, 197672, 197684, 200843, 257011, 320128...                                                                                                                                                                                                                                                                   \n",
       "1    75650, 80813, 136788, 197684, 200843, 254761, ...                                                                                                                                                                                                                                                                   \n",
       "2    75865, 75936, 77670, 78232, 134118, 134460, 13...                                                                                                                                                                                                                                                                   \n",
       "3    133384, 133655, 4115169, 4115170, 4115367, 413...                                                                                                                                                                                                                                                                   \n",
       "4    25297, 197381, 260123, 318736, 318800, 372328,...                                                                                                                                                                                                                                                                   \n",
       "..                                                 ...                                                                                                                                                                                                                                                                   \n",
       "438  30437, 31317, 31967, 196523, 197381, 198678, 2...                                                                                                                                                                                                                                                                   \n",
       "439  75860, 75909, 192367, 195562, 201078, 439080, ...                                                                                                                                                                                                                                                                   \n",
       "440  75860, 194133, 195083, 197684, 200219, 201603,...                                                                                                                                                                                                                                                                   \n",
       "441  195259, 442077, 442588, 3661408, 4273307, 3731...                                                                                                                                                                                                                                                                   \n",
       "442  27674, 73819, 75936, 78473, 80767, 134118, 135...                                                                                                                                                                                                                                                                   \n",
       "\n",
       "    27674, 31967, 81902, 139900, 141371, 194081, 197684, 254761, 314754, 378253, 432545, 438028, 439235, 440383, 442077, 4002818, 4041283, 4077577, 4092411, 4152376, 4177206, 4196024, 4224968, 4282096, 4300092+23.0+0+1+522+38003564.0+8532.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0  \\\n",
       "0    27674, 29735, 141323, 196456, 196726, 198263, ...                                                                                                                                                                                                                                                                    \n",
       "1    75004, 193518, 195585, 197684, 200447, 201618,...                                                                                                                                                                                                                                                                    \n",
       "2    23325, 27674, 31967, 73819, 77670, 79864, 8018...                                                                                                                                                                                                                                                                    \n",
       "3    434613, 442077+32.0+0+1+0+0.0+0.0+0.0+0.0+0.0+...                                                                                                                                                                                                                                                                    \n",
       "4    79864, 80813, 134460, 137809, 198846, 254761, ...                                                                                                                                                                                                                                                                    \n",
       "..                                                 ...                                                                                                                                                                                                                                                                    \n",
       "438  25297, 25518, 26711, 27674, 74104, 77340, 7761...                                                                                                                                                                                                                                                                    \n",
       "439  434480, 4014295, 4114164+1.0+0+1+0+0.0+0.0+0.0...                                                                                                                                                                                                                                                                    \n",
       "440  31967, 78508, 195083, 200219, 201826, 256451, ...                                                                                                                                                                                                                                                                    \n",
       "441  254058+1.0+0+0+207+38003563.0+8532.0+0.0+0.0+0...                                                                                                                                                                                                                                                                    \n",
       "442  25297, 201603, 257011, 4025325, 37311061+24.0+...                                                                                                                                                                                                                                                                    \n",
       "\n",
       "    254761, 37311061+33.0+0+1+0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0  \\\n",
       "0    78832, 136788, 141104, 194133, 376707, 443769+...                                                        \n",
       "1    134461, 318736, 374640, 433736, 437541, 439407...                                                        \n",
       "2    27674, 31317, 73819, 80180, 81902, 135930, 137...                                                        \n",
       "3    195083, 200219, 4103703, 4155909, 4166126, 425...                                                        \n",
       "4    80180, 132797, 320128, 433736, 437663, 442588,...                                                        \n",
       "..                                                 ...                                                        \n",
       "438  24134, 73571, 75909, 137813, 194133, 195562, 1...                                                        \n",
       "439  257011+3.0+0+1+430+38003564.0+8532.0+8527.0+0....                                                        \n",
       "440  25297, 81144, 197938, 254761, 320136, 375281, ...                                                        \n",
       "441  73840, 432867, 438720, 440674, 441051, 443211,...                                                        \n",
       "442  25297, 28060, 77670, 78272, 80552, 140480, 141...                                                        \n",
       "\n",
       "    28060, 31967, 79864, 81902, 135473, 197381, 197684, 201340, 201603, 256451, 257012, 433753, 434613, 437663, 440383, 442077, 4025325, 4041283, 4214376, 4280498+40.0+0+1+240+38003564.0+8532.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+1.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0  \\\n",
       "0    23325, 24134, 24818, 25297, 30234, 31967, 7323...                                                                                                                                                                                                                     \n",
       "1    24609, 75650, 77670, 80767, 133810, 138388, 19...                                                                                                                                                                                                                     \n",
       "2    24134, 24818, 25297, 72618, 73560, 74130, 7557...                                                                                                                                                                                                                     \n",
       "3    196158, 4033717, 4193704+50.0+0+0+0+0.0+0.0+0....                                                                                                                                                                                                                     \n",
       "4    74725, 80189, 320128, 433736, 438720, 442588, ...                                                                                                                                                                                                                     \n",
       "..                                                 ...                                                                                                                                                                                                                     \n",
       "438  194702, 200588, 441641, 444094, 444114, 404128...                                                                                                                                                                                                                     \n",
       "439  24660, 25297+19.0+0+1+0+0.0+0.0+0.0+0.0+0.0+0....                                                                                                                                                                                                                     \n",
       "440  77670, 78232, 138825, 194133, 197925, 198520, ...                                                                                                                                                                                                                     \n",
       "441  78472, 81878, 195876, 200780, 201627, 257011, ...                                                                                                                                                                                                                     \n",
       "442  74722, 75910, 77670, 78232, 80502, 132344, 133...                                                                                                                                                                                                                     \n",
       "\n",
       "    81902, 442077, 4081648+25.0+0+1+372+38003564.0+8532.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+1.0+0.0  \\\n",
       "0    24134, 26662, 27587, 31317, 74125, 77635, 7767...                                                                             \n",
       "1    81902, 194847, 200962, 320128, 437827, 442012,...                                                                             \n",
       "2    24134, 79908, 80813, 135777, 135930, 136198, 1...                                                                             \n",
       "3    257007, 313236, 432586, 436076, 440076, 419159...                                                                             \n",
       "4    197684, 440029, 4103703, 4273307+57.0+0+0+0+0....                                                                             \n",
       "..                                                 ...                                                                             \n",
       "438  4028474, 37311061+13.0+0+1+019+38003564.0+8532...                                                                             \n",
       "439  23986, 24134, 75860, 77074, 77670, 132797, 133...                                                                             \n",
       "440  4193704+40.0+0+1+0+0.0+0.0+0.0+0.0+0.0+0.0+0.0...                                                                             \n",
       "441  24134, 76565, 78272, 81175, 134159, 136788, 19...                                                                             \n",
       "442  24134, 75909, 77074, 77670, 78232, 80502, 8187...                                                                             \n",
       "\n",
       "    135340, 200485+57.0+1+0+0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0  \\\n",
       "0    193322, 194133, 194406, 195596, 197320, 198803...                                                      \n",
       "1    320128, 4236484+22.0+0+1+0+0.0+8507.0+0.0+0.0+...                                                      \n",
       "2    74104, 435640, 444094, 4061157, 4244438, 36712...                                                      \n",
       "3    23164, 80189, 133857, 136497, 138525, 194133, ...                                                      \n",
       "4    254761, 257011, 378427, 379805, 4200056+32.0+0...                                                      \n",
       "..                                                 ...                                                      \n",
       "438  378735, 433316, 442752, 4169095, 4211231, 4576...                                                      \n",
       "439  77395, 80242, 198803, 201072, 377889, 378765, ...                                                      \n",
       "440  254761, 312437, 320128, 436070, 437827, 404128...                                                      \n",
       "441  438409, 442077, 4113821+20.0+0+1+0+0.0+0.0+0.0...                                                      \n",
       "442  140673, 373785, 378425, 4144111, 4265749, 3710...                                                      \n",
       "\n",
       "    31967, 133228, 137977, 374375, 433316, 436230, 439658, 443800, 4062790, 4152351, 4170137+26.0+0+1+0+38003563.0+8532.0+8527.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0+0.0  \n",
       "0    27674, 30437, 31967, 78232, 80180, 192353, 195...                                                                                                                                            \n",
       "1    26823, 28457, 31317, 75860, 135852, 135930, 13...                                                                                                                                            \n",
       "2    138384, 140673, 196168, 197044, 201909, 317002...                                                                                                                                            \n",
       "3    23653, 26727, 30437, 30753, 31317, 73819, 7847...                                                                                                                                            \n",
       "4    77670, 78605, 78619, 138102, 140673, 141932, 2...                                                                                                                                            \n",
       "..                                                 ...                                                                                                                                            \n",
       "438  4008161+17.0+0+1+0+38003564.0+8507.0+0.0+0.0+0...                                                                                                                                            \n",
       "439  23325, 31057, 72418, 134452, 134736, 140214, 1...                                                                                                                                            \n",
       "440  30437, 77670, 78508, 137989, 196569, 197381, 2...                                                                                                                                            \n",
       "441  81902, 192450, 197320, 255573, 316139, 319835,...                                                                                                                                            \n",
       "442  80180, 193782, 201826, 261326, 320128, 433316,...                                                                                                                                            \n",
       "\n",
       "[443 rows x 1000 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_arr = df.values\n",
    "np_arr1 = np.reshape(np_arr, (-1))\n",
    "df = pd.DataFrame(np_arr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140214, 254761, 257011, 377889, 4141481+6.0+0+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78473, 137548, 4236484, 37311338, 37311341+41....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>195867, 197684, 201078, 4031128, 4289526, 3701...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>133834, 140214, 438134, 438398, 442588, 444131...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25297, 254761+38.0+0+1+0+0.0+0.0+0.0+0.0+0.0+0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442995</th>\n",
       "      <td>25297, 28060, 77670, 78272, 80552, 140480, 141...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442996</th>\n",
       "      <td>74722, 75910, 77670, 78232, 80502, 132344, 133...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442997</th>\n",
       "      <td>24134, 75909, 77074, 77670, 78232, 80502, 8187...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442998</th>\n",
       "      <td>140673, 373785, 378425, 4144111, 4265749, 3710...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442999</th>\n",
       "      <td>80180, 193782, 201826, 261326, 320128, 433316,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>443000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0\n",
       "0       140214, 254761, 257011, 377889, 4141481+6.0+0+...\n",
       "1       78473, 137548, 4236484, 37311338, 37311341+41....\n",
       "2       195867, 197684, 201078, 4031128, 4289526, 3701...\n",
       "3       133834, 140214, 438134, 438398, 442588, 444131...\n",
       "4       25297, 254761+38.0+0+1+0+0.0+0.0+0.0+0.0+0.0+0...\n",
       "...                                                   ...\n",
       "442995  25297, 28060, 77670, 78272, 80552, 140480, 141...\n",
       "442996  74722, 75910, 77670, 78232, 80502, 132344, 133...\n",
       "442997  24134, 75909, 77074, 77670, 78232, 80502, 8187...\n",
       "442998  140673, 373785, 378425, 4144111, 4265749, 3710...\n",
       "442999  80180, 193782, 201826, 261326, 320128, 433316,...\n",
       "\n",
       "[443000 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['conditions', 'age' ,'severity_covid_death', 'outcome', 'zip', 'ethnicity_concept_id', 'gender_concept_id', 'race_concept_id','trazodone', 'amitriptyline', 'fluoxetine', 'citalopram', 'paroxetine', 'venlafaxine', 'vilazodone', 'vortioxetine', 'sertraline', 'bupropion', 'mirtazapine', 'desvenlafaxine', 'doxepin', 'duloxetine', 'escitalopram', 'nortriptyline']] = df[0].apply(lambda x: pd.Series(str(x).split('+')))\n",
    "df.drop(columns=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conditions</th>\n",
       "      <th>age</th>\n",
       "      <th>severity_covid_death</th>\n",
       "      <th>outcome</th>\n",
       "      <th>zip</th>\n",
       "      <th>ethnicity_concept_id</th>\n",
       "      <th>gender_concept_id</th>\n",
       "      <th>race_concept_id</th>\n",
       "      <th>trazodone</th>\n",
       "      <th>amitriptyline</th>\n",
       "      <th>...</th>\n",
       "      <th>vilazodone</th>\n",
       "      <th>vortioxetine</th>\n",
       "      <th>sertraline</th>\n",
       "      <th>bupropion</th>\n",
       "      <th>mirtazapine</th>\n",
       "      <th>desvenlafaxine</th>\n",
       "      <th>doxepin</th>\n",
       "      <th>duloxetine</th>\n",
       "      <th>escitalopram</th>\n",
       "      <th>nortriptyline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140214, 254761, 257011, 377889, 4141481</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>631</td>\n",
       "      <td>38003564.0</td>\n",
       "      <td>8532.0</td>\n",
       "      <td>8516.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78473, 137548, 4236484, 37311338, 37311341</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>195867, 197684, 201078, 4031128, 4289526, 3701...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>970</td>\n",
       "      <td>38003563.0</td>\n",
       "      <td>8532.0</td>\n",
       "      <td>8527.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>133834, 140214, 438134, 438398, 442588, 444131...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8507.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25297, 254761</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442995</th>\n",
       "      <td>25297, 28060, 77670, 78272, 80552, 140480, 141...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>841</td>\n",
       "      <td>38003563.0</td>\n",
       "      <td>8532.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442996</th>\n",
       "      <td>74722, 75910, 77670, 78232, 80502, 132344, 133...</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>38003564.0</td>\n",
       "      <td>8507.0</td>\n",
       "      <td>8516.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442997</th>\n",
       "      <td>24134, 75909, 77074, 77670, 78232, 80502, 8187...</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>605</td>\n",
       "      <td>38003564.0</td>\n",
       "      <td>8532.0</td>\n",
       "      <td>8527.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442998</th>\n",
       "      <td>140673, 373785, 378425, 4144111, 4265749, 3710...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>524</td>\n",
       "      <td>38003564.0</td>\n",
       "      <td>8532.0</td>\n",
       "      <td>8527.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442999</th>\n",
       "      <td>80180, 193782, 201826, 261326, 320128, 433316,...</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>443000 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               conditions   age  \\\n",
       "0                 140214, 254761, 257011, 377889, 4141481   6.0   \n",
       "1              78473, 137548, 4236484, 37311338, 37311341  41.0   \n",
       "2       195867, 197684, 201078, 4031128, 4289526, 3701...  62.0   \n",
       "3       133834, 140214, 438134, 438398, 442588, 444131...  26.0   \n",
       "4                                           25297, 254761  38.0   \n",
       "...                                                   ...   ...   \n",
       "442995  25297, 28060, 77670, 78272, 80552, 140480, 141...  16.0   \n",
       "442996  74722, 75910, 77670, 78232, 80502, 132344, 133...  75.0   \n",
       "442997  24134, 75909, 77074, 77670, 78232, 80502, 8187...  79.0   \n",
       "442998  140673, 373785, 378425, 4144111, 4265749, 3710...  45.0   \n",
       "442999  80180, 193782, 201826, 261326, 320128, 433316,...  83.0   \n",
       "\n",
       "       severity_covid_death outcome  zip ethnicity_concept_id  \\\n",
       "0                         0       1  631           38003564.0   \n",
       "1                         0       1    0                  0.0   \n",
       "2                         0       1  970           38003563.0   \n",
       "3                         0       1  463                  0.0   \n",
       "4                         0       1    0                  0.0   \n",
       "...                     ...     ...  ...                  ...   \n",
       "442995                    0       0  841           38003563.0   \n",
       "442996                    0       0  145           38003564.0   \n",
       "442997                    0       1  605           38003564.0   \n",
       "442998                    0       1  524           38003564.0   \n",
       "442999                    0       1    0                  0.0   \n",
       "\n",
       "       gender_concept_id race_concept_id trazodone amitriptyline  ...  \\\n",
       "0                 8532.0          8516.0       0.0           0.0  ...   \n",
       "1                    0.0             0.0       0.0           0.0  ...   \n",
       "2                 8532.0          8527.0       0.0           0.0  ...   \n",
       "3                 8507.0             0.0       0.0           0.0  ...   \n",
       "4                    0.0             0.0       0.0           0.0  ...   \n",
       "...                  ...             ...       ...           ...  ...   \n",
       "442995            8532.0             0.0       0.0           0.0  ...   \n",
       "442996            8507.0          8516.0       0.0           0.0  ...   \n",
       "442997            8532.0          8527.0       0.0           0.0  ...   \n",
       "442998            8532.0          8527.0       0.0           0.0  ...   \n",
       "442999               0.0             0.0       0.0           0.0  ...   \n",
       "\n",
       "       vilazodone vortioxetine sertraline bupropion mirtazapine  \\\n",
       "0             0.0          0.0        0.0       0.0         0.0   \n",
       "1             0.0          0.0        0.0       0.0         0.0   \n",
       "2             0.0          0.0        0.0       0.0         0.0   \n",
       "3             0.0          0.0        0.0       0.0         0.0   \n",
       "4             0.0          0.0        0.0       0.0         0.0   \n",
       "...           ...          ...        ...       ...         ...   \n",
       "442995        0.0          0.0        0.0       0.0         0.0   \n",
       "442996        0.0          0.0        0.0       0.0         0.0   \n",
       "442997        0.0          0.0        0.0       0.0         0.0   \n",
       "442998        0.0          0.0        0.0       0.0         0.0   \n",
       "442999        0.0          0.0        0.0       0.0         0.0   \n",
       "\n",
       "       desvenlafaxine doxepin duloxetine escitalopram nortriptyline  \n",
       "0                 0.0     0.0        0.0          0.0           0.0  \n",
       "1                 0.0     0.0        0.0          0.0           0.0  \n",
       "2                 0.0     0.0        0.0          0.0           0.0  \n",
       "3                 0.0     0.0        0.0          0.0           0.0  \n",
       "4                 0.0     0.0        0.0          0.0           0.0  \n",
       "...               ...     ...        ...          ...           ...  \n",
       "442995            0.0     0.0        0.0          1.0           0.0  \n",
       "442996            0.0     0.0        0.0          0.0           0.0  \n",
       "442997            0.0     0.0        0.0          0.0           0.0  \n",
       "442998            0.0     0.0        0.0          0.0           0.0  \n",
       "442999            0.0     0.0        0.0          0.0           0.0  \n",
       "\n",
       "[443000 rows x 24 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.conditions = df.conditions.apply(lambda x: str(x).split(','))\n",
    "df.conditions = df.conditions.apply(lambda x: list(map(int, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conditions</th>\n",
       "      <th>age</th>\n",
       "      <th>severity_covid_death</th>\n",
       "      <th>outcome</th>\n",
       "      <th>zip</th>\n",
       "      <th>ethnicity_concept_id</th>\n",
       "      <th>gender_concept_id</th>\n",
       "      <th>race_concept_id</th>\n",
       "      <th>trazodone</th>\n",
       "      <th>amitriptyline</th>\n",
       "      <th>...</th>\n",
       "      <th>vilazodone</th>\n",
       "      <th>vortioxetine</th>\n",
       "      <th>sertraline</th>\n",
       "      <th>bupropion</th>\n",
       "      <th>mirtazapine</th>\n",
       "      <th>desvenlafaxine</th>\n",
       "      <th>doxepin</th>\n",
       "      <th>duloxetine</th>\n",
       "      <th>escitalopram</th>\n",
       "      <th>nortriptyline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[140214, 254761, 257011, 377889, 4141481]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>631</td>\n",
       "      <td>38003564.0</td>\n",
       "      <td>8532.0</td>\n",
       "      <td>8516.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[78473, 137548, 4236484, 37311338, 37311341]</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[195867, 197684, 201078, 4031128, 4289526, 370...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>970</td>\n",
       "      <td>38003563.0</td>\n",
       "      <td>8532.0</td>\n",
       "      <td>8527.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[133834, 140214, 438134, 438398, 442588, 44413...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8507.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[25297, 254761]</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442995</th>\n",
       "      <td>[25297, 28060, 77670, 78272, 80552, 140480, 14...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>841</td>\n",
       "      <td>38003563.0</td>\n",
       "      <td>8532.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442996</th>\n",
       "      <td>[74722, 75910, 77670, 78232, 80502, 132344, 13...</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>38003564.0</td>\n",
       "      <td>8507.0</td>\n",
       "      <td>8516.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442997</th>\n",
       "      <td>[24134, 75909, 77074, 77670, 78232, 80502, 818...</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>605</td>\n",
       "      <td>38003564.0</td>\n",
       "      <td>8532.0</td>\n",
       "      <td>8527.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442998</th>\n",
       "      <td>[140673, 373785, 378425, 4144111, 4265749, 371...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>524</td>\n",
       "      <td>38003564.0</td>\n",
       "      <td>8532.0</td>\n",
       "      <td>8527.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442999</th>\n",
       "      <td>[80180, 193782, 201826, 261326, 320128, 433316...</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>443000 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               conditions   age  \\\n",
       "0               [140214, 254761, 257011, 377889, 4141481]   6.0   \n",
       "1            [78473, 137548, 4236484, 37311338, 37311341]  41.0   \n",
       "2       [195867, 197684, 201078, 4031128, 4289526, 370...  62.0   \n",
       "3       [133834, 140214, 438134, 438398, 442588, 44413...  26.0   \n",
       "4                                         [25297, 254761]  38.0   \n",
       "...                                                   ...   ...   \n",
       "442995  [25297, 28060, 77670, 78272, 80552, 140480, 14...  16.0   \n",
       "442996  [74722, 75910, 77670, 78232, 80502, 132344, 13...  75.0   \n",
       "442997  [24134, 75909, 77074, 77670, 78232, 80502, 818...  79.0   \n",
       "442998  [140673, 373785, 378425, 4144111, 4265749, 371...  45.0   \n",
       "442999  [80180, 193782, 201826, 261326, 320128, 433316...  83.0   \n",
       "\n",
       "       severity_covid_death outcome  zip ethnicity_concept_id  \\\n",
       "0                         0       1  631           38003564.0   \n",
       "1                         0       1    0                  0.0   \n",
       "2                         0       1  970           38003563.0   \n",
       "3                         0       1  463                  0.0   \n",
       "4                         0       1    0                  0.0   \n",
       "...                     ...     ...  ...                  ...   \n",
       "442995                    0       0  841           38003563.0   \n",
       "442996                    0       0  145           38003564.0   \n",
       "442997                    0       1  605           38003564.0   \n",
       "442998                    0       1  524           38003564.0   \n",
       "442999                    0       1    0                  0.0   \n",
       "\n",
       "       gender_concept_id race_concept_id trazodone amitriptyline  ...  \\\n",
       "0                 8532.0          8516.0       0.0           0.0  ...   \n",
       "1                    0.0             0.0       0.0           0.0  ...   \n",
       "2                 8532.0          8527.0       0.0           0.0  ...   \n",
       "3                 8507.0             0.0       0.0           0.0  ...   \n",
       "4                    0.0             0.0       0.0           0.0  ...   \n",
       "...                  ...             ...       ...           ...  ...   \n",
       "442995            8532.0             0.0       0.0           0.0  ...   \n",
       "442996            8507.0          8516.0       0.0           0.0  ...   \n",
       "442997            8532.0          8527.0       0.0           0.0  ...   \n",
       "442998            8532.0          8527.0       0.0           0.0  ...   \n",
       "442999               0.0             0.0       0.0           0.0  ...   \n",
       "\n",
       "       vilazodone vortioxetine sertraline bupropion mirtazapine  \\\n",
       "0             0.0          0.0        0.0       0.0         0.0   \n",
       "1             0.0          0.0        0.0       0.0         0.0   \n",
       "2             0.0          0.0        0.0       0.0         0.0   \n",
       "3             0.0          0.0        0.0       0.0         0.0   \n",
       "4             0.0          0.0        0.0       0.0         0.0   \n",
       "...           ...          ...        ...       ...         ...   \n",
       "442995        0.0          0.0        0.0       0.0         0.0   \n",
       "442996        0.0          0.0        0.0       0.0         0.0   \n",
       "442997        0.0          0.0        0.0       0.0         0.0   \n",
       "442998        0.0          0.0        0.0       0.0         0.0   \n",
       "442999        0.0          0.0        0.0       0.0         0.0   \n",
       "\n",
       "       desvenlafaxine doxepin duloxetine escitalopram nortriptyline  \n",
       "0                 0.0     0.0        0.0          0.0           0.0  \n",
       "1                 0.0     0.0        0.0          0.0           0.0  \n",
       "2                 0.0     0.0        0.0          0.0           0.0  \n",
       "3                 0.0     0.0        0.0          0.0           0.0  \n",
       "4                 0.0     0.0        0.0          0.0           0.0  \n",
       "...               ...     ...        ...          ...           ...  \n",
       "442995            0.0     0.0        0.0          1.0           0.0  \n",
       "442996            0.0     0.0        0.0          0.0           0.0  \n",
       "442997            0.0     0.0        0.0          0.0           0.0  \n",
       "442998            0.0     0.0        0.0          0.0           0.0  \n",
       "442999            0.0     0.0        0.0          0.0           0.0  \n",
       "\n",
       "[443000 rows x 24 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int(x):\n",
    "    try:\n",
    "        return int(float(x))\n",
    "    except ValueError:\n",
    "        return 0\n",
    "for col in df.columns[1:]:\n",
    "    df.loc[:,col] = df.loc[:,col].apply(convert_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conditions</th>\n",
       "      <th>age</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[140214, 254761, 257011, 377889, 4141481]</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[78473, 137548, 4236484, 37311338, 37311341]</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[195867, 197684, 201078, 4031128, 4289526, 370...</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[133834, 140214, 438134, 438398, 442588, 44413...</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[25297, 254761]</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442995</th>\n",
       "      <td>[25297, 28060, 77670, 78272, 80552, 140480, 14...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442996</th>\n",
       "      <td>[74722, 75910, 77670, 78232, 80502, 132344, 13...</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442997</th>\n",
       "      <td>[24134, 75909, 77074, 77670, 78232, 80502, 818...</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442998</th>\n",
       "      <td>[140673, 373785, 378425, 4144111, 4265749, 371...</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442999</th>\n",
       "      <td>[80180, 193782, 201826, 261326, 320128, 433316...</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>443000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               conditions  age  outcome\n",
       "0               [140214, 254761, 257011, 377889, 4141481]    6        1\n",
       "1            [78473, 137548, 4236484, 37311338, 37311341]   41        1\n",
       "2       [195867, 197684, 201078, 4031128, 4289526, 370...   62        1\n",
       "3       [133834, 140214, 438134, 438398, 442588, 44413...   26        1\n",
       "4                                         [25297, 254761]   38        1\n",
       "...                                                   ...  ...      ...\n",
       "442995  [25297, 28060, 77670, 78272, 80552, 140480, 14...   16        0\n",
       "442996  [74722, 75910, 77670, 78232, 80502, 132344, 13...   75        0\n",
       "442997  [24134, 75909, 77074, 77670, 78232, 80502, 818...   79        1\n",
       "442998  [140673, 373785, 378425, 4144111, 4265749, 371...   45        1\n",
       "442999  [80180, 193782, 201826, 261326, 320128, 433316...   83        1\n",
       "\n",
       "[443000 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['severity_covid_death', 'zip', 'ethnicity_concept_id', 'gender_concept_id', 'race_concept_id', 'trazodone', 'amitriptyline', 'fluoxetine', 'citalopram', 'paroxetine', 'venlafaxine', 'vilazodone', 'vortioxetine', 'sertraline', 'bupropion', 'mirtazapine', 'desvenlafaxine', 'doxepin', 'duloxetine', 'escitalopram', 'nortriptyline'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading concept id to snomed mapping.\n",
    "\n",
    "df_sn2con = pd.read_excel(\"/data2/users/ssultana/dataset/N3C_benchmarking/snomed_to_concept_id.xlsx\")\n",
    "df_sn2con.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_arr = df_sn2con.values\n",
    "np_arr1 = np.reshape(np_arr, (-1))\n",
    "df_sn2con = pd.DataFrame(np_arr1[:-30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sn2con[['snomed_id', 'condition_concept_id']] = df_sn2con[0].apply(lambda x: pd.Series(str(x).split('+')))\n",
    "df_sn2con.drop(columns=0, inplace=True)\n",
    "\n",
    "df_sn2con.snomed_id = df_sn2con.snomed_id.astype(int)\n",
    "df_sn2con.condition_concept_id = df_sn2con.condition_concept_id.apply(lambda x: int(float(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sn2con_dict = dict(zip(df_sn2con.condition_concept_id, df_sn2con.snomed_id))\n",
    "df['snomed_conditions'] = df.conditions.apply(lambda con: [df_sn2con_dict[x] for x in con])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['conditions'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>outcome</th>\n",
       "      <th>snomed_conditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>[271807003, 49727002, 54398005, 15188001, 2661...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>[266578003, 54404000, 91019004, 816055008, 816...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>[22741003, 49650001, 52441000, 109351006, 3696...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>[24079001, 271807003, 77692006, 111583006, 782...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>[363746003, 49727002]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442995</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[363746003, 43878008, 29857009, 70704007, 4281...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442996</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>[25343008, 2089002, 29857009, 267949000, 64859...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442997</th>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>[81680005, 76069003, 57676002, 29857009, 26794...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442998</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>[40930008, 10810001, 41446000, 266435005, 3975...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442999</th>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>[396275006, 46177005, 44054006, 75570004, 5962...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>443000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  outcome                                  snomed_conditions\n",
       "0         6        1  [271807003, 49727002, 54398005, 15188001, 2661...\n",
       "1        41        1  [266578003, 54404000, 91019004, 816055008, 816...\n",
       "2        62        1  [22741003, 49650001, 52441000, 109351006, 3696...\n",
       "3        26        1  [24079001, 271807003, 77692006, 111583006, 782...\n",
       "4        38        1                              [363746003, 49727002]\n",
       "...     ...      ...                                                ...\n",
       "442995   16        0  [363746003, 43878008, 29857009, 70704007, 4281...\n",
       "442996   75        0  [25343008, 2089002, 29857009, 267949000, 64859...\n",
       "442997   79        1  [81680005, 76069003, 57676002, 29857009, 26794...\n",
       "442998   45        1  [40930008, 10810001, 41446000, 266435005, 3975...\n",
       "442999   83        1  [396275006, 46177005, 44054006, 75570004, 5962...\n",
       "\n",
       "[443000 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# mlb = MultiLabelBinarizer()\n",
    "\n",
    "# df = pd.DataFrame(mlb.fit_transform(df['snomed_conditions']),columns=mlb.classes_, index=df.index)\n",
    "# print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data1/mrahman/snomed_embeddings/line.json') as f:\n",
    "    line_emb = json.load(f)\n",
    "# line_emb[14265004]\n",
    "line_emb = {int(k):v for k,v in line_emb.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_emb(snmd_con):\n",
    "    p = np.array([line_emb.get(x,[0]*128) for x in snmd_con])\n",
    "    p = p[~np.all(p == 0, axis=1)]\n",
    "    p = p.mean(axis=0)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embaded_snmd = df.snomed_conditions.apply(mean_emb)\n",
    "embaded_snmd\n",
    "embaded_snmd.isna().sum()\n",
    "embaded_snmd_line_arr = np.array(embaded_snmd.values.tolist())\n",
    "embaded_snmd_line_arr = np.nan_to_num(embaded_snmd_line_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_id = pd.DataFrame(embaded_snmd_line_arr, columns=list(range(0, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_id.reset_index(drop=True, inplace=True)\n",
    "df['age'].reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([df[['age']], s_id], axis=1)\n",
    "Y = df['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode age as 1, Unpriviledged as 0\n",
    "X.loc[X.age < 65, 'age'] = 1\n",
    "X.loc[X.age > 65, 'age'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>-0.001358</td>\n",
       "      <td>0.002084</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>-0.003194</td>\n",
       "      <td>-0.002017</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>-0.001622</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003053</td>\n",
       "      <td>0.004080</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>-0.000884</td>\n",
       "      <td>-0.002012</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>-0.002627</td>\n",
       "      <td>-0.001140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>-0.002621</td>\n",
       "      <td>0.002994</td>\n",
       "      <td>-0.000147</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>-0.000846</td>\n",
       "      <td>-0.001370</td>\n",
       "      <td>0.003216</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003449</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>-0.001343</td>\n",
       "      <td>-0.000485</td>\n",
       "      <td>-0.003039</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>-0.002352</td>\n",
       "      <td>0.001427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.004674</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>-0.001051</td>\n",
       "      <td>-0.000277</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>-0.000182</td>\n",
       "      <td>-0.000464</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006437</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>-0.000326</td>\n",
       "      <td>-0.002678</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>-0.001729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>-0.000915</td>\n",
       "      <td>-0.000376</td>\n",
       "      <td>0.002417</td>\n",
       "      <td>-0.001285</td>\n",
       "      <td>-0.002320</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004135</td>\n",
       "      <td>-0.002713</td>\n",
       "      <td>0.002329</td>\n",
       "      <td>-0.001288</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>-0.000912</td>\n",
       "      <td>0.002028</td>\n",
       "      <td>-0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.003684</td>\n",
       "      <td>-0.006422</td>\n",
       "      <td>0.003219</td>\n",
       "      <td>-0.002425</td>\n",
       "      <td>-0.002449</td>\n",
       "      <td>-0.000478</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>-0.004667</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001524</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>-0.000471</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>-0.001324</td>\n",
       "      <td>-0.004981</td>\n",
       "      <td>-0.000333</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>-0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442995</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.000467</td>\n",
       "      <td>-0.000825</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>-0.000290</td>\n",
       "      <td>-0.001395</td>\n",
       "      <td>-0.000198</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>-0.001215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>-0.001475</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>-0.000750</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>0.000674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442996</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.000658</td>\n",
       "      <td>-0.000474</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-0.000488</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-0.001617</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>-0.001093</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>-0.000706</td>\n",
       "      <td>-0.001100</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442997</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>-0.001067</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>-0.000341</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>-0.001181</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>-0.000623</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442998</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>-0.001729</td>\n",
       "      <td>-0.001714</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>-0.000282</td>\n",
       "      <td>0.001644</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000406</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>-0.000257</td>\n",
       "      <td>0.001249</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.001954</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>-0.001233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442999</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.000752</td>\n",
       "      <td>-0.000343</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>-0.001411</td>\n",
       "      <td>-0.001533</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>-0.001016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>-0.001191</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000420</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>0.001162</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>443000 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age         0         1         2         3         4         5  \\\n",
       "0         1  0.001320 -0.001358  0.002084  0.003564 -0.003194 -0.002017   \n",
       "1         1  0.000648 -0.002621  0.002994 -0.000147  0.002184  0.002705   \n",
       "2         1  0.004674  0.001998  0.002030  0.002025 -0.001051 -0.000277   \n",
       "3         1  0.001593 -0.000915 -0.000376  0.002417 -0.001285 -0.002320   \n",
       "4         1  0.003684 -0.006422  0.003219 -0.002425 -0.002449 -0.000478   \n",
       "...     ...       ...       ...       ...       ...       ...       ...   \n",
       "442995    1 -0.000467 -0.000825  0.000370 -0.000290 -0.001395 -0.000198   \n",
       "442996    0 -0.000658 -0.000474  0.000412  0.000091  0.000035 -0.000488   \n",
       "442997    0  0.000509 -0.001067  0.000397  0.000728 -0.000040  0.000596   \n",
       "442998    1  0.001082 -0.000088  0.001382  0.000173 -0.001729 -0.001714   \n",
       "442999    0 -0.000752 -0.000343  0.000046  0.000061 -0.001411 -0.001533   \n",
       "\n",
       "               6         7         8  ...       118       119       120  \\\n",
       "0       0.001431 -0.001622  0.001089  ...  0.003053  0.004080  0.000644   \n",
       "1      -0.000846 -0.001370  0.003216  ... -0.003449  0.000419  0.003201   \n",
       "2       0.001123 -0.000182 -0.000464  ... -0.006437  0.001706  0.000031   \n",
       "3       0.000633  0.000135  0.000067  ...  0.004135 -0.002713  0.002329   \n",
       "4       0.003236 -0.004667  0.000084  ... -0.001524  0.002558 -0.000471   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "442995  0.001334  0.000422 -0.001215  ...  0.000451  0.000471 -0.001475   \n",
       "442996 -0.000074 -0.001617 -0.000129  ...  0.000510  0.000491 -0.001093   \n",
       "442997 -0.000341 -0.000141 -0.000070  ...  0.001042  0.001216  0.001001   \n",
       "442998  0.001024 -0.000282  0.001644  ... -0.000406 -0.000125  0.001609   \n",
       "442999  0.000859  0.000895 -0.001016  ... -0.000087  0.001199  0.001943   \n",
       "\n",
       "             121       122       123       124       125       126       127  \n",
       "0       0.000411 -0.000884 -0.002012  0.002476  0.000626 -0.002627 -0.001140  \n",
       "1      -0.001343 -0.000485 -0.003039  0.001567  0.001480 -0.002352  0.001427  \n",
       "2       0.005099  0.002060 -0.000326 -0.002678  0.002073  0.001232 -0.001729  \n",
       "3      -0.001288  0.001073  0.001029  0.001538 -0.000912  0.002028 -0.000900  \n",
       "4       0.001785  0.003185 -0.001324 -0.004981 -0.000333  0.000254 -0.002200  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "442995  0.000336 -0.000064  0.001415 -0.000750  0.001002 -0.000078  0.000674  \n",
       "442996  0.000509  0.000042  0.000419 -0.000706 -0.001100  0.000032  0.000349  \n",
       "442997 -0.001181  0.000718 -0.000623  0.001059 -0.000044  0.000049  0.000059  \n",
       "442998 -0.000257  0.001249  0.000658  0.001954 -0.000659  0.002196 -0.001233  \n",
       "442999  0.001054 -0.001191  0.000029 -0.000420 -0.000034  0.001162  0.000115  \n",
       "\n",
       "[443000 rows x 129 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biased Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done   1 out of   1 | elapsed:  4.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression (Biased Dataset): 0.704\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "\n",
    "biased_lr = LogisticRegression(max_iter=1000 ,C=1, verbose=1, n_jobs=30).fit(X_train,y_train)\n",
    "\n",
    "y_lr_pred=biased_lr.predict(X_test)\n",
    "acc_lr_biased = metrics.accuracy_score(y_test, y_lr_pred)\n",
    "print(\"Accuracy of Logistic Regression (Biased Dataset): {:0.3f}\".format(acc_lr_biased))\n",
    "# cr_lr_biased = classification_report(y_test, y_lr_pred)\n",
    "# print(\"AClassification Report of Logistic Regression (Biased Dataset):\\n\", cr_lr_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend ThreadingBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done 100 out of 100 | elapsed:   46.8s finished\n",
      "[Parallel(n_jobs=30)]: Using backend ThreadingBackend with 30 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest (Biased Dataset): 0.759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Done 100 out of 100 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "biased_rf=RandomForestClassifier(n_estimators=100, verbose=1, n_jobs=30)\n",
    "\n",
    "biased_rf.fit(X_train,y_train)\n",
    "\n",
    "y_rf_pred=biased_rf.predict(X_test)\n",
    "acc_rf_biased = metrics.accuracy_score(y_test, y_rf_pred)\n",
    "print(\"Accuracy of Random Forest (Biased Dataset): {:0.3f}\".format(acc_rf_biased))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM].."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          130     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.45651D+05    |proj g|=  1.04611D+05\n",
      "\n",
      "At iterate   50    f=  2.12701D+05    |proj g|=  3.16118D+03\n",
      "\n",
      "At iterate  100    f=  2.12584D+05    |proj g|=  3.30809D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  130    114    132      1     0     0   1.242D+01   2.126D+05\n",
      "  F =   212580.12057693905     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      ".......WARN: libsvm Solver reached max_iter\n",
      "optimization finished, #iter = 10000\n",
      "obj = -20001.677277, rho = 0.043477\n",
      "nSV = 20000, nBSV = 20000\n",
      "Total nSV = 20000\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "biased_svm= svm.SVC(kernel='linear', verbose=1, max_iter=10000)\n",
    "biased_svm.fit(X_train,y_train)\n",
    "\n",
    "y_svm_pred=biased_svm.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM (Biased Dataset): 0.704\n"
     ]
    }
   ],
   "source": [
    "acc_svm_biased = metrics.accuracy_score(y_test, y_svm_pred)\n",
    "print(\"Accuracy of SVM (Biased Dataset): {:0.3f}\".format(acc_svm_biased))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 12:10:07.824849: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-11-03 12:10:07.824950: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (thor): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "355/355 [==============================] - 4s 6ms/step - loss: 0.6022 - accuracy: 0.7080 - val_loss: 0.5673 - val_accuracy: 0.7229\n",
      "Epoch 2/16\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.5651 - accuracy: 0.7250 - val_loss: 0.5654 - val_accuracy: 0.7199\n",
      "Epoch 3/16\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.5624 - accuracy: 0.7252 - val_loss: 0.5613 - val_accuracy: 0.7232\n",
      "Epoch 4/16\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.5596 - accuracy: 0.7260 - val_loss: 0.5587 - val_accuracy: 0.7242\n",
      "Epoch 5/16\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.5573 - accuracy: 0.7271 - val_loss: 0.5567 - val_accuracy: 0.7249\n",
      "Epoch 6/16\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.5557 - accuracy: 0.7284 - val_loss: 0.5555 - val_accuracy: 0.7238\n",
      "Epoch 7/16\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.5547 - accuracy: 0.7290 - val_loss: 0.5544 - val_accuracy: 0.7255\n",
      "Epoch 8/16\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.5537 - accuracy: 0.7290 - val_loss: 0.5532 - val_accuracy: 0.7279\n",
      "Epoch 9/16\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.5529 - accuracy: 0.7294 - val_loss: 0.5523 - val_accuracy: 0.7293\n",
      "Epoch 10/16\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.5520 - accuracy: 0.7299 - val_loss: 0.5512 - val_accuracy: 0.7295\n",
      "Epoch 11/16\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.5508 - accuracy: 0.7305 - val_loss: 0.5503 - val_accuracy: 0.7284\n",
      "Epoch 12/16\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.5489 - accuracy: 0.7311 - val_loss: 0.5471 - val_accuracy: 0.7313\n",
      "Epoch 13/16\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.5461 - accuracy: 0.7313 - val_loss: 0.5448 - val_accuracy: 0.7326\n",
      "Epoch 14/16\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.5440 - accuracy: 0.7321 - val_loss: 0.5428 - val_accuracy: 0.7328\n",
      "Epoch 15/16\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.5420 - accuracy: 0.7328 - val_loss: 0.5410 - val_accuracy: 0.7332\n",
      "Epoch 16/16\n",
      "355/355 [==============================] - 2s 5ms/step - loss: 0.5405 - accuracy: 0.7334 - val_loss: 0.5395 - val_accuracy: 0.7341\n",
      "2769/2769 [==============================] - 5s 2ms/step - loss: 0.5395 - accuracy: 0.7341\n",
      "2769/2769 [==============================] - 5s 2ms/step\n",
      "Accuracy of Neural Network (Biased Dataset): 0.734\n",
      "Precision of Neural Network (Biased Dataset): 0.760\n",
      "Recall of Neural Network (Biased Dataset): 0.909\n"
     ]
    }
   ],
   "source": [
    "#Neural Network\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=129, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=16, batch_size=1000)\n",
    "model.evaluate(X_test, y_test)\n",
    "y_nn_pred=model.predict(X_test).round()\n",
    "\n",
    "acc_nn_biased = metrics.accuracy_score(y_test, y_nn_pred)\n",
    "print(\"Accuracy of Neural Network (Biased Dataset): {:0.3f}\".format(acc_nn_biased))\n",
    "precision_nn_biased = metrics.precision_score(y_test, y_nn_pred)\n",
    "print(\"Precision of Neural Network (Biased Dataset): {:0.3f}\".format(precision_nn_biased))\n",
    "recall_nn_biased = metrics.recall_score(y_test, y_nn_pred)\n",
    "print(\"Recall of Neural Network (Biased Dataset): {:0.3f}\".format(recall_nn_biased))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIF360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# IBM's fairness tooolbox:\n",
    "from aif360.datasets import BinaryLabelDataset  # To handle the data\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric  # For calculating metrics\n",
    "from aif360.explainers import MetricTextExplainer  # For explaining metrics\n",
    "\n",
    "# Preprocessing technique\n",
    "from aif360.algorithms.preprocessing import Reweighing  # Preprocessing technique\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover # Preprocessing technique\n",
    "from aif360.algorithms.preprocessing.lfr import LFR # Preprocessing technique\n",
    "from aif360.algorithms.preprocessing.optim_preproc import OptimPreproc\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.distortion_functions import get_distortion_german\n",
    "\n",
    "# Inprocessing technique\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "from aif360.algorithms.inprocessing import MetaFairClassifier\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "from aif360.algorithms.inprocessing.exponentiated_gradient_reduction import ExponentiatedGradientReduction\n",
    "\n",
    "# Post-processing algorithm\n",
    "from aif360.algorithms.postprocessing.calibrated_eq_odds_postprocessing import CalibratedEqOddsPostprocessing\n",
    "from aif360.algorithms.postprocessing.reject_option_classification import RejectOptionClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pp_bld = BinaryLabelDataset(df=pd.concat((X_train, y_train),\n",
    "                                               axis=1),\n",
    "                                  label_names=['outcome'],\n",
    "                                  protected_attribute_names=['age'],\n",
    "                                  favorable_label=1,\n",
    "                                  unfavorable_label=0)\n",
    "\n",
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               instance weights            features                          \\\n",
       "                                protected attribute                           \n",
       "                                                age             0         1   \n",
       "instance names                                                                \n",
       "300540                      1.0                 1.0  6.602000e-04 -0.000992   \n",
       "37374                       1.0                 1.0  4.039167e-04 -0.000230   \n",
       "144904                      1.0                 1.0 -2.627500e-03 -0.000679   \n",
       "244352                      1.0                 1.0  0.000000e+00  0.000000   \n",
       "421368                      1.0                 1.0 -9.230769e-07 -0.000072   \n",
       "...                         ...                 ...           ...       ...   \n",
       "32549                       1.0                 1.0 -1.419000e-03  0.002717   \n",
       "423282                      1.0                 1.0  2.582500e-03 -0.001489   \n",
       "200646                      1.0                 1.0  5.599565e-04  0.000289   \n",
       "378514                      1.0                 1.0 -1.309882e-03  0.001524   \n",
       "22427                       1.0                 1.0 -9.364884e-04 -0.001654   \n",
       "\n",
       "                                                                            \\\n",
       "                                                                             \n",
       "                       2         3         4         5         6         7   \n",
       "instance names                                                               \n",
       "300540         -0.000173 -0.003740 -0.000183 -0.004009 -0.001467  0.001593   \n",
       "37374           0.001566  0.000126 -0.000472 -0.000481  0.000351  0.000567   \n",
       "144904         -0.002358  0.000954 -0.000411  0.001335 -0.003098  0.002973   \n",
       "244352          0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "421368          0.000032  0.000966  0.001630 -0.001064 -0.000848  0.000242   \n",
       "...                  ...       ...       ...       ...       ...       ...   \n",
       "32549           0.005562 -0.000420 -0.000449  0.002637  0.003535  0.003777   \n",
       "423282         -0.001343  0.005320  0.002566 -0.003452  0.001344  0.003716   \n",
       "200646          0.000516 -0.000066  0.000335  0.000480  0.000948  0.000985   \n",
       "378514          0.000324  0.000941 -0.000492 -0.001678  0.001214  0.001278   \n",
       "22427           0.000961  0.000648  0.000286  0.000876 -0.000771 -0.000040   \n",
       "\n",
       "                ...                                                    \\\n",
       "                ...                                                     \n",
       "                ...       119       120       121       122       123   \n",
       "instance names  ...                                                     \n",
       "300540          ...  0.000080  0.003412 -0.004865 -0.001593  0.001690   \n",
       "37374           ...  0.001743 -0.000240 -0.000471  0.000634 -0.000617   \n",
       "144904          ... -0.000179 -0.000818  0.002344  0.002892  0.000638   \n",
       "244352          ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "421368          ... -0.000802 -0.000086  0.000655  0.000383 -0.000378   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "32549           ...  0.002191  0.000826 -0.000209 -0.003134 -0.001650   \n",
       "423282          ... -0.003683  0.001698  0.000269  0.001500  0.000837   \n",
       "200646          ... -0.000195 -0.000479  0.001419  0.000939  0.000640   \n",
       "378514          ...  0.002048 -0.000818 -0.001150  0.000698  0.000552   \n",
       "22427           ...  0.001460 -0.000145  0.000132  0.000448 -0.000383   \n",
       "\n",
       "                                                       labels  \n",
       "                                                               \n",
       "                     124       125       126       127         \n",
       "instance names                                                 \n",
       "300540         -0.000469 -0.000122 -0.001249 -0.003073    1.0  \n",
       "37374           0.000873 -0.000755 -0.000299 -0.000421    1.0  \n",
       "144904         -0.001385 -0.001888  0.002836  0.002010    1.0  \n",
       "244352          0.000000  0.000000  0.000000  0.000000    0.0  \n",
       "421368          0.001097 -0.000827  0.000779 -0.000029    1.0  \n",
       "...                  ...       ...       ...       ...    ...  \n",
       "32549          -0.003491 -0.001407 -0.001520  0.000813    0.0  \n",
       "423282          0.005629  0.003858  0.004807 -0.000918    1.0  \n",
       "200646          0.000554 -0.000271  0.000714 -0.001722    0.0  \n",
       "378514         -0.000747 -0.000453  0.001763 -0.000341    1.0  \n",
       "22427           0.000896  0.000019 -0.000337 -0.000977    0.0  \n",
       "\n",
       "[354400 rows x 131 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pp_bld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pp_bld = BinaryLabelDataset(df=pd.concat((X_test, y_test),\n",
    "                                               axis=1),\n",
    "                                  label_names=['outcome'],\n",
    "                                  protected_attribute_names=['age'],\n",
    "                                  favorable_label=1,\n",
    "                                  unfavorable_label=0)\n",
    "\n",
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the original dataset is biased or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.327738\n",
      "Train set: Compute Disparate Impact ratio for unprivileged and privileged groups = 0.576708\n",
      "Train set: Compute base rate for privileged groups = 0.774261\n"
     ]
    }
   ],
   "source": [
    "# Metric for the original dataset\n",
    "metric_orig_train = BinaryLabelDatasetMetric(train_pp_bld, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.statistical_parity_difference())\n",
    "print(\"Train set: Compute Disparate Impact ratio for unprivileged and privileged groups = %f\" % metric_orig_train.disparate_impact())\n",
    "print(\"Train set: Compute base rate for privileged groups = %f\" % metric_orig_train.base_rate(privileged=privileged_groups))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original test dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.317301\n",
      "Test set: Compute Disparate Impact ratio for unprivileged and privileged groups = 0.589934\n",
      "Test set: Compute base rate for privileged groups = 0.773780\n"
     ]
    }
   ],
   "source": [
    "# Metric for the original dataset\n",
    "metric_orig_test = BinaryLabelDatasetMetric(test_pp_bld, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original test dataset\"))\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_test.mean_difference())\n",
    "print(\"Test set: Compute Disparate Impact ratio for unprivileged and privileged groups = %f\" % metric_orig_test.disparate_impact())\n",
    "print(\"Test set: Compute base rate for privileged groups = %f\" % metric_orig_test.base_rate(privileged=privileged_groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment 1: Mean difference of training and test dataset shows that privileged group is getting almost 32% more fabourable outcomes in the training dataset. So bias exists that needs to reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Bias Mitigation Algorithms: applied to the training data and modifies the training samples to reduce biasness.\n",
    "\n",
    "1. Reweighing pre-processing\n",
    "2. Disparate Impact Ratio pre-processing\n",
    "3. Learning Fair Representation pre-processing\n",
    "4. Optimized pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rewighing Pre-processing Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "train_pp_bld_rw = RW.fit_transform(train_pp_bld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Transformed training dataset - Reweighing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.000000\n",
      "Difference Disparate Impact between unprivileged and privileged groups = 1.000000\n"
     ]
    }
   ],
   "source": [
    "### Compute fairness metric on transformed dataset-rw\n",
    "metric_transf_train_rw = BinaryLabelDatasetMetric(train_pp_bld_rw, \n",
    "                                               unprivileged_groups=unprivileged_groups,\n",
    "                                               privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Transformed training dataset - Reweighing\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_train_rw.mean_difference())\n",
    "print(\"Difference Disparate Impact between unprivileged and privileged groups = %f\" % metric_transf_train_rw.disparate_impact())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training dataset is unbiased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply classifiers on transformed dataset via Reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_transf_rw = StandardScaler()\n",
    "\n",
    "## transformed training dataset\n",
    "X_train_rw = scale_transf_rw.fit_transform(train_pp_bld_rw.features)\n",
    "y_train_rw = train_pp_bld_rw.labels.ravel()\n",
    "\n",
    "## transformed test dataset\n",
    "dataset_transf_test_pred_rw = test_pp_bld.copy(deepcopy=True)\n",
    "X_test_rw = scale_transf_rw.fit_transform(dataset_transf_test_pred_rw.features)\n",
    "y_test_rw = dataset_transf_test_pred_rw.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Logistics Regresssion on Transformed Data(Bias Mitigated Dataset via Reweighing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmod_rw = LogisticRegression(max_iter=10000, n_jobs = 30)\n",
    "lmod_rw.fit(X_train_rw, y_train_rw,\n",
    "        sample_weight=train_pp_bld_rw.instance_weights)\n",
    "y_train_pred_rw = lmod_rw.predict(X_train_rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Model: LR- debiasing with RW - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.708025\n",
      "Test set: Balanced classification accuracy = 0.525985\n",
      "Test set: Disparate impact = 0.968844\n",
      "Test set: Equal opportunity difference = -0.008142\n",
      "Test set: Average odds difference = -0.016777\n",
      "Test set: Theil_index = 0.076739\n"
     ]
    }
   ],
   "source": [
    "y_lr_pred_rw = lmod_rw.predict(X_test_rw)\n",
    "\n",
    "\n",
    "dataset_debias_test_lr_rw = test_pp_bld.copy(deepcopy = True)\n",
    "dataset_debias_test_lr_rw.labels = y_lr_pred_rw.ravel()\n",
    "display(Markdown(\"#### Model: LR- debiasing with RW - classification metrics\"))\n",
    "classified_metric_debiasing_test_lr_rw = ClassificationMetric(test_pp_bld, \n",
    "                                                 dataset_debias_test_lr_rw,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test_lr_rw.accuracy())\n",
    "TPR_lr_rw = classified_metric_debiasing_test_lr_rw.true_positive_rate()\n",
    "TNR_lr_rw = classified_metric_debiasing_test_lr_rw.true_negative_rate()\n",
    "bal_acc_debiasing_test_lr_rw = 0.5*(TPR_lr_rw+TNR_lr_rw)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test_lr_rw)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test_lr_rw.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test_lr_rw.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test_lr_rw.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test_lr_rw.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Random Forest on Transformed Data(Bias Mitigated Dataset via Reweighing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend ThreadingBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done 100 out of 100 | elapsed:   47.4s finished\n",
      "[Parallel(n_jobs=30)]: Using backend ThreadingBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done 100 out of 100 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Model: RF- debiasing with RW - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.752935\n",
      "Test set: Balanced classification accuracy = 0.616506\n",
      "Test set: Disparate impact = 0.743227\n",
      "Test set: Equal opportunity difference = -0.160190\n",
      "Test set: Average odds difference = -0.182495\n",
      "Test set: Theil_index = 0.086854\n"
     ]
    }
   ],
   "source": [
    "rf_rw=RandomForestClassifier(n_estimators=100, verbose=1, n_jobs=30).fit(X_train_rw, y_train_rw,\n",
    "                                                                            sample_weight=train_pp_bld_rw.instance_weights)\n",
    "                                                                            \n",
    "y_rf_pred_rw=rf_rw.predict(X_test_rw)\n",
    "dataset_debias_test_rf_rw = test_pp_bld.copy(deepcopy = True)\n",
    "dataset_debias_test_rf_rw.labels = y_rf_pred_rw.ravel()\n",
    "display(Markdown(\"#### Model: RF- debiasing with RW - classification metrics\"))\n",
    "classified_metric_debiasing_test_rf_rw = ClassificationMetric(test_pp_bld, \n",
    "                                                 dataset_debias_test_rf_rw,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test_rf_rw.accuracy())\n",
    "TPR_rf_rw = classified_metric_debiasing_test_rf_rw.true_positive_rate()\n",
    "TNR_rf_rw = classified_metric_debiasing_test_rf_rw.true_negative_rate()\n",
    "bal_acc_debiasing_test_rf_rw = 0.5*(TPR_rf_rw+TNR_rf_rw)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test_rf_rw)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test_rf_rw.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test_rf_rw.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test_rf_rw.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test_rf_rw.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 SVM on Transformed Data(Bias Mitigated Dataset via Reweighing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM].........WARN: libsvm Solver reached max_iter\n",
      "optimization finished, #iter = 10000\n",
      "obj = -7532.605080, rho = 0.685099\n",
      "nSV = 9746, nBSV = 6454\n",
      "Total nSV = 9746\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "svm_rw= svm.SVC(kernel='linear', verbose=1, max_iter=10000).fit(X_train_rw, y_train_rw,\n",
    "                                                                            sample_weight=train_pp_bld_rw.instance_weights)\n",
    "                                                                            \n",
    "\n",
    "y_svm_pred_rw=svm_rw.predict(X_test_rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Model: SVM- debiasing with RW - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.708025\n",
      "Test set: Balanced classification accuracy = 0.525985\n",
      "Test set: Disparate impact = 1.010823\n",
      "Test set: Equal opportunity difference = 0.013591\n",
      "Test set: Average odds difference = 0.009956\n",
      "Test set: Theil_index = 0.072336\n"
     ]
    }
   ],
   "source": [
    "# acc_svm_rw = metrics.accuracy_score(y_test_rw, y_svm_pred_rw)\n",
    "# print(\"Rewighing Preprocessing: SVM accuracy: {:0.3f}\".format(acc_svm_rw))\n",
    "dataset_debias_test_svm_rw = test_pp_bld.copy(deepcopy = True)\n",
    "dataset_debias_test_svm_rw.labels = y_svm_pred_rw.ravel()\n",
    "display(Markdown(\"#### Model: SVM- debiasing with RW - classification metrics\"))\n",
    "classified_metric_debiasing_test_svm_rw = ClassificationMetric(test_pp_bld, \n",
    "                                                 dataset_debias_test_svm_rw,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test_lr_rw.accuracy())\n",
    "TPR_svm_rw = classified_metric_debiasing_test_svm_rw.true_positive_rate()\n",
    "TNR_svm_rw = classified_metric_debiasing_test_svm_rw.true_negative_rate()\n",
    "bal_acc_debiasing_test_svm_rw = 0.5*(TPR_lr_rw+TNR_lr_rw)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test_svm_rw)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test_svm_rw.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test_svm_rw.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test_svm_rw.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test_svm_rw.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Neural Network on transformed dataset (Reweighing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 354400 samples, validate on 88600 samples\n",
      "Epoch 1/16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 12:55:28.185769: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354400/354400 [==============================] - 2s 5us/sample - loss: 0.5772 - accuracy: 0.7085 - val_loss: 0.5578 - val_accuracy: 0.7287\n",
      "Epoch 2/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5528 - accuracy: 0.7336 - val_loss: 0.5446 - val_accuracy: 0.7416\n",
      "Epoch 3/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5432 - accuracy: 0.7414 - val_loss: 0.5380 - val_accuracy: 0.7469\n",
      "Epoch 4/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5385 - accuracy: 0.7442 - val_loss: 0.5352 - val_accuracy: 0.7478\n",
      "Epoch 5/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5359 - accuracy: 0.7459 - val_loss: 0.5336 - val_accuracy: 0.7482\n",
      "Epoch 6/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5339 - accuracy: 0.7472 - val_loss: 0.5324 - val_accuracy: 0.7493\n",
      "Epoch 7/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5325 - accuracy: 0.7479 - val_loss: 0.5318 - val_accuracy: 0.7498\n",
      "Epoch 8/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5313 - accuracy: 0.7490 - val_loss: 0.5303 - val_accuracy: 0.7500\n",
      "Epoch 9/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5304 - accuracy: 0.7493 - val_loss: 0.5298 - val_accuracy: 0.7506\n",
      "Epoch 10/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5297 - accuracy: 0.7498 - val_loss: 0.5290 - val_accuracy: 0.7508\n",
      "Epoch 11/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5292 - accuracy: 0.7499 - val_loss: 0.5288 - val_accuracy: 0.7511\n",
      "Epoch 12/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5286 - accuracy: 0.7502 - val_loss: 0.5284 - val_accuracy: 0.7509\n",
      "Epoch 13/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5281 - accuracy: 0.7503 - val_loss: 0.5284 - val_accuracy: 0.7508\n",
      "Epoch 14/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5277 - accuracy: 0.7507 - val_loss: 0.5278 - val_accuracy: 0.7516\n",
      "Epoch 15/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5273 - accuracy: 0.7513 - val_loss: 0.5283 - val_accuracy: 0.7514\n",
      "Epoch 16/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5271 - accuracy: 0.7508 - val_loss: 0.5278 - val_accuracy: 0.7508\n"
     ]
    }
   ],
   "source": [
    "#Neural Network\n",
    "\n",
    "nn_rw = Sequential()\n",
    "nn_rw.add(Dense(12, input_dim=129, activation='relu'))\n",
    "nn_rw.add(Dense(8, activation='relu'))\n",
    "nn_rw.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "nn_rw.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "nn_rw.fit(X_train_rw, y_train_rw, validation_data=(X_test_rw, y_test_rw), epochs=16, batch_size=1000)\n",
    "nn_rw.evaluate(X_test_rw, y_test_rw)\n",
    "y_nn_pred_rw=nn_rw.predict(X_test_rw).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Model: NN- debiasing with RW - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.750847\n",
      "Test set: Balanced classification accuracy = 0.625044\n",
      "Test set: Disparate impact = 0.732356\n",
      "Test set: Equal opportunity difference = -0.166594\n",
      "Test set: Average odds difference = -0.180465\n",
      "Test set: Theil_index = 0.098765\n"
     ]
    }
   ],
   "source": [
    "dataset_debias_test_nn_rw = test_pp_bld.copy(deepcopy = True)\n",
    "dataset_debias_test_nn_rw.labels = y_nn_pred_rw\n",
    "display(Markdown(\"#### Model: NN- debiasing with RW - classification metrics\"))\n",
    "classified_metric_debiasing_test_nn_rw = ClassificationMetric(test_pp_bld, \n",
    "                                                 dataset_debias_test_nn_rw,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test_nn_rw.accuracy())\n",
    "TPR_nn_rw = classified_metric_debiasing_test_nn_rw.true_positive_rate()\n",
    "TNR_nn_rw = classified_metric_debiasing_test_nn_rw.true_negative_rate()\n",
    "bal_acc_debiasing_test_nn_rw = 0.5*(TPR_nn_rw+TNR_nn_rw)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test_nn_rw)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test_nn_rw.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test_nn_rw.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test_nn_rw.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test_nn_rw.theil_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Accuracy on Test Set [Preprocessing Algorithm:]:')\n",
    "\n",
    "# # iterables = [[\"Logistic Regression\", \"Random Forest\", \"SVM\", \"Neural Network\"], [\"Classification Accuracy\", \"Balanced Classification Accuracy\", \"Disparate Impact\", \"Equal opportunity difference\", \"Average odds difference\", \"Theil Index\"]]\n",
    "\n",
    "# iterables = [[\"Logistic Regression\", \"Random Forest\", \"Neural Network\"], [\"Classification Accuracy\", \"Balanced Classification Accuracy\", \"Disparate Impact\", \"Equal opportunity difference\", \"Average odds difference\", \"Theil Index\"]]\n",
    "\n",
    "# index = pd.MultiIndex.from_product(iterables, names=[\"Classification\", \"Fairness Matrics\"])\n",
    "\n",
    "\n",
    "\n",
    "# # df = pd.DataFrame({'Count A': [12., 70., 30., 20.], 'Count B': [12., 70., 30., 20.]}, index=index)\n",
    "\n",
    "\n",
    "# result_acc = pd.DataFrame({'Classification Accuracy':             [classified_metric_debiasing_test_lr_rw.accuracy(), \n",
    "#                                                                   classified_metric_debiasing_test_rf_rw.accuracy(),\n",
    "#                                                                   classified_metric_debiasing_test_nn_rw.accuracy()\n",
    "#                                                                   ],\n",
    "#                         'Balanced Classification Accuracy':       [bal_acc_debiasing_test_lr_rw,\n",
    "#                                                                   bal_acc_debiasing_test_rf_rw,\n",
    "#                                                                   bal_acc_debiasing_test_nn_rw\n",
    "#                                                                   ],\n",
    "#                         'Disparate impact':                       [classified_metric_debiasing_test_lr_rw.disparate_impact(),\n",
    "#                                                                   classified_metric_debiasing_test_rf_rw.disparate_impact(),\n",
    "#                                                                   classified_metric_debiasing_test_nn_rw.disparate_impact()\n",
    "#                                                                   ],\n",
    "#                         'Equal opportunity difference':           [classified_metric_debiasing_test_lr_rw.equal_opportunity_difference(),\n",
    "#                                                                   classified_metric_debiasing_test_rf_rw.equal_opportunity_difference(),\n",
    "#                                                                   classified_metric_debiasing_test_nn_rw.equal_opportunity_difference()\n",
    "#                                                                   ],\n",
    "#                         'Average odds difference':                [classified_metric_debiasing_test_lr_rw.average_odds_difference(),\n",
    "#                                                                   classified_metric_debiasing_test_rf_rw.average_odds_difference(),\n",
    "#                                                                   classified_metric_debiasing_test_nn_rw.average_odds_difference()\n",
    "#                                                                   ],\n",
    "#                         'Theil_index':                            [classified_metric_debiasing_test_lr_rw.theil_index(),\n",
    "#                                                                   classified_metric_debiasing_test_rf_rw.theil_index(),\n",
    "#                                                                   classified_metric_debiasing_test_nn_rw.theil_index()\n",
    "#                                                                   ],\n",
    "\n",
    "#                         }, \n",
    "#                      index = index)\n",
    "\n",
    "# result_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Disparate Impact Ratio Pre-processing Algorithm\n",
    "Disparate Impact Remover is a pre-processing technique that edits values, which will be used as features, to increase fairness between the groups. It compares the proportion of individuals that receive a positive output for two groups: an unprivileged group and a privileged group. The calculation is the proportion of the unprivileged group that received the positive outcome divided by the proportion of the privileged group that received the positive outcome.\n",
    "The industry standard is a four-fifths rule: if the unprivileged group receives a positive outcome less than 80% of their proportion of the privileged group, this is a disparate impact violation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "DI = DisparateImpactRemover(repair_level=1)\n",
    "train_pp_bld_di = DI.fit_transform(train_pp_bld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Transformed training dataset - Disparate Impact"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.327738\n",
      "Difference Disparate Impact between unprivileged and privileged groups = 0.576708\n"
     ]
    }
   ],
   "source": [
    "### Compute fairness metric on transformed dataset-DI\n",
    "metric_transf_train_di = BinaryLabelDatasetMetric(train_pp_bld_di, \n",
    "                                               unprivileged_groups=unprivileged_groups,\n",
    "                                               privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Transformed training dataset - Disparate Impact\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_train_di.mean_difference())\n",
    "print(\"Difference Disparate Impact between unprivileged and privileged groups = %f\" % metric_transf_train_di.disparate_impact())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply classifiers on transformed dataset via Disparate Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_transf_di = StandardScaler()\n",
    "\n",
    "## transformed training dataset\n",
    "X_train_di = scale_transf_di.fit_transform(train_pp_bld_di.features)\n",
    "y_train_di = train_pp_bld_di.labels.ravel()\n",
    "\n",
    "## transformed test dataset\n",
    "dataset_transf_test_pred_di = test_pp_bld.copy(deepcopy=True)\n",
    "X_test_di = scale_transf_di.fit_transform(dataset_transf_test_pred_di.features)\n",
    "y_test_di = dataset_transf_test_pred_di.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Logistic Regression on transformed (DI) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced', max_iter=10000, n_jobs=30,\n",
       "                   solver='liblinear')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmod_di = LogisticRegression(max_iter=10000, n_jobs = 30, class_weight='balanced', solver='liblinear')\n",
    "lmod_di.fit(X_train_di, y_train_di,\n",
    "        sample_weight=train_pp_bld_di.instance_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Model: LR- debiasing with DI - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.608736\n",
      "Test set: Balanced classification accuracy = 0.619379\n",
      "Test set: Disparate impact = 0.563515\n",
      "Test set: Equal opportunity difference = -0.226970\n",
      "Test set: Average odds difference = -0.191190\n",
      "Test set: Theil_index = 0.378022\n"
     ]
    }
   ],
   "source": [
    "y_lr_pred_di = lmod_di.predict(X_test_rw)\n",
    "\n",
    "dataset_debias_test_lr_di = test_pp_bld.copy(deepcopy = True)\n",
    "dataset_debias_test_lr_di.labels = y_lr_pred_di\n",
    "display(Markdown(\"#### Model: LR- debiasing with DI - classification metrics\"))\n",
    "classified_metric_debiasing_test_lr_di = ClassificationMetric(test_pp_bld, \n",
    "                                                 dataset_debias_test_lr_di,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test_lr_di.accuracy())\n",
    "TPR_lr_di = classified_metric_debiasing_test_lr_di.true_positive_rate()\n",
    "TNR_lr_di = classified_metric_debiasing_test_lr_di.true_negative_rate()\n",
    "bal_acc_debiasing_test_lr_di = 0.5*(TPR_lr_di+TNR_lr_di)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test_lr_di)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test_lr_di.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test_lr_di.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test_lr_di.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test_lr_di.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random Forest on transformed (DI) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend ThreadingBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done 100 out of 100 | elapsed:   51.4s finished\n",
      "[Parallel(n_jobs=30)]: Using backend ThreadingBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done 100 out of 100 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Model: RF- debiasing with di - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.753386\n",
      "Test set: Balanced classification accuracy = 0.616572\n",
      "Test set: Disparate impact = 0.747360\n",
      "Test set: Equal opportunity difference = -0.154361\n",
      "Test set: Average odds difference = -0.178429\n",
      "Test set: Theil_index = 0.086129\n"
     ]
    }
   ],
   "source": [
    "rf_di=RandomForestClassifier(n_estimators=100, verbose=1, n_jobs=30).fit(X_train_rw, y_train_rw,\n",
    "                                                                            sample_weight=train_pp_bld_rw.instance_weights)\n",
    "                                                                            \n",
    "y_rf_pred_di=rf_di.predict(X_test_di)\n",
    "\n",
    "dataset_debias_test_rf_di = test_pp_bld.copy(deepcopy = True)\n",
    "dataset_debias_test_rf_di.labels = y_rf_pred_di\n",
    "display(Markdown(\"#### Model: RF- debiasing with di - classification metrics\"))\n",
    "classified_metric_debiasing_test_rf_di = ClassificationMetric(test_pp_bld, \n",
    "                                                 dataset_debias_test_rf_di,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test_rf_di.accuracy())\n",
    "TPR_rf_di = classified_metric_debiasing_test_rf_di.true_positive_rate()\n",
    "TNR_rf_di = classified_metric_debiasing_test_rf_di.true_negative_rate()\n",
    "bal_acc_debiasing_test_rf_di = 0.5*(TPR_rf_di+TNR_rf_di)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test_rf_di)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test_rf_di.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test_rf_di.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test_rf_di.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test_rf_di.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 SVM on transformed (DI) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM].........WARN: libsvm Solver reached max_iter\n",
      "optimization finished, #iter = 10000\n",
      "obj = -4030.413983, rho = 0.867157\n",
      "nSV = 7858, nBSV = 3671\n",
      "Total nSV = 7858\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "svm_di= svm.SVC(kernel='linear', verbose=1, max_iter=10000).fit(X_train_di, y_train_di,\n",
    "                                                                            sample_weight=train_pp_bld_di.instance_weights)\n",
    "                                                                            \n",
    "\n",
    "y_svm_pred_di=svm_di.predict(X_test_di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Model: SVM- debiasing with DI - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.708025\n",
      "Test set: Balanced classification accuracy = 0.619379\n",
      "Test set: Disparate impact = 1.007426\n",
      "Test set: Equal opportunity difference = 0.010338\n",
      "Test set: Average odds difference = 0.005510\n",
      "Test set: Theil_index = 0.069522\n"
     ]
    }
   ],
   "source": [
    "dataset_debias_test_svm_di = test_pp_bld.copy(deepcopy = True)\n",
    "dataset_debias_test_svm_di.labels = y_svm_pred_di\n",
    "display(Markdown(\"#### Model: SVM- debiasing with DI - classification metrics\"))\n",
    "classified_metric_debiasing_test_svm_di = ClassificationMetric(test_pp_bld, \n",
    "                                                 dataset_debias_test_svm_di,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test_lr_rw.accuracy())\n",
    "TPR_svm_di = classified_metric_debiasing_test_svm_di.true_positive_rate()\n",
    "TNR_svm_di = classified_metric_debiasing_test_svm_di.true_negative_rate()\n",
    "bal_acc_debiasing_test_svm_di= 0.5*(TPR_lr_di+TNR_lr_di)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test_svm_di)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test_svm_di.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test_svm_di.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test_svm_di.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test_svm_di.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Neural Network on transformed (DI) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 354400 samples, validate on 88600 samples\n",
      "Epoch 1/16\n",
      "354400/354400 [==============================] - 2s 5us/sample - loss: 0.6033 - accuracy: 0.6881 - val_loss: 0.5683 - val_accuracy: 0.7132\n",
      "Epoch 2/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5741 - accuracy: 0.7133 - val_loss: 0.5598 - val_accuracy: 0.7174\n",
      "Epoch 3/16\n",
      "354400/354400 [==============================] - 2s 5us/sample - loss: 0.5656 - accuracy: 0.7202 - val_loss: 0.5526 - val_accuracy: 0.7263\n",
      "Epoch 4/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5572 - accuracy: 0.7274 - val_loss: 0.5453 - val_accuracy: 0.7334\n",
      "Epoch 5/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5478 - accuracy: 0.7355 - val_loss: 0.5391 - val_accuracy: 0.7386\n",
      "Epoch 6/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5377 - accuracy: 0.7426 - val_loss: 0.5332 - val_accuracy: 0.7444\n",
      "Epoch 7/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5289 - accuracy: 0.7483 - val_loss: 0.5296 - val_accuracy: 0.7450\n",
      "Epoch 8/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5237 - accuracy: 0.7501 - val_loss: 0.5280 - val_accuracy: 0.7454\n",
      "Epoch 9/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5207 - accuracy: 0.7518 - val_loss: 0.5272 - val_accuracy: 0.7451\n",
      "Epoch 10/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5187 - accuracy: 0.7525 - val_loss: 0.5246 - val_accuracy: 0.7466\n",
      "Epoch 11/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5173 - accuracy: 0.7534 - val_loss: 0.5230 - val_accuracy: 0.7481\n",
      "Epoch 12/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5163 - accuracy: 0.7537 - val_loss: 0.5240 - val_accuracy: 0.7467\n",
      "Epoch 13/16\n",
      "354400/354400 [==============================] - 2s 5us/sample - loss: 0.5154 - accuracy: 0.7546 - val_loss: 0.5244 - val_accuracy: 0.7481\n",
      "Epoch 14/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5147 - accuracy: 0.7548 - val_loss: 0.5222 - val_accuracy: 0.7482\n",
      "Epoch 15/16\n",
      "354400/354400 [==============================] - 2s 5us/sample - loss: 0.5141 - accuracy: 0.7554 - val_loss: 0.5213 - val_accuracy: 0.7496\n",
      "Epoch 16/16\n",
      "354400/354400 [==============================] - 2s 5us/sample - loss: 0.5134 - accuracy: 0.7556 - val_loss: 0.5229 - val_accuracy: 0.7483\n"
     ]
    }
   ],
   "source": [
    "#Neural Network\n",
    "\n",
    "nn_di = Sequential()\n",
    "nn_di.add(Dense(12, input_dim=129, activation='relu'))\n",
    "nn_di.add(Dense(8, activation='relu'))\n",
    "nn_di.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "nn_di.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "nn_di.fit(X_train_di, y_train_di, validation_data=(X_test_di, y_test_di), epochs=16, batch_size=1000)\n",
    "nn_di.evaluate(X_test_di, y_test_di)\n",
    "y_nn_pred_di=nn_di.predict(X_test_di).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Model: NN- debiasing with RW - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.748273\n",
      "Test set: Balanced classification accuracy = 0.636596\n",
      "Test set: Disparate impact = 0.401421\n",
      "Test set: Equal opportunity difference = -0.483871\n",
      "Test set: Average odds difference = -0.528388\n",
      "Test set: Theil_index = 0.114568\n"
     ]
    }
   ],
   "source": [
    "dataset_debias_test_nn_di = test_pp_bld.copy(deepcopy = True)\n",
    "dataset_debias_test_nn_di.labels = y_nn_pred_di\n",
    "display(Markdown(\"#### Model: NN- debiasing with RW - classification metrics\"))\n",
    "classified_metric_debiasing_test_nn_di = ClassificationMetric(test_pp_bld, \n",
    "                                                 dataset_debias_test_nn_di,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test_nn_di.accuracy())\n",
    "TPR_nn_di = classified_metric_debiasing_test_nn_di.true_positive_rate()\n",
    "TNR_nn_di = classified_metric_debiasing_test_nn_di.true_negative_rate()\n",
    "bal_acc_debiasing_test_nn_di = 0.5*(TPR_nn_di+TNR_nn_di)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test_nn_di)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test_nn_di.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test_nn_di.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test_nn_di.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test_nn_di.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learning Fair Representation Pre-processing Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 0.6985769994202978, L_x: 0.5090786759673003,  L_y: 0.6388413686980396,  L_z: 0.0044138815627640824\n",
      "step: 250, loss: 0.6985769558576311, L_x: 0.5090787146424105,  L_y: 0.6388413223650936,  L_z: 0.004413881014148258\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1865010/3520606771.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         )\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Transform training data and align features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_pp_bld_lfr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pp_bld\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data2/users/ssultana/anaconda3/envs/aif360/lib/python3.7/site-packages/aif360/algorithms/transformer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mnew_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mnew_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/users/ssultana/anaconda3/envs/aif360/lib/python3.7/site-packages/aif360/algorithms/preprocessing/lfr.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, dataset, maxiter, maxfun, threshold)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \"\"\"\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data2/users/ssultana/anaconda3/envs/aif360/lib/python3.7/site-packages/aif360/algorithms/transformer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mnew_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mnew_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/users/ssultana/anaconda3/envs/aif360/lib/python3.7/site-packages/aif360/algorithms/preprocessing/lfr.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, maxiter, maxfun)\u001b[0m\n\u001b[1;32m    101\u001b[0m                                         self.Ay, self.Az, self.print_interval, self.verbose),\n\u001b[1;32m    102\u001b[0m                                                       \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapprox_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxfun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                                                       maxiter=maxiter, disp=self.verbose)[0]\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearned_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearned_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/users/ssultana/anaconda3/envs/aif360/lib/python3.7/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0;32m--> 198\u001b[0;31m                            **opts)\n\u001b[0m\u001b[1;32m    199\u001b[0m     d = {'grad': res['jac'],\n\u001b[1;32m    200\u001b[0m          \u001b[0;34m'task'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/users/ssultana/anaconda3/envs/aif360/lib/python3.7/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    306\u001b[0m     sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n\u001b[1;32m    307\u001b[0m                                   \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_bounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                                   finite_diff_rel_step=finite_diff_rel_step)\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0mfunc_and_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun_and_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/users/ssultana/anaconda3/envs/aif360/lib/python3.7/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;31m# calculation reduces overall function evaluations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     sf = ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0;32m--> 262\u001b[0;31m                         finite_diff_rel_step, bounds, epsilon=epsilon)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/users/ssultana/anaconda3/envs/aif360/lib/python3.7/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# Hessian Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/users/ssultana/anaconda3/envs/aif360/lib/python3.7/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/users/ssultana/anaconda3/envs/aif360/lib/python3.7/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_grad\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 self.g = approx_derivative(fun_wrapped, self.x, f0=self.f,\n\u001b[0;32m---> 92\u001b[0;31m                                            **finite_diff_options)\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/users/ssultana/anaconda3/envs/aif360/lib/python3.7/site-packages/scipy/optimize/_numdiff.py\u001b[0m in \u001b[0;36mapprox_derivative\u001b[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparsity\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             return _dense_difference(fun_wrapped, x0, f0, h,\n\u001b[0;32m--> 427\u001b[0;31m                                      use_one_sided, method)\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparsity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparsity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/users/ssultana/anaconda3/envs/aif360/lib/python3.7/site-packages/scipy/optimize/_numdiff.py\u001b[0m in \u001b[0;36m_dense_difference\u001b[0;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Recompute dx as exactly representable number.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'3-point'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muse_one_sided\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/users/ssultana/anaconda3/envs/aif360/lib/python3.7/site-packages/scipy/optimize/_numdiff.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             raise RuntimeError(\"`fun` return value has \"\n",
      "\u001b[0;32m/data2/users/ssultana/anaconda3/envs/aif360/lib/python3.7/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnfev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/users/ssultana/anaconda3/envs/aif360/lib/python3.7/site-packages/aif360/algorithms/preprocessing/lfr_helpers/helpers.py\u001b[0m in \u001b[0;36mLFR_optim_objective\u001b[0;34m(parameters, x_unprivileged, x_privileged, y_unprivileged, y_privileged, k, A_x, A_y, A_z, print_interval, verbose)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mM_unprivileged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_hat_unprivileged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat_unprivileged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_xhat_y_hat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprototypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_unprivileged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mM_privileged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_hat_privileged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat_privileged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_xhat_y_hat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprototypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_privileged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_hat_unprivileged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat_privileged\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/users/ssultana/anaconda3/envs/aif360/lib/python3.7/site-packages/aif360/algorithms/preprocessing/lfr_helpers/helpers.py\u001b[0m in \u001b[0;36mget_xhat_y_hat\u001b[0;34m(prototypes, w, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_xhat_y_hat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprototypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprototypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprototypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     y_hat = np.clip(\n",
      "\u001b[0;32m/data2/users/ssultana/anaconda3/envs/aif360/lib/python3.7/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mcdist\u001b[0;34m(XA, XB, metric, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2769\u001b[0m             cdist_fn = getattr(_distance_wrap,\n\u001b[1;32m   2770\u001b[0m                                \"cdist_%s_%s_wrap\" % (metric_name, typ))\n\u001b[0;32m-> 2771\u001b[0;31m             \u001b[0mcdist_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2772\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Input recontruction quality - Ax\n",
    "# Fairness constraint - Az\n",
    "# Output prediction error - Ay\n",
    "    \n",
    "TR = LFR(unprivileged_groups=unprivileged_groups,\n",
    "         privileged_groups=privileged_groups,\n",
    "         k=10, Ax=0.1, Ay=1.0, Az=2.0,\n",
    "         verbose=1\n",
    "        )\n",
    "# Transform training data and align features\n",
    "train_pp_bld_lfr = TR.fit_transform(train_pp_bld, maxiter=5000, maxfun=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Transformed training dataset - Learning Fair Representation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n",
      "Difference in disparate impact between unprivileged and privileged groups = 1.000000\n"
     ]
    }
   ],
   "source": [
    "### Compute fairness metric on transformed dataset-lfr\n",
    "metric_transf_train_lfr = BinaryLabelDatasetMetric(train_pp_bld_lfr, \n",
    "                                               unprivileged_groups=unprivileged_groups,\n",
    "                                               privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Transformed training dataset - Learning Fair Representation\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_train_lfr.mean_difference())\n",
    "print(\"Difference in disparate impact between unprivileged and privileged groups = %f\" % metric_transf_train_lfr.disparate_impact())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply classifiers on Transformed Data(Bias Mitigated Dataset via Learning Fair Representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_transf_lfr = StandardScaler()\n",
    "X_train_lfr = scale_transf_lfr.fit_transform(train_pp_bld_lfr.features)\n",
    "y_train_lfr = train_pp_bld.labels.ravel()\n",
    "\n",
    "dataset_transf_test_pred_lfr = test_pp_bld.copy(deepcopy=True)\n",
    "X_test_lfr = scale_transf_lfr.fit_transform(dataset_transf_test_pred_lfr.features)\n",
    "y_test_lfr = dataset_transf_test_pred_lfr.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Logistic Regression on transformed (LFR) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmod_lfr = LogisticRegression(max_iter=10000, n_jobs = 30, class_weight='balanced', solver='liblinear')\n",
    "lmod_lfr.fit(X_train_lfr, y_train_lfr,sample_weight=train_pp_bld_lfr.instance_weights)\n",
    "y_lr_pred_lfr = lmod_lfr.predict(X_test_lfr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Model: LR- debiasing with lfr - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.539977\n",
      "Test set: Balanced classification accuracy = 0.555877\n",
      "Test set: Disparate impact = 0.753121\n",
      "Test set: Equal opportunity difference = -0.088668\n",
      "Test set: Average odds difference = -0.100114\n",
      "Test set: Theil_index = 0.462495\n"
     ]
    }
   ],
   "source": [
    "dataset_debias_test_lr_lfr = test_pp_bld.copy(deepcopy = True)\n",
    "dataset_debias_test_lr_lfr.labels = y_lr_pred_lfr.ravel()\n",
    "display(Markdown(\"#### Model: LR- debiasing with lfr - classification metrics\"))\n",
    "classified_metric_debiasing_test_lr_lfr = ClassificationMetric(test_pp_bld, \n",
    "                                                 dataset_debias_test_lr_lfr,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test_lr_lfr.accuracy())\n",
    "TPR_lr_lfr = classified_metric_debiasing_test_lr_lfr.true_positive_rate()\n",
    "TNR_lr_lfr = classified_metric_debiasing_test_lr_lfr.true_negative_rate()\n",
    "bal_acc_debiasing_test_lr_lfr = 0.5*(TPR_lr_lfr+TNR_lr_lfr)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test_lr_lfr)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test_lr_lfr.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test_lr_lfr.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test_lr_lfr.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test_lr_lfr.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Random Forest on transformed (LFR) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend ThreadingBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done 100 out of 100 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=30)]: Using backend ThreadingBackend with 30 concurrent workers.\n",
      "[Parallel(n_jobs=30)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "rf_lfr=RandomForestClassifier(n_estimators=100, verbose=1, n_jobs=30).fit(X_train_lfr, y_train_lfr,\n",
    "                                                                            sample_weight=train_pp_bld_lfr.instance_weights)\n",
    "                                                                            \n",
    "y_rf_pred_lfr=rf_lfr.predict(X_test_lfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Model: RF- debiasing with lfr - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.388973\n",
      "Test set: Balanced classification accuracy = 0.498901\n",
      "Test set: Disparate impact = 0.980870\n",
      "Test set: Equal opportunity difference = -0.007870\n",
      "Test set: Average odds difference = -0.005535\n",
      "Test set: Theil_index = 0.822529\n"
     ]
    }
   ],
   "source": [
    "dataset_debias_test_rf_lfr = test_pp_bld.copy(deepcopy = True)\n",
    "dataset_debias_test_rf_lfr.labels = y_rf_pred_lfr\n",
    "display(Markdown(\"#### Model: RF- debiasing with lfr - classification metrics\"))\n",
    "classified_metric_debiasing_test_rf_lfr = ClassificationMetric(test_pp_bld, \n",
    "                                                 dataset_debias_test_rf_lfr,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test_rf_lfr.accuracy())\n",
    "TPR_rf_lfr = classified_metric_debiasing_test_rf_lfr.true_positive_rate()\n",
    "TNR_rf_lfr = classified_metric_debiasing_test_rf_lfr.true_negative_rate()\n",
    "bal_acc_debiasing_test_rf_lfr = 0.5*(TPR_rf_lfr+TNR_rf_lfr)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test_rf_lfr)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test_rf_lfr.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test_rf_lfr.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test_rf_lfr.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test_rf_lfr.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 SVM classifier on transformed (LFR) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM].........WARN: libsvm Solver reached max_iter\n",
      "optimization finished, #iter = 10000\n",
      "obj = -19991.107642, rho = 0.973414\n",
      "nSV = 19998, nBSV = 19996\n",
      "Total nSV = 19998\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "svm_lfr= svm.SVC(kernel='linear', verbose=1, max_iter=10000).fit(X_train_lfr, y_train_lfr,\n",
    "                                                                            sample_weight=train_pp_bld_lfr.instance_weights)\n",
    "                                                                            \n",
    "\n",
    "y_svm_pred_lfr=svm_lfr.predict(X_test_lfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Model: SVM- debiasing with lfr - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.539977\n",
      "Test set: Balanced classification accuracy = 0.555877\n",
      "Test set: Disparate impact = 1.012777\n",
      "Test set: Equal opportunity difference = 0.013461\n",
      "Test set: Average odds difference = 0.010521\n",
      "Test set: Theil_index = 0.072511\n"
     ]
    }
   ],
   "source": [
    "dataset_debias_test_svm_lfr = test_pp_bld.copy(deepcopy = True)\n",
    "dataset_debias_test_svm_lfr.labels = y_svm_pred_di\n",
    "display(Markdown(\"#### Model: SVM- debiasing with lfr - classification metrics\"))\n",
    "classified_metric_debiasing_test_svm_lfr = ClassificationMetric(test_pp_bld, \n",
    "                                                 dataset_debias_test_svm_lfr,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test_lr_lfr.accuracy())\n",
    "TPR_svm_lfr = classified_metric_debiasing_test_svm_lfr.true_positive_rate()\n",
    "TNR_svm_lfr = classified_metric_debiasing_test_svm_lfr.true_negative_rate()\n",
    "bal_acc_debiasing_test_svm_lfr= 0.5*(TPR_lr_lfr+TNR_lr_lfr)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test_svm_lfr)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test_svm_lfr.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test_svm_lfr.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test_svm_lfr.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test_svm_lfr.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Neural Network on tranformed (LFR) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 354400 samples, validate on 88600 samples\n",
      "Epoch 1/16\n",
      "354400/354400 [==============================] - 2s 4us/sample - loss: 0.5872 - accuracy: 0.7109 - val_loss: 1.0264 - val_accuracy: 0.4639\n",
      "Epoch 2/16\n",
      "354400/354400 [==============================] - 1s 4us/sample - loss: 0.5688 - accuracy: 0.7245 - val_loss: 1.0162 - val_accuracy: 0.4661\n",
      "Epoch 3/16\n",
      "354400/354400 [==============================] - 1s 4us/sample - loss: 0.5687 - accuracy: 0.7247 - val_loss: 1.0086 - val_accuracy: 0.4719\n",
      "Epoch 4/16\n",
      "354400/354400 [==============================] - 1s 4us/sample - loss: 0.5688 - accuracy: 0.7247 - val_loss: 1.0048 - val_accuracy: 0.4768\n",
      "Epoch 5/16\n",
      "354400/354400 [==============================] - 1s 4us/sample - loss: 0.5687 - accuracy: 0.7247 - val_loss: 0.9875 - val_accuracy: 0.4831\n",
      "Epoch 6/16\n",
      "354400/354400 [==============================] - 1s 4us/sample - loss: 0.5686 - accuracy: 0.7247 - val_loss: 0.9843 - val_accuracy: 0.4882\n",
      "Epoch 7/16\n",
      "354400/354400 [==============================] - 1s 4us/sample - loss: 0.5687 - accuracy: 0.7247 - val_loss: 0.9792 - val_accuracy: 0.4916\n",
      "Epoch 8/16\n",
      "354400/354400 [==============================] - 1s 4us/sample - loss: 0.5687 - accuracy: 0.7247 - val_loss: 0.9688 - val_accuracy: 0.4964\n",
      "Epoch 9/16\n",
      "354400/354400 [==============================] - 1s 4us/sample - loss: 0.5686 - accuracy: 0.7247 - val_loss: 0.9731 - val_accuracy: 0.4984\n",
      "Epoch 10/16\n",
      "354400/354400 [==============================] - 1s 4us/sample - loss: 0.5686 - accuracy: 0.7247 - val_loss: 0.9710 - val_accuracy: 0.5013\n",
      "Epoch 11/16\n",
      "354400/354400 [==============================] - 1s 4us/sample - loss: 0.5686 - accuracy: 0.7247 - val_loss: 0.9624 - val_accuracy: 0.5053\n",
      "Epoch 12/16\n",
      "354400/354400 [==============================] - 1s 4us/sample - loss: 0.5686 - accuracy: 0.7247 - val_loss: 0.9740 - val_accuracy: 0.5071\n",
      "Epoch 13/16\n",
      "354400/354400 [==============================] - 1s 4us/sample - loss: 0.5686 - accuracy: 0.7247 - val_loss: 0.9690 - val_accuracy: 0.5096\n",
      "Epoch 14/16\n",
      "354400/354400 [==============================] - 1s 4us/sample - loss: 0.5686 - accuracy: 0.7247 - val_loss: 0.9653 - val_accuracy: 0.5285\n",
      "Epoch 15/16\n",
      "354400/354400 [==============================] - 1s 4us/sample - loss: 0.5685 - accuracy: 0.7247 - val_loss: 0.9692 - val_accuracy: 0.5296\n",
      "Epoch 16/16\n",
      "354400/354400 [==============================] - 2s 5us/sample - loss: 0.5685 - accuracy: 0.7247 - val_loss: 0.9667 - val_accuracy: 0.5302\n"
     ]
    }
   ],
   "source": [
    "#Neural Network\n",
    "\n",
    "nn_lfr = Sequential()\n",
    "nn_lfr.add(Dense(12, input_dim=129, activation='relu'))\n",
    "nn_lfr.add(Dense(8, activation='relu'))\n",
    "nn_lfr.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "nn_lfr.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "nn_lfr.fit(X_train_lfr, y_train_lfr, validation_data=(X_test_lfr, y_test_lfr), epochs=16, batch_size=1000)\n",
    "nn_lfr.evaluate(X_test_lfr, y_test_lfr)\n",
    "y_nn_pred_lfr=nn_lfr.predict(X_test_lfr).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Model: NN- debiasing with lfr - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.530169\n",
      "Test set: Balanced classification accuracy = 0.505680\n",
      "Test set: Disparate impact = 1.010694\n",
      "Test set: Equal opportunity difference = 0.043368\n",
      "Test set: Average odds difference = 0.008013\n",
      "Test set: Theil_index = 0.418910\n"
     ]
    }
   ],
   "source": [
    "dataset_debias_test_nn_lfr = test_pp_bld.copy(deepcopy = True)\n",
    "dataset_debias_test_nn_lfr.labels = y_nn_pred_lfr\n",
    "display(Markdown(\"#### Model: NN- debiasing with lfr - classification metrics\"))\n",
    "classified_metric_debiasing_test_nn_lfr = ClassificationMetric(test_pp_bld, \n",
    "                                                 dataset_debias_test_nn_lfr,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test_nn_lfr.accuracy())\n",
    "TPR_nn_lfr = classified_metric_debiasing_test_nn_lfr.true_positive_rate()\n",
    "TNR_nn_lfr = classified_metric_debiasing_test_nn_lfr.true_negative_rate()\n",
    "bal_acc_debiasing_test_nn_lfr = 0.5*(TPR_nn_lfr+TNR_nn_lfr)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test_nn_lfr)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test_nn_lfr.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test_nn_lfr.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test_nn_lfr.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test_nn_lfr.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optim Pre-processing Algorithm\n",
    "\n",
    "Optimized preprocessing is a preprocessing technique that learns a probabilistic transformation that edits the features and labels in the data with group fairness, individual distortion, and data fidelity constraints and objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim_options = {\n",
    "#             \"distortion_fun\": get_distortion_german,\n",
    "#             \"epsilon\": 0.05,\n",
    "#             # \"clist\": [0.99],\n",
    "#             \"dlist\": [.1]\n",
    "#         }\n",
    "# OP = OptimPreproc(OptTools, optim_options)\n",
    "\n",
    "# OP = OP.fit(train_pp_bld)\n",
    "# train_pp_bld_op = OP.transform(train_pp_bld, transform_Y=True)\n",
    "\n",
    "# train_pp_bld_op = train_pp_bld.align_datasets(train_pp_bld_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result (Pre-Processing Algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Accuracy on Test Set [Preprocessing Algorithm:]:')\n",
    "\n",
    "# result_acc = pd.DataFrame({'Logistic Regression': [acc_lr_biased, \n",
    "#                                                 acc_lr_rw,\n",
    "#                                                 acc_lr_di,\n",
    "#                                                 acc_lr_lfr],\n",
    "#                         'Random Forest':        [acc_rf_biased,\n",
    "#                                                 acc_rf_rw,\n",
    "#                                                 acc_rf_di,\n",
    "#                                                 acc_rf_lfr],\n",
    "#                         'SVM':                  [acc_svm_biased,\n",
    "#                                                 acc_svm_rw,\n",
    "#                                                 acc_svm_di,\n",
    "#                                                 acc_svm_lfr],\n",
    "#                         'Neural Network':       [acc_nn_biased,\n",
    "#                                                 acc_nn_rw,\n",
    "#                                                 acc_nn_di,\n",
    "#                                                 acc_nn_lfr]\n",
    "#                         }, \n",
    "#                      index =['Biased', 'Reweighing', 'Disparate Impact Remover', 'Learning Fair Representation'])\n",
    "# result_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data\n",
    "Information_RW = {'Classifier': [\"Logistic Regression\", \"\", \"\",\"\", \"\", \"\", \n",
    "                            \"Random Forest\", \"\", \"\", \"\", \"\", \"\",\n",
    "                            \"SVM\", \"\", \"\", \"\", \"\", \"\", \n",
    "                            \"Neural Network\", \"\", \"\", \"\", \"\", \"\"],\n",
    "                \n",
    "               'Metrics': [\"Classification accuracy\", \" Balanced classification accuracy\", \"Disparate impact\", \"Equal opportunity difference\", \"Average odds difference\", \"Theil_index\",\n",
    "                        \"Classification accuracy\", \" Balanced classification accuracy\", \"Disparate impact\", \"Equal opportunity difference\", \"Average odds difference\", \"Theil_index\",\n",
    "                        \"Classification accuracy\", \" Balanced classification accuracy\", \"Disparate impact\", \"Equal opportunity difference\", \"Average odds difference\", \"Theil_index\",\n",
    "                        \"Classification accuracy\", \" Balanced classification accuracy\", \"Disparate impact\", \"Equal opportunity difference\", \"Average odds difference\", \"Theil_index\"\n",
    "                        ],\n",
    "                \n",
    "               'Score': [classified_metric_debiasing_test_lr_rw.accuracy(), bal_acc_debiasing_test_lr_rw, classified_metric_debiasing_test_lr_rw.disparate_impact(), classified_metric_debiasing_test_lr_rw.equal_opportunity_difference(), classified_metric_debiasing_test_lr_rw.average_odds_difference(), classified_metric_debiasing_test_lr_rw.theil_index(), \n",
    "                        classified_metric_debiasing_test_rf_rw.accuracy(), bal_acc_debiasing_test_rf_rw, classified_metric_debiasing_test_rf_rw.disparate_impact(), classified_metric_debiasing_test_rf_rw.equal_opportunity_difference(), classified_metric_debiasing_test_rf_rw.average_odds_difference(), classified_metric_debiasing_test_rf_rw.theil_index(), \n",
    "                        classified_metric_debiasing_test_svm_rw.accuracy(), bal_acc_debiasing_test_svm_rw, classified_metric_debiasing_test_svm_rw.disparate_impact(), classified_metric_debiasing_test_svm_rw.equal_opportunity_difference(), classified_metric_debiasing_test_svm_rw.average_odds_difference(), classified_metric_debiasing_test_svm_rw.theil_index(),  \n",
    "                        classified_metric_debiasing_test_nn_rw.accuracy(), bal_acc_debiasing_test_nn_rw, classified_metric_debiasing_test_nn_rw.disparate_impact(), classified_metric_debiasing_test_nn_rw.equal_opportunity_difference(), classified_metric_debiasing_test_nn_rw.average_odds_difference(), classified_metric_debiasing_test_rf_rw.theil_index()\n",
    "                        ]}\n",
    " \n",
    "# Dataframing the whole data\n",
    "df_RW = pd.DataFrame(Information_RW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Comparisons based on DI Preprocessing Algorithm:]:')\n",
    "# Creating data\n",
    "Information_DI = {'Classifier': [\"Logistic Regression\", \"\", \"\",\"\", \"\", \"\", \n",
    "                            \"Random Forest\", \"\", \"\", \"\", \"\", \"\",\n",
    "                            \"SVM\", \"\", \"\", \"\", \"\", \"\", \n",
    "                            \"Neural Network\", \"\", \"\", \"\", \"\", \"\"],\n",
    "                \n",
    "               'Metrics': [\"Classification accuracy\", \" Balanced classification accuracy\", \"Disparate impact\", \"Equal opportunity difference\", \"Average odds difference\", \"Theil_index\",\n",
    "                        \"Classification accuracy\", \" Balanced classification accuracy\", \"Disparate impact\", \"Equal opportunity difference\", \"Average odds difference\", \"Theil_index\",\n",
    "                        \"Classification accuracy\", \" Balanced classification accuracy\", \"Disparate impact\", \"Equal opportunity difference\", \"Average odds difference\", \"Theil_index\",\n",
    "                        \"Classification accuracy\", \" Balanced classification accuracy\", \"Disparate impact\", \"Equal opportunity difference\", \"Average odds difference\", \"Theil_index\"\n",
    "                        ],\n",
    "                \n",
    "               'Score': [classified_metric_debiasing_test_lr_di.accuracy(), bal_acc_debiasing_test_lr_di, classified_metric_debiasing_test_lr_di.disparate_impact(), classified_metric_debiasing_test_lr_di.equal_opportunity_difference(), classified_metric_debiasing_test_lr_di.average_odds_difference(), classified_metric_debiasing_test_lr_di.theil_index(), \n",
    "                        classified_metric_debiasing_test_rf_di.accuracy(), bal_acc_debiasing_test_rf_di, classified_metric_debiasing_test_rf_di.disparate_impact(), classified_metric_debiasing_test_rf_di.equal_opportunity_difference(), classified_metric_debiasing_test_rf_di.average_odds_difference(), classified_metric_debiasing_test_rf_di.theil_index(), \n",
    "                        classified_metric_debiasing_test_svm_di.accuracy(), bal_acc_debiasing_test_svm_di, classified_metric_debiasing_test_svm_di.disparate_impact(), classified_metric_debiasing_test_svm_di.equal_opportunity_difference(), classified_metric_debiasing_test_svm_di.average_odds_difference(), classified_metric_debiasing_test_svm_di.theil_index(),  \n",
    "                        classified_metric_debiasing_test_nn_di.accuracy(), bal_acc_debiasing_test_nn_di, classified_metric_debiasing_test_nn_di.disparate_impact(), classified_metric_debiasing_test_nn_di.equal_opportunity_difference(), classified_metric_debiasing_test_nn_di.average_odds_difference(), classified_metric_debiasing_test_rf_di.theil_index()\n",
    "                        ]}\n",
    " \n",
    "# Dataframing the whole data\n",
    "df_DI = pd.DataFrame(Information_DI)\n",
    " \n",
    "# # Showing the above data\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#     print(df_DI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Comparisons based on Reweighing Preprocessing Algorithm:]:')\n",
    "# Creating data\n",
    "Information_lfr = {'Classifier': [\"Logistic Regression\", \"\", \"\",\"\", \"\", \"\", \n",
    "                            \"Random Forest\", \"\", \"\", \"\", \"\", \"\",\n",
    "                            \"SVM\", \"\", \"\", \"\", \"\", \"\", \n",
    "                            \"Neural Network\", \"\", \"\", \"\", \"\", \"\"],\n",
    "                \n",
    "               'Metrics': [\"Classification accuracy\", \" Balanced classification accuracy\", \"Disparate impact\", \"Equal opportunity difference\", \"Average odds difference\", \"Theil_index\",\n",
    "                        \"Classification accuracy\", \" Balanced classification accuracy\", \"Disparate impact\", \"Equal opportunity difference\", \"Average odds difference\", \"Theil_index\",\n",
    "                        \"Classification accuracy\", \" Balanced classification accuracy\", \"Disparate impact\", \"Equal opportunity difference\", \"Average odds difference\", \"Theil_index\",\n",
    "                        \"Classification accuracy\", \" Balanced classification accuracy\", \"Disparate impact\", \"Equal opportunity difference\", \"Average odds difference\", \"Theil_index\"\n",
    "                        ],\n",
    "                \n",
    "               'Score': [classified_metric_debiasing_test_lr_lfr.accuracy(), bal_acc_debiasing_test_lr_lfr, classified_metric_debiasing_test_lr_lfr.disparate_impact(), classified_metric_debiasing_test_lr_lfr.equal_opportunity_difference(), classified_metric_debiasing_test_lr_lfr.average_odds_difference(), classified_metric_debiasing_test_lr_lfr.theil_index(), \n",
    "                        classified_metric_debiasing_test_rf_lfr.accuracy(), bal_acc_debiasing_test_rf_lfr, classified_metric_debiasing_test_rf_lfr.disparate_impact(), classified_metric_debiasing_test_rf_lfr.equal_opportunity_difference(), classified_metric_debiasing_test_rf_lfr.average_odds_difference(), classified_metric_debiasing_test_rf_lfr.theil_index(), \n",
    "                        classified_metric_debiasing_test_svm_lfr.accuracy(), bal_acc_debiasing_test_svm_lfr, classified_metric_debiasing_test_svm_lfr.disparate_impact(), classified_metric_debiasing_test_svm_lfr.equal_opportunity_difference(), classified_metric_debiasing_test_svm_lfr.average_odds_difference(), classified_metric_debiasing_test_svm_lfr.theil_index(),  \n",
    "                        classified_metric_debiasing_test_nn_lfr.accuracy(), bal_acc_debiasing_test_nn_lfr, classified_metric_debiasing_test_nn_lfr.disparate_impact(), classified_metric_debiasing_test_nn_lfr.equal_opportunity_difference(), classified_metric_debiasing_test_nn_lfr.average_odds_difference(), classified_metric_debiasing_test_rf_lfr.theil_index()\n",
    "                        ]}\n",
    " \n",
    "# Dataframing the whole data\n",
    "df_LFR = pd.DataFrame(Information_lfr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparisons based on Reweighing Preprocessing Algorithm:]:\n",
      "\n",
      "             Classifier                            Metrics     Score\n",
      "0   Logistic Regression            Classification accuracy  0.707280\n",
      "1                         Balanced classification accuracy  0.524883\n",
      "2                                         Disparate impact  0.971971\n",
      "3                             Equal opportunity difference -0.006803\n",
      "4                                  Average odds difference -0.013391\n",
      "5                                              Theil_index  0.078282\n",
      "6         Random Forest            Classification accuracy  0.753894\n",
      "7                         Balanced classification accuracy  0.616837\n",
      "8                                         Disparate impact  0.747663\n",
      "9                             Equal opportunity difference -0.151578\n",
      "10                                 Average odds difference -0.176421\n",
      "11                                             Theil_index  0.086497\n",
      "12                  SVM            Classification accuracy  0.695530\n",
      "13                        Balanced classification accuracy  0.524883\n",
      "14                                        Disparate impact  1.009895\n",
      "15                            Equal opportunity difference  0.011926\n",
      "16                                 Average odds difference  0.007328\n",
      "17                                             Theil_index  0.070181\n",
      "18       Neural Network            Classification accuracy  0.754120\n",
      "19                        Balanced classification accuracy  0.635954\n",
      "20                                        Disparate impact  0.578927\n",
      "21                            Equal opportunity difference -0.301540\n",
      "22                                 Average odds difference -0.332413\n",
      "23                                             Theil_index  0.086497\n",
      "\n",
      "\n",
      "Comparisons based on Disparate Impact Preprocessing Algorithm:]:\n",
      "\n",
      "             Classifier                            Metrics     Score\n",
      "0   Logistic Regression            Classification accuracy  0.614571\n",
      "1                         Balanced classification accuracy  0.622482\n",
      "2                                         Disparate impact  0.565951\n",
      "3                             Equal opportunity difference -0.225645\n",
      "4                                  Average odds difference -0.190442\n",
      "5                                              Theil_index  0.368759\n",
      "6         Random Forest            Classification accuracy  0.754391\n",
      "7                         Balanced classification accuracy  0.616989\n",
      "8                                         Disparate impact  0.755492\n",
      "9                             Equal opportunity difference -0.141304\n",
      "10                                 Average odds difference -0.168260\n",
      "11                                             Theil_index  0.085778\n",
      "12                  SVM            Classification accuracy  0.694199\n",
      "13                        Balanced classification accuracy  0.622482\n",
      "14                                        Disparate impact  1.012777\n",
      "15                            Equal opportunity difference  0.013461\n",
      "16                                 Average odds difference  0.010521\n",
      "17                                             Theil_index  0.072511\n",
      "18       Neural Network            Classification accuracy  0.748679\n",
      "19                        Balanced classification accuracy  0.635655\n",
      "20                                        Disparate impact  0.416921\n",
      "21                            Equal opportunity difference -0.468727\n",
      "22                                 Average odds difference -0.507161\n",
      "23                                             Theil_index  0.085778\n",
      "\n",
      "\n",
      "Comparisons based on Learning Fair Representation Preprocessing Algorithm:]:\n",
      "\n",
      "             Classifier                            Metrics     Score\n",
      "0   Logistic Regression            Classification accuracy  0.539977\n",
      "1                         Balanced classification accuracy  0.555877\n",
      "2                                         Disparate impact  0.753121\n",
      "3                             Equal opportunity difference -0.088668\n",
      "4                                  Average odds difference -0.100114\n",
      "5                                              Theil_index  0.462495\n",
      "6         Random Forest            Classification accuracy  0.388973\n",
      "7                         Balanced classification accuracy  0.498901\n",
      "8                                         Disparate impact  0.980870\n",
      "9                             Equal opportunity difference -0.007870\n",
      "10                                 Average odds difference -0.005535\n",
      "11                                             Theil_index  0.822529\n",
      "12                  SVM            Classification accuracy  0.694199\n",
      "13                        Balanced classification accuracy  0.555877\n",
      "14                                        Disparate impact  1.012777\n",
      "15                            Equal opportunity difference  0.013461\n",
      "16                                 Average odds difference  0.010521\n",
      "17                                             Theil_index  0.072511\n",
      "18       Neural Network            Classification accuracy  0.530169\n",
      "19                        Balanced classification accuracy  0.505680\n",
      "20                                        Disparate impact  1.010694\n",
      "21                            Equal opportunity difference  0.043368\n",
      "22                                 Average odds difference  0.008013\n",
      "23                                             Theil_index  0.822529\n"
     ]
    }
   ],
   "source": [
    "# Showing the above data\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print('Comparisons based on Reweighing Preprocessing Algorithm:]:\\n')\n",
    "    print(df_RW)\n",
    "    print ('\\n')\n",
    "    print('Comparisons based on Disparate Impact Preprocessing Algorithm:]:\\n')\n",
    "    print(df_DI)\n",
    "    print ('\\n')\n",
    "    print('Comparisons based on Learning Fair Representation Preprocessing Algorithm:]:\\n')\n",
    "    print(df_LFR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-processing Algorithms: \n",
    "\n",
    "With in-processing techniques, we want to create a classifier that is explicitly aware of our fairness goals. That is, in training the classifier, it is not enough to simply optimize for accuracy on the training data. Instead, we modify the loss function to account simultaneously for our two goals: our model should be both accurate and fair.\n",
    "\n",
    "1. Adversial Debiasing In-processing algorithm\n",
    "2. Meta Fair Classifier\n",
    "3. Prejudice Remover\n",
    "4. Exponentiated Gradient Reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Adversial Debiasing In-processing Algorithm\n",
    "Adversarial debiasing is an in-processing technique that learns a classifier to maximize prediction accuracy and simultaneously reduce an adversary’s ability to determine the protected attribute from the predictions [5]. This approach leads to a fair classifier as the predictions cannot carry any group discrimination information that the adversary can exploit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn plane model without debiasing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-13 11:36:29.427514: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-06-13 11:36:29.427565: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "plain_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='plain_classifier',\n",
    "                          debias=False,\n",
    "                          sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /data2/users/ssultana/anaconda3/envs/aif360/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-13 11:36:34.173478: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.677886\n",
      "epoch 0; iter: 200; batch classifier loss: 0.555166\n",
      "epoch 0; iter: 400; batch classifier loss: 0.577297\n",
      "epoch 0; iter: 600; batch classifier loss: 0.632255\n",
      "epoch 0; iter: 800; batch classifier loss: 0.633876\n",
      "epoch 0; iter: 1000; batch classifier loss: 0.569676\n",
      "epoch 0; iter: 1200; batch classifier loss: 0.584164\n",
      "epoch 0; iter: 1400; batch classifier loss: 0.558872\n",
      "epoch 0; iter: 1600; batch classifier loss: 0.536281\n",
      "epoch 0; iter: 1800; batch classifier loss: 0.525641\n",
      "epoch 0; iter: 2000; batch classifier loss: 0.590979\n",
      "epoch 0; iter: 2200; batch classifier loss: 0.562403\n",
      "epoch 0; iter: 2400; batch classifier loss: 0.548421\n",
      "epoch 0; iter: 2600; batch classifier loss: 0.482196\n",
      "epoch 1; iter: 0; batch classifier loss: 0.516783\n",
      "epoch 1; iter: 200; batch classifier loss: 0.518328\n",
      "epoch 1; iter: 400; batch classifier loss: 0.564152\n",
      "epoch 1; iter: 600; batch classifier loss: 0.583995\n",
      "epoch 1; iter: 800; batch classifier loss: 0.506066\n",
      "epoch 1; iter: 1000; batch classifier loss: 0.603193\n",
      "epoch 1; iter: 1200; batch classifier loss: 0.593023\n",
      "epoch 1; iter: 1400; batch classifier loss: 0.567432\n",
      "epoch 1; iter: 1600; batch classifier loss: 0.562213\n",
      "epoch 1; iter: 1800; batch classifier loss: 0.530124\n",
      "epoch 1; iter: 2000; batch classifier loss: 0.553774\n",
      "epoch 1; iter: 2200; batch classifier loss: 0.487579\n",
      "epoch 1; iter: 2400; batch classifier loss: 0.527020\n",
      "epoch 1; iter: 2600; batch classifier loss: 0.538488\n",
      "epoch 2; iter: 0; batch classifier loss: 0.557887\n",
      "epoch 2; iter: 200; batch classifier loss: 0.601860\n",
      "epoch 2; iter: 400; batch classifier loss: 0.445793\n",
      "epoch 2; iter: 600; batch classifier loss: 0.514454\n",
      "epoch 2; iter: 800; batch classifier loss: 0.524190\n",
      "epoch 2; iter: 1000; batch classifier loss: 0.598609\n",
      "epoch 2; iter: 1200; batch classifier loss: 0.499609\n",
      "epoch 2; iter: 1400; batch classifier loss: 0.560072\n",
      "epoch 2; iter: 1600; batch classifier loss: 0.535621\n",
      "epoch 2; iter: 1800; batch classifier loss: 0.581779\n",
      "epoch 2; iter: 2000; batch classifier loss: 0.556143\n",
      "epoch 2; iter: 2200; batch classifier loss: 0.561380\n",
      "epoch 2; iter: 2400; batch classifier loss: 0.527860\n",
      "epoch 2; iter: 2600; batch classifier loss: 0.515875\n",
      "epoch 3; iter: 0; batch classifier loss: 0.498468\n",
      "epoch 3; iter: 200; batch classifier loss: 0.606347\n",
      "epoch 3; iter: 400; batch classifier loss: 0.492497\n",
      "epoch 3; iter: 600; batch classifier loss: 0.460391\n",
      "epoch 3; iter: 800; batch classifier loss: 0.451815\n",
      "epoch 3; iter: 1000; batch classifier loss: 0.527861\n",
      "epoch 3; iter: 1200; batch classifier loss: 0.525122\n",
      "epoch 3; iter: 1400; batch classifier loss: 0.578572\n",
      "epoch 3; iter: 1600; batch classifier loss: 0.537815\n",
      "epoch 3; iter: 1800; batch classifier loss: 0.513260\n",
      "epoch 3; iter: 2000; batch classifier loss: 0.574200\n",
      "epoch 3; iter: 2200; batch classifier loss: 0.579992\n",
      "epoch 3; iter: 2400; batch classifier loss: 0.550096\n",
      "epoch 3; iter: 2600; batch classifier loss: 0.527340\n",
      "epoch 4; iter: 0; batch classifier loss: 0.581209\n",
      "epoch 4; iter: 200; batch classifier loss: 0.469796\n",
      "epoch 4; iter: 400; batch classifier loss: 0.467292\n",
      "epoch 4; iter: 600; batch classifier loss: 0.578009\n",
      "epoch 4; iter: 800; batch classifier loss: 0.464941\n",
      "epoch 4; iter: 1000; batch classifier loss: 0.526059\n",
      "epoch 4; iter: 1200; batch classifier loss: 0.529924\n",
      "epoch 4; iter: 1400; batch classifier loss: 0.520732\n",
      "epoch 4; iter: 1600; batch classifier loss: 0.517413\n",
      "epoch 4; iter: 1800; batch classifier loss: 0.623249\n",
      "epoch 4; iter: 2000; batch classifier loss: 0.479192\n",
      "epoch 4; iter: 2200; batch classifier loss: 0.529257\n",
      "epoch 4; iter: 2400; batch classifier loss: 0.485040\n",
      "epoch 4; iter: 2600; batch classifier loss: 0.457713\n",
      "epoch 5; iter: 0; batch classifier loss: 0.499901\n",
      "epoch 5; iter: 200; batch classifier loss: 0.484812\n",
      "epoch 5; iter: 400; batch classifier loss: 0.559944\n",
      "epoch 5; iter: 600; batch classifier loss: 0.485331\n",
      "epoch 5; iter: 800; batch classifier loss: 0.606992\n",
      "epoch 5; iter: 1000; batch classifier loss: 0.480343\n",
      "epoch 5; iter: 1200; batch classifier loss: 0.548735\n",
      "epoch 5; iter: 1400; batch classifier loss: 0.643737\n",
      "epoch 5; iter: 1600; batch classifier loss: 0.592433\n",
      "epoch 5; iter: 1800; batch classifier loss: 0.582663\n",
      "epoch 5; iter: 2000; batch classifier loss: 0.633479\n",
      "epoch 5; iter: 2200; batch classifier loss: 0.541237\n",
      "epoch 5; iter: 2400; batch classifier loss: 0.527516\n",
      "epoch 5; iter: 2600; batch classifier loss: 0.510194\n",
      "epoch 6; iter: 0; batch classifier loss: 0.616316\n",
      "epoch 6; iter: 200; batch classifier loss: 0.588652\n",
      "epoch 6; iter: 400; batch classifier loss: 0.488545\n",
      "epoch 6; iter: 600; batch classifier loss: 0.537401\n",
      "epoch 6; iter: 800; batch classifier loss: 0.512021\n",
      "epoch 6; iter: 1000; batch classifier loss: 0.525917\n",
      "epoch 6; iter: 1200; batch classifier loss: 0.555893\n",
      "epoch 6; iter: 1400; batch classifier loss: 0.500647\n",
      "epoch 6; iter: 1600; batch classifier loss: 0.481392\n",
      "epoch 6; iter: 1800; batch classifier loss: 0.543246\n",
      "epoch 6; iter: 2000; batch classifier loss: 0.542667\n",
      "epoch 6; iter: 2200; batch classifier loss: 0.628155\n",
      "epoch 6; iter: 2400; batch classifier loss: 0.522251\n",
      "epoch 6; iter: 2600; batch classifier loss: 0.470269\n",
      "epoch 7; iter: 0; batch classifier loss: 0.571956\n",
      "epoch 7; iter: 200; batch classifier loss: 0.540903\n",
      "epoch 7; iter: 400; batch classifier loss: 0.546830\n",
      "epoch 7; iter: 600; batch classifier loss: 0.454611\n",
      "epoch 7; iter: 800; batch classifier loss: 0.493633\n",
      "epoch 7; iter: 1000; batch classifier loss: 0.554975\n",
      "epoch 7; iter: 1200; batch classifier loss: 0.525934\n",
      "epoch 7; iter: 1400; batch classifier loss: 0.548920\n",
      "epoch 7; iter: 1600; batch classifier loss: 0.525316\n",
      "epoch 7; iter: 1800; batch classifier loss: 0.594271\n",
      "epoch 7; iter: 2000; batch classifier loss: 0.550478\n",
      "epoch 7; iter: 2200; batch classifier loss: 0.523096\n",
      "epoch 7; iter: 2400; batch classifier loss: 0.543489\n",
      "epoch 7; iter: 2600; batch classifier loss: 0.505206\n",
      "epoch 8; iter: 0; batch classifier loss: 0.555660\n",
      "epoch 8; iter: 200; batch classifier loss: 0.506982\n",
      "epoch 8; iter: 400; batch classifier loss: 0.524954\n",
      "epoch 8; iter: 600; batch classifier loss: 0.549073\n",
      "epoch 8; iter: 800; batch classifier loss: 0.539677\n",
      "epoch 8; iter: 1000; batch classifier loss: 0.541013\n",
      "epoch 8; iter: 1200; batch classifier loss: 0.528595\n",
      "epoch 8; iter: 1400; batch classifier loss: 0.556313\n",
      "epoch 8; iter: 1600; batch classifier loss: 0.593200\n",
      "epoch 8; iter: 1800; batch classifier loss: 0.614160\n",
      "epoch 8; iter: 2000; batch classifier loss: 0.507406\n",
      "epoch 8; iter: 2200; batch classifier loss: 0.528535\n",
      "epoch 8; iter: 2400; batch classifier loss: 0.492486\n",
      "epoch 8; iter: 2600; batch classifier loss: 0.580484\n",
      "epoch 9; iter: 0; batch classifier loss: 0.539739\n",
      "epoch 9; iter: 200; batch classifier loss: 0.530322\n",
      "epoch 9; iter: 400; batch classifier loss: 0.517939\n",
      "epoch 9; iter: 600; batch classifier loss: 0.487191\n",
      "epoch 9; iter: 800; batch classifier loss: 0.561559\n",
      "epoch 9; iter: 1000; batch classifier loss: 0.536554\n",
      "epoch 9; iter: 1200; batch classifier loss: 0.469916\n",
      "epoch 9; iter: 1400; batch classifier loss: 0.543924\n",
      "epoch 9; iter: 1600; batch classifier loss: 0.506375\n",
      "epoch 9; iter: 1800; batch classifier loss: 0.456515\n",
      "epoch 9; iter: 2000; batch classifier loss: 0.508512\n",
      "epoch 9; iter: 2200; batch classifier loss: 0.499306\n",
      "epoch 9; iter: 2400; batch classifier loss: 0.513589\n",
      "epoch 9; iter: 2600; batch classifier loss: 0.532811\n",
      "epoch 10; iter: 0; batch classifier loss: 0.519404\n",
      "epoch 10; iter: 200; batch classifier loss: 0.516632\n",
      "epoch 10; iter: 400; batch classifier loss: 0.538341\n",
      "epoch 10; iter: 600; batch classifier loss: 0.561872\n",
      "epoch 10; iter: 800; batch classifier loss: 0.513539\n",
      "epoch 10; iter: 1000; batch classifier loss: 0.514261\n",
      "epoch 10; iter: 1200; batch classifier loss: 0.519517\n",
      "epoch 10; iter: 1400; batch classifier loss: 0.534855\n",
      "epoch 10; iter: 1600; batch classifier loss: 0.552759\n",
      "epoch 10; iter: 1800; batch classifier loss: 0.607937\n",
      "epoch 10; iter: 2000; batch classifier loss: 0.510154\n",
      "epoch 10; iter: 2200; batch classifier loss: 0.621720\n",
      "epoch 10; iter: 2400; batch classifier loss: 0.489879\n",
      "epoch 10; iter: 2600; batch classifier loss: 0.643185\n",
      "epoch 11; iter: 0; batch classifier loss: 0.463975\n",
      "epoch 11; iter: 200; batch classifier loss: 0.520657\n",
      "epoch 11; iter: 400; batch classifier loss: 0.546907\n",
      "epoch 11; iter: 600; batch classifier loss: 0.525802\n",
      "epoch 11; iter: 800; batch classifier loss: 0.568534\n",
      "epoch 11; iter: 1000; batch classifier loss: 0.526842\n",
      "epoch 11; iter: 1200; batch classifier loss: 0.580932\n",
      "epoch 11; iter: 1400; batch classifier loss: 0.567758\n",
      "epoch 11; iter: 1600; batch classifier loss: 0.553641\n",
      "epoch 11; iter: 1800; batch classifier loss: 0.607840\n",
      "epoch 11; iter: 2000; batch classifier loss: 0.563997\n",
      "epoch 11; iter: 2200; batch classifier loss: 0.517839\n",
      "epoch 11; iter: 2400; batch classifier loss: 0.593981\n",
      "epoch 11; iter: 2600; batch classifier loss: 0.589011\n",
      "epoch 12; iter: 0; batch classifier loss: 0.513769\n",
      "epoch 12; iter: 200; batch classifier loss: 0.551321\n",
      "epoch 12; iter: 400; batch classifier loss: 0.457762\n",
      "epoch 12; iter: 600; batch classifier loss: 0.566885\n",
      "epoch 12; iter: 800; batch classifier loss: 0.513623\n",
      "epoch 12; iter: 1000; batch classifier loss: 0.548749\n",
      "epoch 12; iter: 1200; batch classifier loss: 0.619235\n",
      "epoch 12; iter: 1400; batch classifier loss: 0.540637\n",
      "epoch 12; iter: 1600; batch classifier loss: 0.519169\n",
      "epoch 12; iter: 1800; batch classifier loss: 0.584945\n",
      "epoch 12; iter: 2000; batch classifier loss: 0.499773\n",
      "epoch 12; iter: 2200; batch classifier loss: 0.507574\n",
      "epoch 12; iter: 2400; batch classifier loss: 0.493081\n",
      "epoch 12; iter: 2600; batch classifier loss: 0.499002\n",
      "epoch 13; iter: 0; batch classifier loss: 0.490259\n",
      "epoch 13; iter: 200; batch classifier loss: 0.503789\n",
      "epoch 13; iter: 400; batch classifier loss: 0.594088\n",
      "epoch 13; iter: 600; batch classifier loss: 0.591243\n",
      "epoch 13; iter: 800; batch classifier loss: 0.474502\n",
      "epoch 13; iter: 1000; batch classifier loss: 0.495518\n",
      "epoch 13; iter: 1200; batch classifier loss: 0.502214\n",
      "epoch 13; iter: 1400; batch classifier loss: 0.612688\n",
      "epoch 13; iter: 1600; batch classifier loss: 0.559027\n",
      "epoch 13; iter: 1800; batch classifier loss: 0.572585\n",
      "epoch 13; iter: 2000; batch classifier loss: 0.600536\n",
      "epoch 13; iter: 2200; batch classifier loss: 0.538015\n",
      "epoch 13; iter: 2400; batch classifier loss: 0.510866\n",
      "epoch 13; iter: 2600; batch classifier loss: 0.536075\n",
      "epoch 14; iter: 0; batch classifier loss: 0.583950\n",
      "epoch 14; iter: 200; batch classifier loss: 0.551862\n",
      "epoch 14; iter: 400; batch classifier loss: 0.535718\n",
      "epoch 14; iter: 600; batch classifier loss: 0.533253\n",
      "epoch 14; iter: 800; batch classifier loss: 0.522651\n",
      "epoch 14; iter: 1000; batch classifier loss: 0.548112\n",
      "epoch 14; iter: 1200; batch classifier loss: 0.485738\n",
      "epoch 14; iter: 1400; batch classifier loss: 0.609542\n",
      "epoch 14; iter: 1600; batch classifier loss: 0.495121\n",
      "epoch 14; iter: 1800; batch classifier loss: 0.491684\n",
      "epoch 14; iter: 2000; batch classifier loss: 0.539125\n",
      "epoch 14; iter: 2200; batch classifier loss: 0.518991\n",
      "epoch 14; iter: 2400; batch classifier loss: 0.570782\n",
      "epoch 14; iter: 2600; batch classifier loss: 0.453766\n",
      "epoch 15; iter: 0; batch classifier loss: 0.633806\n",
      "epoch 15; iter: 200; batch classifier loss: 0.546306\n",
      "epoch 15; iter: 400; batch classifier loss: 0.467858\n",
      "epoch 15; iter: 600; batch classifier loss: 0.549010\n",
      "epoch 15; iter: 800; batch classifier loss: 0.539262\n",
      "epoch 15; iter: 1000; batch classifier loss: 0.607944\n",
      "epoch 15; iter: 1200; batch classifier loss: 0.584024\n",
      "epoch 15; iter: 1400; batch classifier loss: 0.507667\n",
      "epoch 15; iter: 1600; batch classifier loss: 0.539733\n",
      "epoch 15; iter: 1800; batch classifier loss: 0.525247\n",
      "epoch 15; iter: 2000; batch classifier loss: 0.554778\n",
      "epoch 15; iter: 2200; batch classifier loss: 0.593104\n",
      "epoch 15; iter: 2400; batch classifier loss: 0.655733\n",
      "epoch 15; iter: 2600; batch classifier loss: 0.498735\n",
      "epoch 16; iter: 0; batch classifier loss: 0.585873\n",
      "epoch 16; iter: 200; batch classifier loss: 0.559323\n",
      "epoch 16; iter: 400; batch classifier loss: 0.579803\n",
      "epoch 16; iter: 600; batch classifier loss: 0.522766\n",
      "epoch 16; iter: 800; batch classifier loss: 0.520266\n",
      "epoch 16; iter: 1000; batch classifier loss: 0.640169\n",
      "epoch 16; iter: 1200; batch classifier loss: 0.538119\n",
      "epoch 16; iter: 1400; batch classifier loss: 0.537629\n",
      "epoch 16; iter: 1600; batch classifier loss: 0.573330\n",
      "epoch 16; iter: 1800; batch classifier loss: 0.513671\n",
      "epoch 16; iter: 2000; batch classifier loss: 0.535105\n",
      "epoch 16; iter: 2200; batch classifier loss: 0.531511\n",
      "epoch 16; iter: 2400; batch classifier loss: 0.470497\n",
      "epoch 16; iter: 2600; batch classifier loss: 0.498749\n",
      "epoch 17; iter: 0; batch classifier loss: 0.557704\n",
      "epoch 17; iter: 200; batch classifier loss: 0.535467\n",
      "epoch 17; iter: 400; batch classifier loss: 0.492627\n",
      "epoch 17; iter: 600; batch classifier loss: 0.631983\n",
      "epoch 17; iter: 800; batch classifier loss: 0.485886\n",
      "epoch 17; iter: 1000; batch classifier loss: 0.526042\n",
      "epoch 17; iter: 1200; batch classifier loss: 0.503091\n",
      "epoch 17; iter: 1400; batch classifier loss: 0.515718\n",
      "epoch 17; iter: 1600; batch classifier loss: 0.437033\n",
      "epoch 17; iter: 1800; batch classifier loss: 0.520631\n",
      "epoch 17; iter: 2000; batch classifier loss: 0.528350\n",
      "epoch 17; iter: 2200; batch classifier loss: 0.520344\n",
      "epoch 17; iter: 2400; batch classifier loss: 0.517689\n",
      "epoch 17; iter: 2600; batch classifier loss: 0.436290\n",
      "epoch 18; iter: 0; batch classifier loss: 0.509437\n",
      "epoch 18; iter: 200; batch classifier loss: 0.518572\n",
      "epoch 18; iter: 400; batch classifier loss: 0.589222\n",
      "epoch 18; iter: 600; batch classifier loss: 0.478338\n",
      "epoch 18; iter: 800; batch classifier loss: 0.597602\n",
      "epoch 18; iter: 1000; batch classifier loss: 0.516538\n",
      "epoch 18; iter: 1200; batch classifier loss: 0.566167\n",
      "epoch 18; iter: 1400; batch classifier loss: 0.564058\n",
      "epoch 18; iter: 1600; batch classifier loss: 0.525488\n",
      "epoch 18; iter: 1800; batch classifier loss: 0.573732\n",
      "epoch 18; iter: 2000; batch classifier loss: 0.478675\n",
      "epoch 18; iter: 2200; batch classifier loss: 0.560062\n",
      "epoch 18; iter: 2400; batch classifier loss: 0.606714\n",
      "epoch 18; iter: 2600; batch classifier loss: 0.563596\n",
      "epoch 19; iter: 0; batch classifier loss: 0.531360\n",
      "epoch 19; iter: 200; batch classifier loss: 0.515556\n",
      "epoch 19; iter: 400; batch classifier loss: 0.520231\n",
      "epoch 19; iter: 600; batch classifier loss: 0.559623\n",
      "epoch 19; iter: 800; batch classifier loss: 0.584840\n",
      "epoch 19; iter: 1000; batch classifier loss: 0.551166\n",
      "epoch 19; iter: 1200; batch classifier loss: 0.523146\n",
      "epoch 19; iter: 1400; batch classifier loss: 0.567622\n",
      "epoch 19; iter: 1600; batch classifier loss: 0.583941\n",
      "epoch 19; iter: 1800; batch classifier loss: 0.489216\n",
      "epoch 19; iter: 2000; batch classifier loss: 0.518438\n",
      "epoch 19; iter: 2200; batch classifier loss: 0.447743\n",
      "epoch 19; iter: 2400; batch classifier loss: 0.577530\n",
      "epoch 19; iter: 2600; batch classifier loss: 0.535225\n",
      "epoch 20; iter: 0; batch classifier loss: 0.520906\n",
      "epoch 20; iter: 200; batch classifier loss: 0.552471\n",
      "epoch 20; iter: 400; batch classifier loss: 0.568040\n",
      "epoch 20; iter: 600; batch classifier loss: 0.538689\n",
      "epoch 20; iter: 800; batch classifier loss: 0.515212\n",
      "epoch 20; iter: 1000; batch classifier loss: 0.636836\n",
      "epoch 20; iter: 1200; batch classifier loss: 0.479121\n",
      "epoch 20; iter: 1400; batch classifier loss: 0.507146\n",
      "epoch 20; iter: 1600; batch classifier loss: 0.540545\n",
      "epoch 20; iter: 1800; batch classifier loss: 0.548076\n",
      "epoch 20; iter: 2000; batch classifier loss: 0.510034\n",
      "epoch 20; iter: 2200; batch classifier loss: 0.507249\n",
      "epoch 20; iter: 2400; batch classifier loss: 0.529397\n",
      "epoch 20; iter: 2600; batch classifier loss: 0.501308\n",
      "epoch 21; iter: 0; batch classifier loss: 0.568563\n",
      "epoch 21; iter: 200; batch classifier loss: 0.485898\n",
      "epoch 21; iter: 400; batch classifier loss: 0.576858\n",
      "epoch 21; iter: 600; batch classifier loss: 0.607112\n",
      "epoch 21; iter: 800; batch classifier loss: 0.513109\n",
      "epoch 21; iter: 1000; batch classifier loss: 0.485086\n",
      "epoch 21; iter: 1200; batch classifier loss: 0.577693\n",
      "epoch 21; iter: 1400; batch classifier loss: 0.612724\n",
      "epoch 21; iter: 1600; batch classifier loss: 0.574152\n",
      "epoch 21; iter: 1800; batch classifier loss: 0.453949\n",
      "epoch 21; iter: 2000; batch classifier loss: 0.536509\n",
      "epoch 21; iter: 2200; batch classifier loss: 0.628933\n",
      "epoch 21; iter: 2400; batch classifier loss: 0.514654\n",
      "epoch 21; iter: 2600; batch classifier loss: 0.674326\n",
      "epoch 22; iter: 0; batch classifier loss: 0.547019\n",
      "epoch 22; iter: 200; batch classifier loss: 0.504130\n",
      "epoch 22; iter: 400; batch classifier loss: 0.553939\n",
      "epoch 22; iter: 600; batch classifier loss: 0.465230\n",
      "epoch 22; iter: 800; batch classifier loss: 0.590683\n",
      "epoch 22; iter: 1000; batch classifier loss: 0.459298\n",
      "epoch 22; iter: 1200; batch classifier loss: 0.579370\n",
      "epoch 22; iter: 1400; batch classifier loss: 0.498787\n",
      "epoch 22; iter: 1600; batch classifier loss: 0.500978\n",
      "epoch 22; iter: 1800; batch classifier loss: 0.546551\n",
      "epoch 22; iter: 2000; batch classifier loss: 0.608978\n",
      "epoch 22; iter: 2200; batch classifier loss: 0.481992\n",
      "epoch 22; iter: 2400; batch classifier loss: 0.503539\n",
      "epoch 22; iter: 2600; batch classifier loss: 0.548089\n",
      "epoch 23; iter: 0; batch classifier loss: 0.523825\n",
      "epoch 23; iter: 200; batch classifier loss: 0.529036\n",
      "epoch 23; iter: 400; batch classifier loss: 0.573878\n",
      "epoch 23; iter: 600; batch classifier loss: 0.493450\n",
      "epoch 23; iter: 800; batch classifier loss: 0.576018\n",
      "epoch 23; iter: 1000; batch classifier loss: 0.557738\n",
      "epoch 23; iter: 1200; batch classifier loss: 0.458069\n",
      "epoch 23; iter: 1400; batch classifier loss: 0.467589\n",
      "epoch 23; iter: 1600; batch classifier loss: 0.562551\n",
      "epoch 23; iter: 1800; batch classifier loss: 0.587786\n",
      "epoch 23; iter: 2000; batch classifier loss: 0.547659\n",
      "epoch 23; iter: 2200; batch classifier loss: 0.538101\n",
      "epoch 23; iter: 2400; batch classifier loss: 0.560467\n",
      "epoch 23; iter: 2600; batch classifier loss: 0.553912\n",
      "epoch 24; iter: 0; batch classifier loss: 0.571222\n",
      "epoch 24; iter: 200; batch classifier loss: 0.478476\n",
      "epoch 24; iter: 400; batch classifier loss: 0.618347\n",
      "epoch 24; iter: 600; batch classifier loss: 0.551421\n",
      "epoch 24; iter: 800; batch classifier loss: 0.517759\n",
      "epoch 24; iter: 1000; batch classifier loss: 0.602345\n",
      "epoch 24; iter: 1200; batch classifier loss: 0.506576\n",
      "epoch 24; iter: 1400; batch classifier loss: 0.531754\n",
      "epoch 24; iter: 1600; batch classifier loss: 0.653250\n",
      "epoch 24; iter: 1800; batch classifier loss: 0.612256\n",
      "epoch 24; iter: 2000; batch classifier loss: 0.503758\n",
      "epoch 24; iter: 2200; batch classifier loss: 0.505272\n",
      "epoch 24; iter: 2400; batch classifier loss: 0.556033\n",
      "epoch 24; iter: 2600; batch classifier loss: 0.548815\n",
      "epoch 25; iter: 0; batch classifier loss: 0.567720\n",
      "epoch 25; iter: 200; batch classifier loss: 0.509458\n",
      "epoch 25; iter: 400; batch classifier loss: 0.481096\n",
      "epoch 25; iter: 600; batch classifier loss: 0.585409\n",
      "epoch 25; iter: 800; batch classifier loss: 0.511412\n",
      "epoch 25; iter: 1000; batch classifier loss: 0.574880\n",
      "epoch 25; iter: 1200; batch classifier loss: 0.498799\n",
      "epoch 25; iter: 1400; batch classifier loss: 0.603440\n",
      "epoch 25; iter: 1600; batch classifier loss: 0.491554\n",
      "epoch 25; iter: 1800; batch classifier loss: 0.525317\n",
      "epoch 25; iter: 2000; batch classifier loss: 0.580808\n",
      "epoch 25; iter: 2200; batch classifier loss: 0.530816\n",
      "epoch 25; iter: 2400; batch classifier loss: 0.548537\n",
      "epoch 25; iter: 2600; batch classifier loss: 0.581856\n",
      "epoch 26; iter: 0; batch classifier loss: 0.534115\n",
      "epoch 26; iter: 200; batch classifier loss: 0.559848\n",
      "epoch 26; iter: 400; batch classifier loss: 0.579368\n",
      "epoch 26; iter: 600; batch classifier loss: 0.510887\n",
      "epoch 26; iter: 800; batch classifier loss: 0.545877\n",
      "epoch 26; iter: 1000; batch classifier loss: 0.598467\n",
      "epoch 26; iter: 1200; batch classifier loss: 0.560415\n",
      "epoch 26; iter: 1400; batch classifier loss: 0.543241\n",
      "epoch 26; iter: 1600; batch classifier loss: 0.527126\n",
      "epoch 26; iter: 1800; batch classifier loss: 0.548522\n",
      "epoch 26; iter: 2000; batch classifier loss: 0.526976\n",
      "epoch 26; iter: 2200; batch classifier loss: 0.527884\n",
      "epoch 26; iter: 2400; batch classifier loss: 0.560267\n",
      "epoch 26; iter: 2600; batch classifier loss: 0.610289\n",
      "epoch 27; iter: 0; batch classifier loss: 0.531189\n",
      "epoch 27; iter: 200; batch classifier loss: 0.523345\n",
      "epoch 27; iter: 400; batch classifier loss: 0.483807\n",
      "epoch 27; iter: 600; batch classifier loss: 0.488838\n",
      "epoch 27; iter: 800; batch classifier loss: 0.489831\n",
      "epoch 27; iter: 1000; batch classifier loss: 0.522520\n",
      "epoch 27; iter: 1200; batch classifier loss: 0.491298\n",
      "epoch 27; iter: 1400; batch classifier loss: 0.547823\n",
      "epoch 27; iter: 1600; batch classifier loss: 0.528685\n",
      "epoch 27; iter: 1800; batch classifier loss: 0.535535\n",
      "epoch 27; iter: 2000; batch classifier loss: 0.462386\n",
      "epoch 27; iter: 2200; batch classifier loss: 0.466919\n",
      "epoch 27; iter: 2400; batch classifier loss: 0.518480\n",
      "epoch 27; iter: 2600; batch classifier loss: 0.584416\n",
      "epoch 28; iter: 0; batch classifier loss: 0.494605\n",
      "epoch 28; iter: 200; batch classifier loss: 0.492480\n",
      "epoch 28; iter: 400; batch classifier loss: 0.452316\n",
      "epoch 28; iter: 600; batch classifier loss: 0.601382\n",
      "epoch 28; iter: 800; batch classifier loss: 0.525503\n",
      "epoch 28; iter: 1000; batch classifier loss: 0.499715\n",
      "epoch 28; iter: 1200; batch classifier loss: 0.491532\n",
      "epoch 28; iter: 1400; batch classifier loss: 0.511419\n",
      "epoch 28; iter: 1600; batch classifier loss: 0.573739\n",
      "epoch 28; iter: 1800; batch classifier loss: 0.575792\n",
      "epoch 28; iter: 2000; batch classifier loss: 0.518804\n",
      "epoch 28; iter: 2200; batch classifier loss: 0.522821\n",
      "epoch 28; iter: 2400; batch classifier loss: 0.601963\n",
      "epoch 28; iter: 2600; batch classifier loss: 0.501228\n",
      "epoch 29; iter: 0; batch classifier loss: 0.519016\n",
      "epoch 29; iter: 200; batch classifier loss: 0.515613\n",
      "epoch 29; iter: 400; batch classifier loss: 0.568284\n",
      "epoch 29; iter: 600; batch classifier loss: 0.487767\n",
      "epoch 29; iter: 800; batch classifier loss: 0.470409\n",
      "epoch 29; iter: 1000; batch classifier loss: 0.591683\n",
      "epoch 29; iter: 1200; batch classifier loss: 0.544578\n",
      "epoch 29; iter: 1400; batch classifier loss: 0.508913\n",
      "epoch 29; iter: 1600; batch classifier loss: 0.485204\n",
      "epoch 29; iter: 1800; batch classifier loss: 0.548895\n",
      "epoch 29; iter: 2000; batch classifier loss: 0.582032\n",
      "epoch 29; iter: 2200; batch classifier loss: 0.530783\n",
      "epoch 29; iter: 2400; batch classifier loss: 0.536947\n",
      "epoch 29; iter: 2600; batch classifier loss: 0.527970\n",
      "epoch 30; iter: 0; batch classifier loss: 0.521143\n",
      "epoch 30; iter: 200; batch classifier loss: 0.451583\n",
      "epoch 30; iter: 400; batch classifier loss: 0.505529\n",
      "epoch 30; iter: 600; batch classifier loss: 0.519785\n",
      "epoch 30; iter: 800; batch classifier loss: 0.537936\n",
      "epoch 30; iter: 1000; batch classifier loss: 0.495400\n",
      "epoch 30; iter: 1200; batch classifier loss: 0.524168\n",
      "epoch 30; iter: 1400; batch classifier loss: 0.483132\n",
      "epoch 30; iter: 1600; batch classifier loss: 0.524580\n",
      "epoch 30; iter: 1800; batch classifier loss: 0.542022\n",
      "epoch 30; iter: 2000; batch classifier loss: 0.506411\n",
      "epoch 30; iter: 2200; batch classifier loss: 0.486336\n",
      "epoch 30; iter: 2400; batch classifier loss: 0.481370\n",
      "epoch 30; iter: 2600; batch classifier loss: 0.450490\n",
      "epoch 31; iter: 0; batch classifier loss: 0.580673\n",
      "epoch 31; iter: 200; batch classifier loss: 0.553429\n",
      "epoch 31; iter: 400; batch classifier loss: 0.538649\n",
      "epoch 31; iter: 600; batch classifier loss: 0.550009\n",
      "epoch 31; iter: 800; batch classifier loss: 0.512842\n",
      "epoch 31; iter: 1000; batch classifier loss: 0.437894\n",
      "epoch 31; iter: 1200; batch classifier loss: 0.549323\n",
      "epoch 31; iter: 1400; batch classifier loss: 0.516560\n",
      "epoch 31; iter: 1600; batch classifier loss: 0.579108\n",
      "epoch 31; iter: 1800; batch classifier loss: 0.485820\n",
      "epoch 31; iter: 2000; batch classifier loss: 0.569239\n",
      "epoch 31; iter: 2200; batch classifier loss: 0.459878\n",
      "epoch 31; iter: 2400; batch classifier loss: 0.571686\n",
      "epoch 31; iter: 2600; batch classifier loss: 0.455761\n",
      "epoch 32; iter: 0; batch classifier loss: 0.556611\n",
      "epoch 32; iter: 200; batch classifier loss: 0.460362\n",
      "epoch 32; iter: 400; batch classifier loss: 0.443719\n",
      "epoch 32; iter: 600; batch classifier loss: 0.504709\n",
      "epoch 32; iter: 800; batch classifier loss: 0.499711\n",
      "epoch 32; iter: 1000; batch classifier loss: 0.492811\n",
      "epoch 32; iter: 1200; batch classifier loss: 0.546047\n",
      "epoch 32; iter: 1400; batch classifier loss: 0.491711\n",
      "epoch 32; iter: 1600; batch classifier loss: 0.498927\n",
      "epoch 32; iter: 1800; batch classifier loss: 0.452252\n",
      "epoch 32; iter: 2000; batch classifier loss: 0.580062\n",
      "epoch 32; iter: 2200; batch classifier loss: 0.544549\n",
      "epoch 32; iter: 2400; batch classifier loss: 0.467597\n",
      "epoch 32; iter: 2600; batch classifier loss: 0.457105\n",
      "epoch 33; iter: 0; batch classifier loss: 0.555462\n",
      "epoch 33; iter: 200; batch classifier loss: 0.603377\n",
      "epoch 33; iter: 400; batch classifier loss: 0.520960\n",
      "epoch 33; iter: 600; batch classifier loss: 0.516043\n",
      "epoch 33; iter: 800; batch classifier loss: 0.507348\n",
      "epoch 33; iter: 1000; batch classifier loss: 0.449123\n",
      "epoch 33; iter: 1200; batch classifier loss: 0.547234\n",
      "epoch 33; iter: 1400; batch classifier loss: 0.546821\n",
      "epoch 33; iter: 1600; batch classifier loss: 0.492488\n",
      "epoch 33; iter: 1800; batch classifier loss: 0.517263\n",
      "epoch 33; iter: 2000; batch classifier loss: 0.551922\n",
      "epoch 33; iter: 2200; batch classifier loss: 0.565413\n",
      "epoch 33; iter: 2400; batch classifier loss: 0.495838\n",
      "epoch 33; iter: 2600; batch classifier loss: 0.497804\n",
      "epoch 34; iter: 0; batch classifier loss: 0.482587\n",
      "epoch 34; iter: 200; batch classifier loss: 0.524070\n",
      "epoch 34; iter: 400; batch classifier loss: 0.550503\n",
      "epoch 34; iter: 600; batch classifier loss: 0.568035\n",
      "epoch 34; iter: 800; batch classifier loss: 0.559252\n",
      "epoch 34; iter: 1000; batch classifier loss: 0.617149\n",
      "epoch 34; iter: 1200; batch classifier loss: 0.515750\n",
      "epoch 34; iter: 1400; batch classifier loss: 0.493473\n",
      "epoch 34; iter: 1600; batch classifier loss: 0.523259\n",
      "epoch 34; iter: 1800; batch classifier loss: 0.597879\n",
      "epoch 34; iter: 2000; batch classifier loss: 0.485520\n",
      "epoch 34; iter: 2200; batch classifier loss: 0.549958\n",
      "epoch 34; iter: 2400; batch classifier loss: 0.504080\n",
      "epoch 34; iter: 2600; batch classifier loss: 0.494380\n",
      "epoch 35; iter: 0; batch classifier loss: 0.579295\n",
      "epoch 35; iter: 200; batch classifier loss: 0.544134\n",
      "epoch 35; iter: 400; batch classifier loss: 0.524910\n",
      "epoch 35; iter: 600; batch classifier loss: 0.502068\n",
      "epoch 35; iter: 800; batch classifier loss: 0.613842\n",
      "epoch 35; iter: 1000; batch classifier loss: 0.535530\n",
      "epoch 35; iter: 1200; batch classifier loss: 0.617173\n",
      "epoch 35; iter: 1400; batch classifier loss: 0.468010\n",
      "epoch 35; iter: 1600; batch classifier loss: 0.564643\n",
      "epoch 35; iter: 1800; batch classifier loss: 0.574901\n",
      "epoch 35; iter: 2000; batch classifier loss: 0.564525\n",
      "epoch 35; iter: 2200; batch classifier loss: 0.553788\n",
      "epoch 35; iter: 2400; batch classifier loss: 0.457154\n",
      "epoch 35; iter: 2600; batch classifier loss: 0.529310\n",
      "epoch 36; iter: 0; batch classifier loss: 0.599926\n",
      "epoch 36; iter: 200; batch classifier loss: 0.511866\n",
      "epoch 36; iter: 400; batch classifier loss: 0.546581\n",
      "epoch 36; iter: 600; batch classifier loss: 0.592321\n",
      "epoch 36; iter: 800; batch classifier loss: 0.531515\n",
      "epoch 36; iter: 1000; batch classifier loss: 0.498153\n",
      "epoch 36; iter: 1200; batch classifier loss: 0.504116\n",
      "epoch 36; iter: 1400; batch classifier loss: 0.568180\n",
      "epoch 36; iter: 1600; batch classifier loss: 0.516510\n",
      "epoch 36; iter: 1800; batch classifier loss: 0.516394\n",
      "epoch 36; iter: 2000; batch classifier loss: 0.500754\n",
      "epoch 36; iter: 2200; batch classifier loss: 0.551993\n",
      "epoch 36; iter: 2400; batch classifier loss: 0.541479\n",
      "epoch 36; iter: 2600; batch classifier loss: 0.460526\n",
      "epoch 37; iter: 0; batch classifier loss: 0.595252\n",
      "epoch 37; iter: 200; batch classifier loss: 0.550554\n",
      "epoch 37; iter: 400; batch classifier loss: 0.485440\n",
      "epoch 37; iter: 600; batch classifier loss: 0.601482\n",
      "epoch 37; iter: 800; batch classifier loss: 0.486730\n",
      "epoch 37; iter: 1000; batch classifier loss: 0.487863\n",
      "epoch 37; iter: 1200; batch classifier loss: 0.541181\n",
      "epoch 37; iter: 1400; batch classifier loss: 0.562340\n",
      "epoch 37; iter: 1600; batch classifier loss: 0.489217\n",
      "epoch 37; iter: 1800; batch classifier loss: 0.608285\n",
      "epoch 37; iter: 2000; batch classifier loss: 0.524527\n",
      "epoch 37; iter: 2200; batch classifier loss: 0.501501\n",
      "epoch 37; iter: 2400; batch classifier loss: 0.561764\n",
      "epoch 37; iter: 2600; batch classifier loss: 0.586325\n",
      "epoch 38; iter: 0; batch classifier loss: 0.498619\n",
      "epoch 38; iter: 200; batch classifier loss: 0.569953\n",
      "epoch 38; iter: 400; batch classifier loss: 0.596009\n",
      "epoch 38; iter: 600; batch classifier loss: 0.484524\n",
      "epoch 38; iter: 800; batch classifier loss: 0.492844\n",
      "epoch 38; iter: 1000; batch classifier loss: 0.505424\n",
      "epoch 38; iter: 1200; batch classifier loss: 0.512580\n",
      "epoch 38; iter: 1400; batch classifier loss: 0.521599\n",
      "epoch 38; iter: 1600; batch classifier loss: 0.528650\n",
      "epoch 38; iter: 1800; batch classifier loss: 0.540175\n",
      "epoch 38; iter: 2000; batch classifier loss: 0.564227\n",
      "epoch 38; iter: 2200; batch classifier loss: 0.449967\n",
      "epoch 38; iter: 2400; batch classifier loss: 0.539870\n",
      "epoch 38; iter: 2600; batch classifier loss: 0.486806\n",
      "epoch 39; iter: 0; batch classifier loss: 0.466187\n",
      "epoch 39; iter: 200; batch classifier loss: 0.568325\n",
      "epoch 39; iter: 400; batch classifier loss: 0.563669\n",
      "epoch 39; iter: 600; batch classifier loss: 0.511306\n",
      "epoch 39; iter: 800; batch classifier loss: 0.628848\n",
      "epoch 39; iter: 1000; batch classifier loss: 0.439483\n",
      "epoch 39; iter: 1200; batch classifier loss: 0.559094\n",
      "epoch 39; iter: 1400; batch classifier loss: 0.532574\n",
      "epoch 39; iter: 1600; batch classifier loss: 0.539423\n",
      "epoch 39; iter: 1800; batch classifier loss: 0.561985\n",
      "epoch 39; iter: 2000; batch classifier loss: 0.535854\n",
      "epoch 39; iter: 2200; batch classifier loss: 0.534485\n",
      "epoch 39; iter: 2400; batch classifier loss: 0.613077\n",
      "epoch 39; iter: 2600; batch classifier loss: 0.542735\n",
      "epoch 40; iter: 0; batch classifier loss: 0.601317\n",
      "epoch 40; iter: 200; batch classifier loss: 0.569974\n",
      "epoch 40; iter: 400; batch classifier loss: 0.637330\n",
      "epoch 40; iter: 600; batch classifier loss: 0.491096\n",
      "epoch 40; iter: 800; batch classifier loss: 0.525474\n",
      "epoch 40; iter: 1000; batch classifier loss: 0.492518\n",
      "epoch 40; iter: 1200; batch classifier loss: 0.457134\n",
      "epoch 40; iter: 1400; batch classifier loss: 0.458971\n",
      "epoch 40; iter: 1600; batch classifier loss: 0.523542\n",
      "epoch 40; iter: 1800; batch classifier loss: 0.468528\n",
      "epoch 40; iter: 2000; batch classifier loss: 0.554665\n",
      "epoch 40; iter: 2200; batch classifier loss: 0.531723\n",
      "epoch 40; iter: 2400; batch classifier loss: 0.465809\n",
      "epoch 40; iter: 2600; batch classifier loss: 0.483420\n",
      "epoch 41; iter: 0; batch classifier loss: 0.466613\n",
      "epoch 41; iter: 200; batch classifier loss: 0.513588\n",
      "epoch 41; iter: 400; batch classifier loss: 0.521169\n",
      "epoch 41; iter: 600; batch classifier loss: 0.598296\n",
      "epoch 41; iter: 800; batch classifier loss: 0.521159\n",
      "epoch 41; iter: 1000; batch classifier loss: 0.568296\n",
      "epoch 41; iter: 1200; batch classifier loss: 0.546656\n",
      "epoch 41; iter: 1400; batch classifier loss: 0.486982\n",
      "epoch 41; iter: 1600; batch classifier loss: 0.605623\n",
      "epoch 41; iter: 1800; batch classifier loss: 0.643540\n",
      "epoch 41; iter: 2000; batch classifier loss: 0.506173\n",
      "epoch 41; iter: 2200; batch classifier loss: 0.557865\n",
      "epoch 41; iter: 2400; batch classifier loss: 0.609879\n",
      "epoch 41; iter: 2600; batch classifier loss: 0.473709\n",
      "epoch 42; iter: 0; batch classifier loss: 0.593854\n",
      "epoch 42; iter: 200; batch classifier loss: 0.509425\n",
      "epoch 42; iter: 400; batch classifier loss: 0.528152\n",
      "epoch 42; iter: 600; batch classifier loss: 0.526464\n",
      "epoch 42; iter: 800; batch classifier loss: 0.566044\n",
      "epoch 42; iter: 1000; batch classifier loss: 0.570925\n",
      "epoch 42; iter: 1200; batch classifier loss: 0.510790\n",
      "epoch 42; iter: 1400; batch classifier loss: 0.507098\n",
      "epoch 42; iter: 1600; batch classifier loss: 0.486468\n",
      "epoch 42; iter: 1800; batch classifier loss: 0.511492\n",
      "epoch 42; iter: 2000; batch classifier loss: 0.508002\n",
      "epoch 42; iter: 2200; batch classifier loss: 0.545081\n",
      "epoch 42; iter: 2400; batch classifier loss: 0.483654\n",
      "epoch 42; iter: 2600; batch classifier loss: 0.494654\n",
      "epoch 43; iter: 0; batch classifier loss: 0.465198\n",
      "epoch 43; iter: 200; batch classifier loss: 0.564711\n",
      "epoch 43; iter: 400; batch classifier loss: 0.478315\n",
      "epoch 43; iter: 600; batch classifier loss: 0.537248\n",
      "epoch 43; iter: 800; batch classifier loss: 0.558032\n",
      "epoch 43; iter: 1000; batch classifier loss: 0.552939\n",
      "epoch 43; iter: 1200; batch classifier loss: 0.557003\n",
      "epoch 43; iter: 1400; batch classifier loss: 0.537089\n",
      "epoch 43; iter: 1600; batch classifier loss: 0.529873\n",
      "epoch 43; iter: 1800; batch classifier loss: 0.463405\n",
      "epoch 43; iter: 2000; batch classifier loss: 0.498512\n",
      "epoch 43; iter: 2200; batch classifier loss: 0.529426\n",
      "epoch 43; iter: 2400; batch classifier loss: 0.486796\n",
      "epoch 43; iter: 2600; batch classifier loss: 0.469198\n",
      "epoch 44; iter: 0; batch classifier loss: 0.487935\n",
      "epoch 44; iter: 200; batch classifier loss: 0.545534\n",
      "epoch 44; iter: 400; batch classifier loss: 0.522365\n",
      "epoch 44; iter: 600; batch classifier loss: 0.632052\n",
      "epoch 44; iter: 800; batch classifier loss: 0.489693\n",
      "epoch 44; iter: 1000; batch classifier loss: 0.580257\n",
      "epoch 44; iter: 1200; batch classifier loss: 0.481846\n",
      "epoch 44; iter: 1400; batch classifier loss: 0.511015\n",
      "epoch 44; iter: 1600; batch classifier loss: 0.519526\n",
      "epoch 44; iter: 1800; batch classifier loss: 0.543125\n",
      "epoch 44; iter: 2000; batch classifier loss: 0.536920\n",
      "epoch 44; iter: 2200; batch classifier loss: 0.460341\n",
      "epoch 44; iter: 2400; batch classifier loss: 0.470623\n",
      "epoch 44; iter: 2600; batch classifier loss: 0.516865\n",
      "epoch 45; iter: 0; batch classifier loss: 0.489728\n",
      "epoch 45; iter: 200; batch classifier loss: 0.549417\n",
      "epoch 45; iter: 400; batch classifier loss: 0.574138\n",
      "epoch 45; iter: 600; batch classifier loss: 0.404270\n",
      "epoch 45; iter: 800; batch classifier loss: 0.488383\n",
      "epoch 45; iter: 1000; batch classifier loss: 0.522804\n",
      "epoch 45; iter: 1200; batch classifier loss: 0.500541\n",
      "epoch 45; iter: 1400; batch classifier loss: 0.570185\n",
      "epoch 45; iter: 1600; batch classifier loss: 0.616290\n",
      "epoch 45; iter: 1800; batch classifier loss: 0.609427\n",
      "epoch 45; iter: 2000; batch classifier loss: 0.472774\n",
      "epoch 45; iter: 2200; batch classifier loss: 0.508328\n",
      "epoch 45; iter: 2400; batch classifier loss: 0.473473\n",
      "epoch 45; iter: 2600; batch classifier loss: 0.563892\n",
      "epoch 46; iter: 0; batch classifier loss: 0.461982\n",
      "epoch 46; iter: 200; batch classifier loss: 0.466236\n",
      "epoch 46; iter: 400; batch classifier loss: 0.449564\n",
      "epoch 46; iter: 600; batch classifier loss: 0.487086\n",
      "epoch 46; iter: 800; batch classifier loss: 0.592939\n",
      "epoch 46; iter: 1000; batch classifier loss: 0.478579\n",
      "epoch 46; iter: 1200; batch classifier loss: 0.565716\n",
      "epoch 46; iter: 1400; batch classifier loss: 0.508688\n",
      "epoch 46; iter: 1600; batch classifier loss: 0.562910\n",
      "epoch 46; iter: 1800; batch classifier loss: 0.522663\n",
      "epoch 46; iter: 2000; batch classifier loss: 0.514737\n",
      "epoch 46; iter: 2200; batch classifier loss: 0.517101\n",
      "epoch 46; iter: 2400; batch classifier loss: 0.555075\n",
      "epoch 46; iter: 2600; batch classifier loss: 0.557901\n",
      "epoch 47; iter: 0; batch classifier loss: 0.584871\n",
      "epoch 47; iter: 200; batch classifier loss: 0.473437\n",
      "epoch 47; iter: 400; batch classifier loss: 0.525741\n",
      "epoch 47; iter: 600; batch classifier loss: 0.522621\n",
      "epoch 47; iter: 800; batch classifier loss: 0.536413\n",
      "epoch 47; iter: 1000; batch classifier loss: 0.507818\n",
      "epoch 47; iter: 1200; batch classifier loss: 0.574642\n",
      "epoch 47; iter: 1400; batch classifier loss: 0.583527\n",
      "epoch 47; iter: 1600; batch classifier loss: 0.497180\n",
      "epoch 47; iter: 1800; batch classifier loss: 0.603592\n",
      "epoch 47; iter: 2000; batch classifier loss: 0.511182\n",
      "epoch 47; iter: 2200; batch classifier loss: 0.495432\n",
      "epoch 47; iter: 2400; batch classifier loss: 0.572178\n",
      "epoch 47; iter: 2600; batch classifier loss: 0.498100\n",
      "epoch 48; iter: 0; batch classifier loss: 0.523462\n",
      "epoch 48; iter: 200; batch classifier loss: 0.458253\n",
      "epoch 48; iter: 400; batch classifier loss: 0.576631\n",
      "epoch 48; iter: 600; batch classifier loss: 0.499129\n",
      "epoch 48; iter: 800; batch classifier loss: 0.525341\n",
      "epoch 48; iter: 1000; batch classifier loss: 0.476018\n",
      "epoch 48; iter: 1200; batch classifier loss: 0.544935\n",
      "epoch 48; iter: 1400; batch classifier loss: 0.522795\n",
      "epoch 48; iter: 1600; batch classifier loss: 0.550713\n",
      "epoch 48; iter: 1800; batch classifier loss: 0.517290\n",
      "epoch 48; iter: 2000; batch classifier loss: 0.582542\n",
      "epoch 48; iter: 2200; batch classifier loss: 0.531809\n",
      "epoch 48; iter: 2400; batch classifier loss: 0.533536\n",
      "epoch 48; iter: 2600; batch classifier loss: 0.517960\n",
      "epoch 49; iter: 0; batch classifier loss: 0.511726\n",
      "epoch 49; iter: 200; batch classifier loss: 0.634751\n",
      "epoch 49; iter: 400; batch classifier loss: 0.554539\n",
      "epoch 49; iter: 600; batch classifier loss: 0.561895\n",
      "epoch 49; iter: 800; batch classifier loss: 0.485837\n",
      "epoch 49; iter: 1000; batch classifier loss: 0.434200\n",
      "epoch 49; iter: 1200; batch classifier loss: 0.496894\n",
      "epoch 49; iter: 1400; batch classifier loss: 0.601598\n",
      "epoch 49; iter: 1600; batch classifier loss: 0.514057\n",
      "epoch 49; iter: 1800; batch classifier loss: 0.521422\n",
      "epoch 49; iter: 2000; batch classifier loss: 0.548648\n",
      "epoch 49; iter: 2200; batch classifier loss: 0.522247\n",
      "epoch 49; iter: 2400; batch classifier loss: 0.506267\n",
      "epoch 49; iter: 2600; batch classifier loss: 0.464247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<aif360.algorithms.inprocessing.adversarial_debiasing.AdversarialDebiasing at 0x7fc3e3fef350>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_model.fit(train_pp_bld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the plain model to test data\n",
    "dataset_nodebiasing_train_ad = plain_model.predict(train_pp_bld)\n",
    "dataset_nodebiasing_test_ad = plain_model.predict(test_pp_bld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Adversial Debiasing pre-processing algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn parameters with debias set to True\n",
    "debiased_model_ad = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='debiased_classifier',\n",
    "                          debias=True,\n",
    "                          sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.687576; batch adversarial loss: 0.726831\n",
      "epoch 0; iter: 200; batch classifier loss: 0.668309; batch adversarial loss: 0.748306\n",
      "epoch 0; iter: 400; batch classifier loss: 0.602222; batch adversarial loss: 0.592351\n",
      "epoch 0; iter: 600; batch classifier loss: 0.541700; batch adversarial loss: 0.477641\n",
      "epoch 0; iter: 800; batch classifier loss: 0.608029; batch adversarial loss: 0.453457\n",
      "epoch 0; iter: 1000; batch classifier loss: 0.626866; batch adversarial loss: 0.477429\n",
      "epoch 0; iter: 1200; batch classifier loss: 0.587422; batch adversarial loss: 0.410003\n",
      "epoch 0; iter: 1400; batch classifier loss: 0.907972; batch adversarial loss: 0.573736\n",
      "epoch 0; iter: 1600; batch classifier loss: 0.912307; batch adversarial loss: 0.586690\n",
      "epoch 0; iter: 1800; batch classifier loss: 0.665068; batch adversarial loss: 0.449919\n",
      "epoch 0; iter: 2000; batch classifier loss: 0.579663; batch adversarial loss: 0.465407\n",
      "epoch 0; iter: 2200; batch classifier loss: 0.562316; batch adversarial loss: 0.513292\n",
      "epoch 0; iter: 2400; batch classifier loss: 0.586491; batch adversarial loss: 0.475373\n",
      "epoch 0; iter: 2600; batch classifier loss: 0.629258; batch adversarial loss: 0.398558\n",
      "epoch 1; iter: 0; batch classifier loss: 0.579950; batch adversarial loss: 0.499797\n",
      "epoch 1; iter: 200; batch classifier loss: 0.616412; batch adversarial loss: 0.443740\n",
      "epoch 1; iter: 400; batch classifier loss: 0.593342; batch adversarial loss: 0.499132\n",
      "epoch 1; iter: 600; batch classifier loss: 0.672257; batch adversarial loss: 0.515765\n",
      "epoch 1; iter: 800; batch classifier loss: 0.584569; batch adversarial loss: 0.434957\n",
      "epoch 1; iter: 1000; batch classifier loss: 0.598342; batch adversarial loss: 0.536617\n",
      "epoch 1; iter: 1200; batch classifier loss: 0.566808; batch adversarial loss: 0.444894\n",
      "epoch 1; iter: 1400; batch classifier loss: 0.596929; batch adversarial loss: 0.451111\n",
      "epoch 1; iter: 1600; batch classifier loss: 0.636671; batch adversarial loss: 0.504518\n",
      "epoch 1; iter: 1800; batch classifier loss: 0.605980; batch adversarial loss: 0.482303\n",
      "epoch 1; iter: 2000; batch classifier loss: 0.563479; batch adversarial loss: 0.509111\n",
      "epoch 1; iter: 2200; batch classifier loss: 0.596150; batch adversarial loss: 0.529641\n",
      "epoch 1; iter: 2400; batch classifier loss: 0.639602; batch adversarial loss: 0.538321\n",
      "epoch 1; iter: 2600; batch classifier loss: 0.588497; batch adversarial loss: 0.475490\n",
      "epoch 2; iter: 0; batch classifier loss: 0.614769; batch adversarial loss: 0.530778\n",
      "epoch 2; iter: 200; batch classifier loss: 0.668037; batch adversarial loss: 0.515032\n",
      "epoch 2; iter: 400; batch classifier loss: 0.721548; batch adversarial loss: 0.560672\n",
      "epoch 2; iter: 600; batch classifier loss: 0.686717; batch adversarial loss: 0.456406\n",
      "epoch 2; iter: 800; batch classifier loss: 0.650940; batch adversarial loss: 0.400324\n",
      "epoch 2; iter: 1000; batch classifier loss: 0.501852; batch adversarial loss: 0.390789\n",
      "epoch 2; iter: 1200; batch classifier loss: 0.608263; batch adversarial loss: 0.456810\n",
      "epoch 2; iter: 1400; batch classifier loss: 0.628075; batch adversarial loss: 0.423689\n",
      "epoch 2; iter: 1600; batch classifier loss: 0.651906; batch adversarial loss: 0.506338\n",
      "epoch 2; iter: 1800; batch classifier loss: 0.630561; batch adversarial loss: 0.413677\n",
      "epoch 2; iter: 2000; batch classifier loss: 0.628664; batch adversarial loss: 0.509032\n",
      "epoch 2; iter: 2200; batch classifier loss: 0.843176; batch adversarial loss: 0.475436\n",
      "epoch 2; iter: 2400; batch classifier loss: 0.830251; batch adversarial loss: 0.515646\n",
      "epoch 2; iter: 2600; batch classifier loss: 0.650787; batch adversarial loss: 0.488093\n",
      "epoch 3; iter: 0; batch classifier loss: 0.589398; batch adversarial loss: 0.384960\n",
      "epoch 3; iter: 200; batch classifier loss: 0.632394; batch adversarial loss: 0.505979\n",
      "epoch 3; iter: 400; batch classifier loss: 0.597580; batch adversarial loss: 0.377883\n",
      "epoch 3; iter: 600; batch classifier loss: 0.605747; batch adversarial loss: 0.448574\n",
      "epoch 3; iter: 800; batch classifier loss: 0.572393; batch adversarial loss: 0.578422\n",
      "epoch 3; iter: 1000; batch classifier loss: 0.683859; batch adversarial loss: 0.490871\n",
      "epoch 3; iter: 1200; batch classifier loss: 0.607313; batch adversarial loss: 0.459208\n",
      "epoch 3; iter: 1400; batch classifier loss: 0.738557; batch adversarial loss: 0.540467\n",
      "epoch 3; iter: 1600; batch classifier loss: 0.903669; batch adversarial loss: 0.489421\n",
      "epoch 3; iter: 1800; batch classifier loss: 0.636954; batch adversarial loss: 0.451686\n",
      "epoch 3; iter: 2000; batch classifier loss: 0.552287; batch adversarial loss: 0.439297\n",
      "epoch 3; iter: 2200; batch classifier loss: 0.634708; batch adversarial loss: 0.533746\n",
      "epoch 3; iter: 2400; batch classifier loss: 0.610156; batch adversarial loss: 0.406440\n",
      "epoch 3; iter: 2600; batch classifier loss: 0.564664; batch adversarial loss: 0.477612\n",
      "epoch 4; iter: 0; batch classifier loss: 0.612597; batch adversarial loss: 0.458404\n",
      "epoch 4; iter: 200; batch classifier loss: 0.544975; batch adversarial loss: 0.598552\n",
      "epoch 4; iter: 400; batch classifier loss: 0.558142; batch adversarial loss: 0.503227\n",
      "epoch 4; iter: 600; batch classifier loss: 0.621891; batch adversarial loss: 0.441389\n",
      "epoch 4; iter: 800; batch classifier loss: 0.644175; batch adversarial loss: 0.448490\n",
      "epoch 4; iter: 1000; batch classifier loss: 0.954065; batch adversarial loss: 0.620119\n",
      "epoch 4; iter: 1200; batch classifier loss: 0.790160; batch adversarial loss: 0.550794\n",
      "epoch 4; iter: 1400; batch classifier loss: 0.551110; batch adversarial loss: 0.468204\n",
      "epoch 4; iter: 1600; batch classifier loss: 0.616741; batch adversarial loss: 0.467385\n",
      "epoch 4; iter: 1800; batch classifier loss: 0.625841; batch adversarial loss: 0.539422\n",
      "epoch 4; iter: 2000; batch classifier loss: 0.522149; batch adversarial loss: 0.303012\n",
      "epoch 4; iter: 2200; batch classifier loss: 0.588172; batch adversarial loss: 0.359741\n",
      "epoch 4; iter: 2400; batch classifier loss: 0.568390; batch adversarial loss: 0.493973\n",
      "epoch 4; iter: 2600; batch classifier loss: 0.588786; batch adversarial loss: 0.413360\n",
      "epoch 5; iter: 0; batch classifier loss: 0.604961; batch adversarial loss: 0.400060\n",
      "epoch 5; iter: 200; batch classifier loss: 0.815911; batch adversarial loss: 0.616066\n",
      "epoch 5; iter: 400; batch classifier loss: 0.843687; batch adversarial loss: 0.550696\n",
      "epoch 5; iter: 600; batch classifier loss: 0.812685; batch adversarial loss: 0.447909\n",
      "epoch 5; iter: 800; batch classifier loss: 0.665020; batch adversarial loss: 0.524272\n",
      "epoch 5; iter: 1000; batch classifier loss: 0.596077; batch adversarial loss: 0.497126\n",
      "epoch 5; iter: 1200; batch classifier loss: 0.581878; batch adversarial loss: 0.448539\n",
      "epoch 5; iter: 1400; batch classifier loss: 0.683762; batch adversarial loss: 0.509051\n",
      "epoch 5; iter: 1600; batch classifier loss: 0.622343; batch adversarial loss: 0.547721\n",
      "epoch 5; iter: 1800; batch classifier loss: 0.613825; batch adversarial loss: 0.513929\n",
      "epoch 5; iter: 2000; batch classifier loss: 0.594717; batch adversarial loss: 0.430192\n",
      "epoch 5; iter: 2200; batch classifier loss: 0.651843; batch adversarial loss: 0.459213\n",
      "epoch 5; iter: 2400; batch classifier loss: 0.700982; batch adversarial loss: 0.404121\n",
      "epoch 5; iter: 2600; batch classifier loss: 0.692800; batch adversarial loss: 0.539798\n",
      "epoch 6; iter: 0; batch classifier loss: 0.659297; batch adversarial loss: 0.409336\n",
      "epoch 6; iter: 200; batch classifier loss: 0.644535; batch adversarial loss: 0.531224\n",
      "epoch 6; iter: 400; batch classifier loss: 0.907908; batch adversarial loss: 0.656914\n",
      "epoch 6; iter: 600; batch classifier loss: 0.929511; batch adversarial loss: 0.522248\n",
      "epoch 6; iter: 800; batch classifier loss: 0.860222; batch adversarial loss: 0.572420\n",
      "epoch 6; iter: 1000; batch classifier loss: 0.821095; batch adversarial loss: 0.459184\n",
      "epoch 6; iter: 1200; batch classifier loss: 0.578318; batch adversarial loss: 0.415578\n",
      "epoch 6; iter: 1400; batch classifier loss: 0.607728; batch adversarial loss: 0.435029\n",
      "epoch 6; iter: 1600; batch classifier loss: 0.620685; batch adversarial loss: 0.347761\n",
      "epoch 6; iter: 1800; batch classifier loss: 0.504849; batch adversarial loss: 0.392459\n",
      "epoch 6; iter: 2000; batch classifier loss: 0.662197; batch adversarial loss: 0.408806\n",
      "epoch 6; iter: 2200; batch classifier loss: 0.642733; batch adversarial loss: 0.372260\n",
      "epoch 6; iter: 2400; batch classifier loss: 0.681535; batch adversarial loss: 0.473129\n",
      "epoch 6; iter: 2600; batch classifier loss: 0.760430; batch adversarial loss: 0.390463\n",
      "epoch 7; iter: 0; batch classifier loss: 0.579824; batch adversarial loss: 0.424353\n",
      "epoch 7; iter: 200; batch classifier loss: 0.668870; batch adversarial loss: 0.493718\n",
      "epoch 7; iter: 400; batch classifier loss: 0.810064; batch adversarial loss: 0.567523\n",
      "epoch 7; iter: 600; batch classifier loss: 0.945516; batch adversarial loss: 0.519715\n",
      "epoch 7; iter: 800; batch classifier loss: 0.799773; batch adversarial loss: 0.472934\n",
      "epoch 7; iter: 1000; batch classifier loss: 0.785292; batch adversarial loss: 0.532562\n",
      "epoch 7; iter: 1200; batch classifier loss: 0.711218; batch adversarial loss: 0.439319\n",
      "epoch 7; iter: 1400; batch classifier loss: 0.730559; batch adversarial loss: 0.425454\n",
      "epoch 7; iter: 1600; batch classifier loss: 0.861141; batch adversarial loss: 0.403748\n",
      "epoch 7; iter: 1800; batch classifier loss: 0.889475; batch adversarial loss: 0.555827\n",
      "epoch 7; iter: 2000; batch classifier loss: 0.758988; batch adversarial loss: 0.526330\n",
      "epoch 7; iter: 2200; batch classifier loss: 0.643427; batch adversarial loss: 0.410179\n",
      "epoch 7; iter: 2400; batch classifier loss: 0.716357; batch adversarial loss: 0.409676\n",
      "epoch 7; iter: 2600; batch classifier loss: 0.706469; batch adversarial loss: 0.512728\n",
      "epoch 8; iter: 0; batch classifier loss: 0.768214; batch adversarial loss: 0.502622\n",
      "epoch 8; iter: 200; batch classifier loss: 0.949286; batch adversarial loss: 0.583440\n",
      "epoch 8; iter: 400; batch classifier loss: 0.907530; batch adversarial loss: 0.623720\n",
      "epoch 8; iter: 600; batch classifier loss: 1.037289; batch adversarial loss: 0.521796\n",
      "epoch 8; iter: 800; batch classifier loss: 0.938925; batch adversarial loss: 0.593038\n",
      "epoch 8; iter: 1000; batch classifier loss: 0.941138; batch adversarial loss: 0.538815\n",
      "epoch 8; iter: 1200; batch classifier loss: 0.693055; batch adversarial loss: 0.432825\n",
      "epoch 8; iter: 1400; batch classifier loss: 0.774799; batch adversarial loss: 0.542520\n",
      "epoch 8; iter: 1600; batch classifier loss: 0.861427; batch adversarial loss: 0.441499\n",
      "epoch 8; iter: 1800; batch classifier loss: 0.722275; batch adversarial loss: 0.475233\n",
      "epoch 8; iter: 2000; batch classifier loss: 0.719483; batch adversarial loss: 0.370769\n",
      "epoch 8; iter: 2200; batch classifier loss: 0.667055; batch adversarial loss: 0.425557\n",
      "epoch 8; iter: 2400; batch classifier loss: 0.831815; batch adversarial loss: 0.476147\n",
      "epoch 8; iter: 2600; batch classifier loss: 0.918336; batch adversarial loss: 0.487590\n",
      "epoch 9; iter: 0; batch classifier loss: 0.902711; batch adversarial loss: 0.413972\n",
      "epoch 9; iter: 200; batch classifier loss: 1.025565; batch adversarial loss: 0.512238\n",
      "epoch 9; iter: 400; batch classifier loss: 0.916232; batch adversarial loss: 0.595776\n",
      "epoch 9; iter: 600; batch classifier loss: 1.182803; batch adversarial loss: 0.532784\n",
      "epoch 9; iter: 800; batch classifier loss: 0.932121; batch adversarial loss: 0.494946\n",
      "epoch 9; iter: 1000; batch classifier loss: 1.015256; batch adversarial loss: 0.480209\n",
      "epoch 9; iter: 1200; batch classifier loss: 0.901326; batch adversarial loss: 0.462235\n",
      "epoch 9; iter: 1400; batch classifier loss: 0.621886; batch adversarial loss: 0.463823\n",
      "epoch 9; iter: 1600; batch classifier loss: 0.932760; batch adversarial loss: 0.482647\n",
      "epoch 9; iter: 1800; batch classifier loss: 1.012662; batch adversarial loss: 0.517902\n",
      "epoch 9; iter: 2000; batch classifier loss: 0.880005; batch adversarial loss: 0.501354\n",
      "epoch 9; iter: 2200; batch classifier loss: 1.030655; batch adversarial loss: 0.418195\n",
      "epoch 9; iter: 2400; batch classifier loss: 0.804947; batch adversarial loss: 0.357467\n",
      "epoch 9; iter: 2600; batch classifier loss: 0.957451; batch adversarial loss: 0.586960\n",
      "epoch 10; iter: 0; batch classifier loss: 0.997746; batch adversarial loss: 0.442844\n",
      "epoch 10; iter: 200; batch classifier loss: 0.869902; batch adversarial loss: 0.505718\n",
      "epoch 10; iter: 400; batch classifier loss: 1.178703; batch adversarial loss: 0.525278\n",
      "epoch 10; iter: 600; batch classifier loss: 0.912833; batch adversarial loss: 0.530713\n",
      "epoch 10; iter: 800; batch classifier loss: 0.986558; batch adversarial loss: 0.464675\n",
      "epoch 10; iter: 1000; batch classifier loss: 0.968736; batch adversarial loss: 0.416612\n",
      "epoch 10; iter: 1200; batch classifier loss: 1.015999; batch adversarial loss: 0.531216\n",
      "epoch 10; iter: 1400; batch classifier loss: 1.298189; batch adversarial loss: 0.530739\n",
      "epoch 10; iter: 1600; batch classifier loss: 1.059634; batch adversarial loss: 0.462788\n",
      "epoch 10; iter: 1800; batch classifier loss: 1.040646; batch adversarial loss: 0.487574\n",
      "epoch 10; iter: 2000; batch classifier loss: 1.012785; batch adversarial loss: 0.452347\n",
      "epoch 10; iter: 2200; batch classifier loss: 1.025274; batch adversarial loss: 0.558308\n",
      "epoch 10; iter: 2400; batch classifier loss: 1.448376; batch adversarial loss: 0.458758\n",
      "epoch 10; iter: 2600; batch classifier loss: 1.572526; batch adversarial loss: 0.443210\n",
      "epoch 11; iter: 0; batch classifier loss: 0.941975; batch adversarial loss: 0.448029\n",
      "epoch 11; iter: 200; batch classifier loss: 1.099857; batch adversarial loss: 0.520809\n",
      "epoch 11; iter: 400; batch classifier loss: 0.885004; batch adversarial loss: 0.409487\n",
      "epoch 11; iter: 600; batch classifier loss: 0.935606; batch adversarial loss: 0.448039\n",
      "epoch 11; iter: 800; batch classifier loss: 1.656088; batch adversarial loss: 0.486404\n",
      "epoch 11; iter: 1000; batch classifier loss: 1.370587; batch adversarial loss: 0.527056\n",
      "epoch 11; iter: 1200; batch classifier loss: 1.098205; batch adversarial loss: 0.519825\n",
      "epoch 11; iter: 1400; batch classifier loss: 0.997201; batch adversarial loss: 0.457255\n",
      "epoch 11; iter: 1600; batch classifier loss: 1.235890; batch adversarial loss: 0.446783\n",
      "epoch 11; iter: 1800; batch classifier loss: 1.064161; batch adversarial loss: 0.540186\n",
      "epoch 11; iter: 2000; batch classifier loss: 1.445380; batch adversarial loss: 0.578398\n",
      "epoch 11; iter: 2200; batch classifier loss: 1.239026; batch adversarial loss: 0.468405\n",
      "epoch 11; iter: 2400; batch classifier loss: 0.988107; batch adversarial loss: 0.483516\n",
      "epoch 11; iter: 2600; batch classifier loss: 0.973359; batch adversarial loss: 0.431796\n",
      "epoch 12; iter: 0; batch classifier loss: 1.119092; batch adversarial loss: 0.522840\n",
      "epoch 12; iter: 200; batch classifier loss: 1.251543; batch adversarial loss: 0.477401\n",
      "epoch 12; iter: 400; batch classifier loss: 0.771877; batch adversarial loss: 0.360279\n",
      "epoch 12; iter: 600; batch classifier loss: 0.795259; batch adversarial loss: 0.417926\n",
      "epoch 12; iter: 800; batch classifier loss: 1.106615; batch adversarial loss: 0.499402\n",
      "epoch 12; iter: 1000; batch classifier loss: 0.970211; batch adversarial loss: 0.432079\n",
      "epoch 12; iter: 1200; batch classifier loss: 1.293324; batch adversarial loss: 0.484241\n",
      "epoch 12; iter: 1400; batch classifier loss: 1.168626; batch adversarial loss: 0.493261\n",
      "epoch 12; iter: 1600; batch classifier loss: 1.155795; batch adversarial loss: 0.534743\n",
      "epoch 12; iter: 1800; batch classifier loss: 1.186142; batch adversarial loss: 0.392316\n",
      "epoch 12; iter: 2000; batch classifier loss: 1.496552; batch adversarial loss: 0.469932\n",
      "epoch 12; iter: 2200; batch classifier loss: 1.065080; batch adversarial loss: 0.558973\n",
      "epoch 12; iter: 2400; batch classifier loss: 1.140412; batch adversarial loss: 0.464855\n",
      "epoch 12; iter: 2600; batch classifier loss: 1.436020; batch adversarial loss: 0.521306\n",
      "epoch 13; iter: 0; batch classifier loss: 0.939341; batch adversarial loss: 0.544505\n",
      "epoch 13; iter: 200; batch classifier loss: 1.532977; batch adversarial loss: 0.531376\n",
      "epoch 13; iter: 400; batch classifier loss: 1.608513; batch adversarial loss: 0.432693\n",
      "epoch 13; iter: 600; batch classifier loss: 1.399170; batch adversarial loss: 0.538586\n",
      "epoch 13; iter: 800; batch classifier loss: 1.212171; batch adversarial loss: 0.462242\n",
      "epoch 13; iter: 1000; batch classifier loss: 1.178684; batch adversarial loss: 0.436255\n",
      "epoch 13; iter: 1200; batch classifier loss: 1.399000; batch adversarial loss: 0.523102\n",
      "epoch 13; iter: 1400; batch classifier loss: 1.425562; batch adversarial loss: 0.526373\n",
      "epoch 13; iter: 1600; batch classifier loss: 1.000493; batch adversarial loss: 0.420952\n",
      "epoch 13; iter: 1800; batch classifier loss: 1.327440; batch adversarial loss: 0.535437\n",
      "epoch 13; iter: 2000; batch classifier loss: 1.297678; batch adversarial loss: 0.507074\n",
      "epoch 13; iter: 2200; batch classifier loss: 1.298871; batch adversarial loss: 0.528816\n",
      "epoch 13; iter: 2400; batch classifier loss: 0.973249; batch adversarial loss: 0.404490\n",
      "epoch 13; iter: 2600; batch classifier loss: 1.605839; batch adversarial loss: 0.521798\n",
      "epoch 14; iter: 0; batch classifier loss: 0.975201; batch adversarial loss: 0.358810\n",
      "epoch 14; iter: 200; batch classifier loss: 1.243903; batch adversarial loss: 0.443140\n",
      "epoch 14; iter: 400; batch classifier loss: 1.238745; batch adversarial loss: 0.459492\n",
      "epoch 14; iter: 600; batch classifier loss: 1.105352; batch adversarial loss: 0.520623\n",
      "epoch 14; iter: 800; batch classifier loss: 1.068494; batch adversarial loss: 0.467358\n",
      "epoch 14; iter: 1000; batch classifier loss: 1.699463; batch adversarial loss: 0.447566\n",
      "epoch 14; iter: 1200; batch classifier loss: 1.108819; batch adversarial loss: 0.507975\n",
      "epoch 14; iter: 1400; batch classifier loss: 1.480843; batch adversarial loss: 0.472797\n",
      "epoch 14; iter: 1600; batch classifier loss: 1.201285; batch adversarial loss: 0.466432\n",
      "epoch 14; iter: 1800; batch classifier loss: 1.221320; batch adversarial loss: 0.488307\n",
      "epoch 14; iter: 2000; batch classifier loss: 1.254353; batch adversarial loss: 0.470536\n",
      "epoch 14; iter: 2200; batch classifier loss: 1.301529; batch adversarial loss: 0.467176\n",
      "epoch 14; iter: 2400; batch classifier loss: 2.016123; batch adversarial loss: 0.505674\n",
      "epoch 14; iter: 2600; batch classifier loss: 0.873086; batch adversarial loss: 0.422780\n",
      "epoch 15; iter: 0; batch classifier loss: 1.273733; batch adversarial loss: 0.468440\n",
      "epoch 15; iter: 200; batch classifier loss: 1.577902; batch adversarial loss: 0.501982\n",
      "epoch 15; iter: 400; batch classifier loss: 1.167179; batch adversarial loss: 0.487192\n",
      "epoch 15; iter: 600; batch classifier loss: 1.210056; batch adversarial loss: 0.448902\n",
      "epoch 15; iter: 800; batch classifier loss: 1.121073; batch adversarial loss: 0.418917\n",
      "epoch 15; iter: 1000; batch classifier loss: 1.047843; batch adversarial loss: 0.508328\n",
      "epoch 15; iter: 1200; batch classifier loss: 0.960251; batch adversarial loss: 0.399978\n",
      "epoch 15; iter: 1400; batch classifier loss: 1.375499; batch adversarial loss: 0.489602\n",
      "epoch 15; iter: 1600; batch classifier loss: 1.179296; batch adversarial loss: 0.525254\n",
      "epoch 15; iter: 1800; batch classifier loss: 1.128486; batch adversarial loss: 0.516968\n",
      "epoch 15; iter: 2000; batch classifier loss: 0.905842; batch adversarial loss: 0.395326\n",
      "epoch 15; iter: 2200; batch classifier loss: 1.016946; batch adversarial loss: 0.447342\n",
      "epoch 15; iter: 2400; batch classifier loss: 1.220909; batch adversarial loss: 0.490946\n",
      "epoch 15; iter: 2600; batch classifier loss: 1.496711; batch adversarial loss: 0.462572\n",
      "epoch 16; iter: 0; batch classifier loss: 1.033060; batch adversarial loss: 0.554503\n",
      "epoch 16; iter: 200; batch classifier loss: 0.822216; batch adversarial loss: 0.393404\n",
      "epoch 16; iter: 400; batch classifier loss: 1.472122; batch adversarial loss: 0.490922\n",
      "epoch 16; iter: 600; batch classifier loss: 1.080550; batch adversarial loss: 0.538131\n",
      "epoch 16; iter: 800; batch classifier loss: 1.131332; batch adversarial loss: 0.510328\n",
      "epoch 16; iter: 1000; batch classifier loss: 1.257231; batch adversarial loss: 0.468822\n",
      "epoch 16; iter: 1200; batch classifier loss: 1.696025; batch adversarial loss: 0.478796\n",
      "epoch 16; iter: 1400; batch classifier loss: 1.613153; batch adversarial loss: 0.529368\n",
      "epoch 16; iter: 1600; batch classifier loss: 1.213858; batch adversarial loss: 0.551155\n",
      "epoch 16; iter: 1800; batch classifier loss: 1.487354; batch adversarial loss: 0.451954\n",
      "epoch 16; iter: 2000; batch classifier loss: 1.091049; batch adversarial loss: 0.452697\n",
      "epoch 16; iter: 2200; batch classifier loss: 1.471422; batch adversarial loss: 0.457178\n",
      "epoch 16; iter: 2400; batch classifier loss: 1.291584; batch adversarial loss: 0.505674\n",
      "epoch 16; iter: 2600; batch classifier loss: 1.313579; batch adversarial loss: 0.484626\n",
      "epoch 17; iter: 0; batch classifier loss: 1.404515; batch adversarial loss: 0.527540\n",
      "epoch 17; iter: 200; batch classifier loss: 1.094401; batch adversarial loss: 0.465412\n",
      "epoch 17; iter: 400; batch classifier loss: 1.448950; batch adversarial loss: 0.354318\n",
      "epoch 17; iter: 600; batch classifier loss: 0.954885; batch adversarial loss: 0.369215\n",
      "epoch 17; iter: 800; batch classifier loss: 1.166490; batch adversarial loss: 0.436391\n",
      "epoch 17; iter: 1000; batch classifier loss: 1.886755; batch adversarial loss: 0.527078\n",
      "epoch 17; iter: 1200; batch classifier loss: 1.748694; batch adversarial loss: 0.441860\n",
      "epoch 17; iter: 1400; batch classifier loss: 1.344826; batch adversarial loss: 0.595756\n",
      "epoch 17; iter: 1600; batch classifier loss: 1.415034; batch adversarial loss: 0.419253\n",
      "epoch 17; iter: 1800; batch classifier loss: 0.962923; batch adversarial loss: 0.417113\n",
      "epoch 17; iter: 2000; batch classifier loss: 1.067397; batch adversarial loss: 0.375512\n",
      "epoch 17; iter: 2200; batch classifier loss: 1.482993; batch adversarial loss: 0.443669\n",
      "epoch 17; iter: 2400; batch classifier loss: 0.931562; batch adversarial loss: 0.403628\n",
      "epoch 17; iter: 2600; batch classifier loss: 1.492644; batch adversarial loss: 0.523919\n",
      "epoch 18; iter: 0; batch classifier loss: 1.483184; batch adversarial loss: 0.529943\n",
      "epoch 18; iter: 200; batch classifier loss: 1.496531; batch adversarial loss: 0.465857\n",
      "epoch 18; iter: 400; batch classifier loss: 1.021314; batch adversarial loss: 0.413571\n",
      "epoch 18; iter: 600; batch classifier loss: 1.684004; batch adversarial loss: 0.390375\n",
      "epoch 18; iter: 800; batch classifier loss: 1.189548; batch adversarial loss: 0.453177\n",
      "epoch 18; iter: 1000; batch classifier loss: 1.876590; batch adversarial loss: 0.532126\n",
      "epoch 18; iter: 1200; batch classifier loss: 1.082518; batch adversarial loss: 0.401778\n",
      "epoch 18; iter: 1400; batch classifier loss: 1.637236; batch adversarial loss: 0.477500\n",
      "epoch 18; iter: 1600; batch classifier loss: 1.413583; batch adversarial loss: 0.506227\n",
      "epoch 18; iter: 1800; batch classifier loss: 1.153998; batch adversarial loss: 0.415295\n",
      "epoch 18; iter: 2000; batch classifier loss: 1.477979; batch adversarial loss: 0.513485\n",
      "epoch 18; iter: 2200; batch classifier loss: 1.746743; batch adversarial loss: 0.517586\n",
      "epoch 18; iter: 2400; batch classifier loss: 2.599613; batch adversarial loss: 0.562378\n",
      "epoch 18; iter: 2600; batch classifier loss: 1.349344; batch adversarial loss: 0.425369\n",
      "epoch 19; iter: 0; batch classifier loss: 1.363044; batch adversarial loss: 0.431278\n",
      "epoch 19; iter: 200; batch classifier loss: 1.484600; batch adversarial loss: 0.473247\n",
      "epoch 19; iter: 400; batch classifier loss: 1.470071; batch adversarial loss: 0.499480\n",
      "epoch 19; iter: 600; batch classifier loss: 1.211412; batch adversarial loss: 0.507789\n",
      "epoch 19; iter: 800; batch classifier loss: 1.117623; batch adversarial loss: 0.487231\n",
      "epoch 19; iter: 1000; batch classifier loss: 1.323264; batch adversarial loss: 0.470523\n",
      "epoch 19; iter: 1200; batch classifier loss: 1.404212; batch adversarial loss: 0.525149\n",
      "epoch 19; iter: 1400; batch classifier loss: 1.174630; batch adversarial loss: 0.446240\n",
      "epoch 19; iter: 1600; batch classifier loss: 1.598532; batch adversarial loss: 0.415929\n",
      "epoch 19; iter: 1800; batch classifier loss: 0.872441; batch adversarial loss: 0.418692\n",
      "epoch 19; iter: 2000; batch classifier loss: 1.328719; batch adversarial loss: 0.450371\n",
      "epoch 19; iter: 2200; batch classifier loss: 1.524838; batch adversarial loss: 0.412488\n",
      "epoch 19; iter: 2400; batch classifier loss: 1.186835; batch adversarial loss: 0.414776\n",
      "epoch 19; iter: 2600; batch classifier loss: 1.024918; batch adversarial loss: 0.517195\n",
      "epoch 20; iter: 0; batch classifier loss: 1.677059; batch adversarial loss: 0.509317\n",
      "epoch 20; iter: 200; batch classifier loss: 1.929600; batch adversarial loss: 0.468763\n",
      "epoch 20; iter: 400; batch classifier loss: 1.128597; batch adversarial loss: 0.631603\n",
      "epoch 20; iter: 600; batch classifier loss: 1.835263; batch adversarial loss: 0.462555\n",
      "epoch 20; iter: 800; batch classifier loss: 1.073680; batch adversarial loss: 0.439634\n",
      "epoch 20; iter: 1000; batch classifier loss: 1.505633; batch adversarial loss: 0.539615\n",
      "epoch 20; iter: 1200; batch classifier loss: 1.308541; batch adversarial loss: 0.540236\n",
      "epoch 20; iter: 1400; batch classifier loss: 1.409693; batch adversarial loss: 0.549364\n",
      "epoch 20; iter: 1600; batch classifier loss: 1.692748; batch adversarial loss: 0.490774\n",
      "epoch 20; iter: 1800; batch classifier loss: 1.112567; batch adversarial loss: 0.455824\n",
      "epoch 20; iter: 2000; batch classifier loss: 1.392387; batch adversarial loss: 0.433344\n",
      "epoch 20; iter: 2200; batch classifier loss: 1.172405; batch adversarial loss: 0.461010\n",
      "epoch 20; iter: 2400; batch classifier loss: 1.521740; batch adversarial loss: 0.445391\n",
      "epoch 20; iter: 2600; batch classifier loss: 1.317647; batch adversarial loss: 0.465113\n",
      "epoch 21; iter: 0; batch classifier loss: 1.535231; batch adversarial loss: 0.446269\n",
      "epoch 21; iter: 200; batch classifier loss: 1.645575; batch adversarial loss: 0.569415\n",
      "epoch 21; iter: 400; batch classifier loss: 1.340269; batch adversarial loss: 0.503574\n",
      "epoch 21; iter: 600; batch classifier loss: 1.381835; batch adversarial loss: 0.493834\n",
      "epoch 21; iter: 800; batch classifier loss: 2.040473; batch adversarial loss: 0.574399\n",
      "epoch 21; iter: 1000; batch classifier loss: 1.464378; batch adversarial loss: 0.515770\n",
      "epoch 21; iter: 1200; batch classifier loss: 1.385038; batch adversarial loss: 0.440454\n",
      "epoch 21; iter: 1400; batch classifier loss: 1.780221; batch adversarial loss: 0.476226\n",
      "epoch 21; iter: 1600; batch classifier loss: 1.441196; batch adversarial loss: 0.441519\n",
      "epoch 21; iter: 1800; batch classifier loss: 1.703707; batch adversarial loss: 0.468467\n",
      "epoch 21; iter: 2000; batch classifier loss: 1.993618; batch adversarial loss: 0.476112\n",
      "epoch 21; iter: 2200; batch classifier loss: 1.733605; batch adversarial loss: 0.400923\n",
      "epoch 21; iter: 2400; batch classifier loss: 1.465254; batch adversarial loss: 0.419644\n",
      "epoch 21; iter: 2600; batch classifier loss: 1.814344; batch adversarial loss: 0.522776\n",
      "epoch 22; iter: 0; batch classifier loss: 1.013153; batch adversarial loss: 0.572260\n",
      "epoch 22; iter: 200; batch classifier loss: 1.331731; batch adversarial loss: 0.492407\n",
      "epoch 22; iter: 400; batch classifier loss: 1.680805; batch adversarial loss: 0.460013\n",
      "epoch 22; iter: 600; batch classifier loss: 1.051536; batch adversarial loss: 0.514771\n",
      "epoch 22; iter: 800; batch classifier loss: 1.433421; batch adversarial loss: 0.516458\n",
      "epoch 22; iter: 1000; batch classifier loss: 1.556386; batch adversarial loss: 0.468121\n",
      "epoch 22; iter: 1200; batch classifier loss: 1.520817; batch adversarial loss: 0.539276\n",
      "epoch 22; iter: 1400; batch classifier loss: 1.515596; batch adversarial loss: 0.508139\n",
      "epoch 22; iter: 1600; batch classifier loss: 1.870148; batch adversarial loss: 0.549616\n",
      "epoch 22; iter: 1800; batch classifier loss: 1.324522; batch adversarial loss: 0.499639\n",
      "epoch 22; iter: 2000; batch classifier loss: 1.367162; batch adversarial loss: 0.470018\n",
      "epoch 22; iter: 2200; batch classifier loss: 1.584954; batch adversarial loss: 0.406275\n",
      "epoch 22; iter: 2400; batch classifier loss: 1.711389; batch adversarial loss: 0.420930\n",
      "epoch 22; iter: 2600; batch classifier loss: 1.538832; batch adversarial loss: 0.533701\n",
      "epoch 23; iter: 0; batch classifier loss: 2.031392; batch adversarial loss: 0.523258\n",
      "epoch 23; iter: 200; batch classifier loss: 1.212308; batch adversarial loss: 0.410927\n",
      "epoch 23; iter: 400; batch classifier loss: 1.324139; batch adversarial loss: 0.437825\n",
      "epoch 23; iter: 600; batch classifier loss: 1.660820; batch adversarial loss: 0.613836\n",
      "epoch 23; iter: 800; batch classifier loss: 1.360090; batch adversarial loss: 0.437099\n",
      "epoch 23; iter: 1000; batch classifier loss: 1.454141; batch adversarial loss: 0.494452\n",
      "epoch 23; iter: 1200; batch classifier loss: 1.459542; batch adversarial loss: 0.496711\n",
      "epoch 23; iter: 1400; batch classifier loss: 1.906964; batch adversarial loss: 0.531553\n",
      "epoch 23; iter: 1600; batch classifier loss: 1.813308; batch adversarial loss: 0.512645\n",
      "epoch 23; iter: 1800; batch classifier loss: 1.919761; batch adversarial loss: 0.408538\n",
      "epoch 23; iter: 2000; batch classifier loss: 1.331211; batch adversarial loss: 0.511837\n",
      "epoch 23; iter: 2200; batch classifier loss: 2.149093; batch adversarial loss: 0.514085\n",
      "epoch 23; iter: 2400; batch classifier loss: 1.449532; batch adversarial loss: 0.479480\n",
      "epoch 23; iter: 2600; batch classifier loss: 1.301931; batch adversarial loss: 0.426625\n",
      "epoch 24; iter: 0; batch classifier loss: 1.814953; batch adversarial loss: 0.389881\n",
      "epoch 24; iter: 200; batch classifier loss: 1.215012; batch adversarial loss: 0.420443\n",
      "epoch 24; iter: 400; batch classifier loss: 1.389835; batch adversarial loss: 0.432302\n",
      "epoch 24; iter: 600; batch classifier loss: 1.236377; batch adversarial loss: 0.478447\n",
      "epoch 24; iter: 800; batch classifier loss: 1.875731; batch adversarial loss: 0.413383\n",
      "epoch 24; iter: 1000; batch classifier loss: 1.467294; batch adversarial loss: 0.429850\n",
      "epoch 24; iter: 1200; batch classifier loss: 1.091766; batch adversarial loss: 0.417962\n",
      "epoch 24; iter: 1400; batch classifier loss: 1.202062; batch adversarial loss: 0.417536\n",
      "epoch 24; iter: 1600; batch classifier loss: 1.424316; batch adversarial loss: 0.517145\n",
      "epoch 24; iter: 1800; batch classifier loss: 1.081568; batch adversarial loss: 0.553116\n",
      "epoch 24; iter: 2000; batch classifier loss: 1.154783; batch adversarial loss: 0.470680\n",
      "epoch 24; iter: 2200; batch classifier loss: 0.957009; batch adversarial loss: 0.424648\n",
      "epoch 24; iter: 2400; batch classifier loss: 1.344468; batch adversarial loss: 0.408930\n",
      "epoch 24; iter: 2600; batch classifier loss: 0.954991; batch adversarial loss: 0.430825\n",
      "epoch 25; iter: 0; batch classifier loss: 1.464612; batch adversarial loss: 0.501933\n",
      "epoch 25; iter: 200; batch classifier loss: 1.413072; batch adversarial loss: 0.390828\n",
      "epoch 25; iter: 400; batch classifier loss: 1.620415; batch adversarial loss: 0.435996\n",
      "epoch 25; iter: 600; batch classifier loss: 1.578485; batch adversarial loss: 0.557697\n",
      "epoch 25; iter: 800; batch classifier loss: 1.177158; batch adversarial loss: 0.398672\n",
      "epoch 25; iter: 1000; batch classifier loss: 1.947003; batch adversarial loss: 0.524363\n",
      "epoch 25; iter: 1200; batch classifier loss: 1.126219; batch adversarial loss: 0.423992\n",
      "epoch 25; iter: 1400; batch classifier loss: 1.287010; batch adversarial loss: 0.530033\n",
      "epoch 25; iter: 1600; batch classifier loss: 1.685710; batch adversarial loss: 0.484779\n",
      "epoch 25; iter: 1800; batch classifier loss: 1.515799; batch adversarial loss: 0.559208\n",
      "epoch 25; iter: 2000; batch classifier loss: 1.273608; batch adversarial loss: 0.348566\n",
      "epoch 25; iter: 2200; batch classifier loss: 1.638933; batch adversarial loss: 0.529550\n",
      "epoch 25; iter: 2400; batch classifier loss: 1.357456; batch adversarial loss: 0.499663\n",
      "epoch 25; iter: 2600; batch classifier loss: 1.540061; batch adversarial loss: 0.403827\n",
      "epoch 26; iter: 0; batch classifier loss: 1.519459; batch adversarial loss: 0.436232\n",
      "epoch 26; iter: 200; batch classifier loss: 1.444039; batch adversarial loss: 0.486792\n",
      "epoch 26; iter: 400; batch classifier loss: 1.131233; batch adversarial loss: 0.462885\n",
      "epoch 26; iter: 600; batch classifier loss: 1.130988; batch adversarial loss: 0.401099\n",
      "epoch 26; iter: 800; batch classifier loss: 1.376102; batch adversarial loss: 0.681079\n",
      "epoch 26; iter: 1000; batch classifier loss: 1.240665; batch adversarial loss: 0.431670\n",
      "epoch 26; iter: 1200; batch classifier loss: 1.423476; batch adversarial loss: 0.467762\n",
      "epoch 26; iter: 1400; batch classifier loss: 1.485794; batch adversarial loss: 0.456934\n",
      "epoch 26; iter: 1600; batch classifier loss: 1.165430; batch adversarial loss: 0.495660\n",
      "epoch 26; iter: 1800; batch classifier loss: 1.496822; batch adversarial loss: 0.441098\n",
      "epoch 26; iter: 2000; batch classifier loss: 1.188074; batch adversarial loss: 0.470085\n",
      "epoch 26; iter: 2200; batch classifier loss: 1.610685; batch adversarial loss: 0.484199\n",
      "epoch 26; iter: 2400; batch classifier loss: 1.386737; batch adversarial loss: 0.398909\n",
      "epoch 26; iter: 2600; batch classifier loss: 1.541887; batch adversarial loss: 0.477380\n",
      "epoch 27; iter: 0; batch classifier loss: 1.670223; batch adversarial loss: 0.509800\n",
      "epoch 27; iter: 200; batch classifier loss: 1.027899; batch adversarial loss: 0.381192\n",
      "epoch 27; iter: 400; batch classifier loss: 1.461266; batch adversarial loss: 0.457431\n",
      "epoch 27; iter: 600; batch classifier loss: 1.411420; batch adversarial loss: 0.388793\n",
      "epoch 27; iter: 800; batch classifier loss: 2.538426; batch adversarial loss: 0.530473\n",
      "epoch 27; iter: 1000; batch classifier loss: 1.378431; batch adversarial loss: 0.457355\n",
      "epoch 27; iter: 1200; batch classifier loss: 2.209529; batch adversarial loss: 0.572826\n",
      "epoch 27; iter: 1400; batch classifier loss: 1.860879; batch adversarial loss: 0.603902\n",
      "epoch 27; iter: 1600; batch classifier loss: 1.810845; batch adversarial loss: 0.480455\n",
      "epoch 27; iter: 1800; batch classifier loss: 1.213862; batch adversarial loss: 0.399882\n",
      "epoch 27; iter: 2000; batch classifier loss: 1.201553; batch adversarial loss: 0.370545\n",
      "epoch 27; iter: 2200; batch classifier loss: 1.325277; batch adversarial loss: 0.441171\n",
      "epoch 27; iter: 2400; batch classifier loss: 1.402352; batch adversarial loss: 0.383421\n",
      "epoch 27; iter: 2600; batch classifier loss: 1.353719; batch adversarial loss: 0.534034\n",
      "epoch 28; iter: 0; batch classifier loss: 1.375085; batch adversarial loss: 0.467429\n",
      "epoch 28; iter: 200; batch classifier loss: 1.952165; batch adversarial loss: 0.578161\n",
      "epoch 28; iter: 400; batch classifier loss: 1.231859; batch adversarial loss: 0.411932\n",
      "epoch 28; iter: 600; batch classifier loss: 1.430430; batch adversarial loss: 0.496689\n",
      "epoch 28; iter: 800; batch classifier loss: 0.983516; batch adversarial loss: 0.457998\n",
      "epoch 28; iter: 1000; batch classifier loss: 1.194926; batch adversarial loss: 0.505544\n",
      "epoch 28; iter: 1200; batch classifier loss: 1.953080; batch adversarial loss: 0.559257\n",
      "epoch 28; iter: 1400; batch classifier loss: 1.273360; batch adversarial loss: 0.431363\n",
      "epoch 28; iter: 1600; batch classifier loss: 1.705475; batch adversarial loss: 0.549433\n",
      "epoch 28; iter: 1800; batch classifier loss: 1.879523; batch adversarial loss: 0.484843\n",
      "epoch 28; iter: 2000; batch classifier loss: 1.395124; batch adversarial loss: 0.476824\n",
      "epoch 28; iter: 2200; batch classifier loss: 1.733224; batch adversarial loss: 0.551733\n",
      "epoch 28; iter: 2400; batch classifier loss: 1.769585; batch adversarial loss: 0.477684\n",
      "epoch 28; iter: 2600; batch classifier loss: 0.939536; batch adversarial loss: 0.378868\n",
      "epoch 29; iter: 0; batch classifier loss: 1.837348; batch adversarial loss: 0.521980\n",
      "epoch 29; iter: 200; batch classifier loss: 1.618384; batch adversarial loss: 0.457272\n",
      "epoch 29; iter: 400; batch classifier loss: 0.882210; batch adversarial loss: 0.469051\n",
      "epoch 29; iter: 600; batch classifier loss: 1.438235; batch adversarial loss: 0.430897\n",
      "epoch 29; iter: 800; batch classifier loss: 1.314360; batch adversarial loss: 0.471758\n",
      "epoch 29; iter: 1000; batch classifier loss: 1.365482; batch adversarial loss: 0.466315\n",
      "epoch 29; iter: 1200; batch classifier loss: 2.070418; batch adversarial loss: 0.464460\n",
      "epoch 29; iter: 1400; batch classifier loss: 1.484367; batch adversarial loss: 0.542075\n",
      "epoch 29; iter: 1600; batch classifier loss: 1.967955; batch adversarial loss: 0.539766\n",
      "epoch 29; iter: 1800; batch classifier loss: 1.238120; batch adversarial loss: 0.516839\n",
      "epoch 29; iter: 2000; batch classifier loss: 1.428632; batch adversarial loss: 0.501517\n",
      "epoch 29; iter: 2200; batch classifier loss: 1.193795; batch adversarial loss: 0.359673\n",
      "epoch 29; iter: 2400; batch classifier loss: 1.426579; batch adversarial loss: 0.453875\n",
      "epoch 29; iter: 2600; batch classifier loss: 1.290562; batch adversarial loss: 0.409074\n",
      "epoch 30; iter: 0; batch classifier loss: 2.160647; batch adversarial loss: 0.450495\n",
      "epoch 30; iter: 200; batch classifier loss: 1.750340; batch adversarial loss: 0.409369\n",
      "epoch 30; iter: 400; batch classifier loss: 1.745375; batch adversarial loss: 0.498743\n",
      "epoch 30; iter: 600; batch classifier loss: 1.481642; batch adversarial loss: 0.398150\n",
      "epoch 30; iter: 800; batch classifier loss: 1.944140; batch adversarial loss: 0.399312\n",
      "epoch 30; iter: 1000; batch classifier loss: 1.659213; batch adversarial loss: 0.377339\n",
      "epoch 30; iter: 1200; batch classifier loss: 1.645256; batch adversarial loss: 0.499882\n",
      "epoch 30; iter: 1400; batch classifier loss: 1.756060; batch adversarial loss: 0.446563\n",
      "epoch 30; iter: 1600; batch classifier loss: 0.709842; batch adversarial loss: 0.406812\n",
      "epoch 30; iter: 1800; batch classifier loss: 1.389931; batch adversarial loss: 0.479721\n",
      "epoch 30; iter: 2000; batch classifier loss: 1.739422; batch adversarial loss: 0.476038\n",
      "epoch 30; iter: 2200; batch classifier loss: 1.609075; batch adversarial loss: 0.504120\n",
      "epoch 30; iter: 2400; batch classifier loss: 1.392069; batch adversarial loss: 0.432483\n",
      "epoch 30; iter: 2600; batch classifier loss: 2.167475; batch adversarial loss: 0.504422\n",
      "epoch 31; iter: 0; batch classifier loss: 1.790081; batch adversarial loss: 0.459594\n",
      "epoch 31; iter: 200; batch classifier loss: 1.204057; batch adversarial loss: 0.560340\n",
      "epoch 31; iter: 400; batch classifier loss: 1.390101; batch adversarial loss: 0.520278\n",
      "epoch 31; iter: 600; batch classifier loss: 1.357096; batch adversarial loss: 0.413289\n",
      "epoch 31; iter: 800; batch classifier loss: 1.277956; batch adversarial loss: 0.423368\n",
      "epoch 31; iter: 1000; batch classifier loss: 1.207487; batch adversarial loss: 0.403464\n",
      "epoch 31; iter: 1200; batch classifier loss: 1.560302; batch adversarial loss: 0.452152\n",
      "epoch 31; iter: 1400; batch classifier loss: 1.436263; batch adversarial loss: 0.411611\n",
      "epoch 31; iter: 1600; batch classifier loss: 1.465221; batch adversarial loss: 0.401935\n",
      "epoch 31; iter: 1800; batch classifier loss: 1.730308; batch adversarial loss: 0.566018\n",
      "epoch 31; iter: 2000; batch classifier loss: 1.203867; batch adversarial loss: 0.494615\n",
      "epoch 31; iter: 2200; batch classifier loss: 1.522219; batch adversarial loss: 0.410257\n",
      "epoch 31; iter: 2400; batch classifier loss: 1.536813; batch adversarial loss: 0.477001\n",
      "epoch 31; iter: 2600; batch classifier loss: 1.633187; batch adversarial loss: 0.416496\n",
      "epoch 32; iter: 0; batch classifier loss: 2.283353; batch adversarial loss: 0.547335\n",
      "epoch 32; iter: 200; batch classifier loss: 2.274915; batch adversarial loss: 0.421311\n",
      "epoch 32; iter: 400; batch classifier loss: 1.464364; batch adversarial loss: 0.470376\n",
      "epoch 32; iter: 600; batch classifier loss: 1.482542; batch adversarial loss: 0.502654\n",
      "epoch 32; iter: 800; batch classifier loss: 1.923695; batch adversarial loss: 0.500912\n",
      "epoch 32; iter: 1000; batch classifier loss: 1.545497; batch adversarial loss: 0.454612\n",
      "epoch 32; iter: 1200; batch classifier loss: 1.531305; batch adversarial loss: 0.519596\n",
      "epoch 32; iter: 1400; batch classifier loss: 1.574724; batch adversarial loss: 0.568401\n",
      "epoch 32; iter: 1600; batch classifier loss: 1.747765; batch adversarial loss: 0.447511\n",
      "epoch 32; iter: 1800; batch classifier loss: 1.461558; batch adversarial loss: 0.362842\n",
      "epoch 32; iter: 2000; batch classifier loss: 2.459130; batch adversarial loss: 0.528050\n",
      "epoch 32; iter: 2200; batch classifier loss: 1.445903; batch adversarial loss: 0.450022\n",
      "epoch 32; iter: 2400; batch classifier loss: 1.798139; batch adversarial loss: 0.466293\n",
      "epoch 32; iter: 2600; batch classifier loss: 0.987217; batch adversarial loss: 0.567166\n",
      "epoch 33; iter: 0; batch classifier loss: 1.452898; batch adversarial loss: 0.556343\n",
      "epoch 33; iter: 200; batch classifier loss: 1.398960; batch adversarial loss: 0.485688\n",
      "epoch 33; iter: 400; batch classifier loss: 2.357808; batch adversarial loss: 0.473816\n",
      "epoch 33; iter: 600; batch classifier loss: 1.443694; batch adversarial loss: 0.397350\n",
      "epoch 33; iter: 800; batch classifier loss: 1.406093; batch adversarial loss: 0.406634\n",
      "epoch 33; iter: 1000; batch classifier loss: 1.869519; batch adversarial loss: 0.476045\n",
      "epoch 33; iter: 1200; batch classifier loss: 1.397440; batch adversarial loss: 0.504377\n",
      "epoch 33; iter: 1400; batch classifier loss: 1.739928; batch adversarial loss: 0.482569\n",
      "epoch 33; iter: 1600; batch classifier loss: 1.489346; batch adversarial loss: 0.537590\n",
      "epoch 33; iter: 1800; batch classifier loss: 1.787965; batch adversarial loss: 0.398490\n",
      "epoch 33; iter: 2000; batch classifier loss: 1.542015; batch adversarial loss: 0.508960\n",
      "epoch 33; iter: 2200; batch classifier loss: 1.227027; batch adversarial loss: 0.467974\n",
      "epoch 33; iter: 2400; batch classifier loss: 1.027803; batch adversarial loss: 0.463152\n",
      "epoch 33; iter: 2600; batch classifier loss: 1.721195; batch adversarial loss: 0.529764\n",
      "epoch 34; iter: 0; batch classifier loss: 1.157583; batch adversarial loss: 0.364027\n",
      "epoch 34; iter: 200; batch classifier loss: 1.214466; batch adversarial loss: 0.510032\n",
      "epoch 34; iter: 400; batch classifier loss: 1.364051; batch adversarial loss: 0.506207\n",
      "epoch 34; iter: 600; batch classifier loss: 1.761486; batch adversarial loss: 0.359949\n",
      "epoch 34; iter: 800; batch classifier loss: 1.720809; batch adversarial loss: 0.573887\n",
      "epoch 34; iter: 1000; batch classifier loss: 1.668381; batch adversarial loss: 0.437058\n",
      "epoch 34; iter: 1200; batch classifier loss: 1.149054; batch adversarial loss: 0.461087\n",
      "epoch 34; iter: 1400; batch classifier loss: 2.005579; batch adversarial loss: 0.449763\n",
      "epoch 34; iter: 1600; batch classifier loss: 1.374330; batch adversarial loss: 0.382345\n",
      "epoch 34; iter: 1800; batch classifier loss: 1.474371; batch adversarial loss: 0.533337\n",
      "epoch 34; iter: 2000; batch classifier loss: 1.191151; batch adversarial loss: 0.422505\n",
      "epoch 34; iter: 2200; batch classifier loss: 1.507129; batch adversarial loss: 0.517263\n",
      "epoch 34; iter: 2400; batch classifier loss: 1.289942; batch adversarial loss: 0.498799\n",
      "epoch 34; iter: 2600; batch classifier loss: 1.147056; batch adversarial loss: 0.400618\n",
      "epoch 35; iter: 0; batch classifier loss: 1.592967; batch adversarial loss: 0.538901\n",
      "epoch 35; iter: 200; batch classifier loss: 1.240942; batch adversarial loss: 0.439755\n",
      "epoch 35; iter: 400; batch classifier loss: 0.897996; batch adversarial loss: 0.444676\n",
      "epoch 35; iter: 600; batch classifier loss: 1.297973; batch adversarial loss: 0.379817\n",
      "epoch 35; iter: 800; batch classifier loss: 1.026192; batch adversarial loss: 0.535561\n",
      "epoch 35; iter: 1000; batch classifier loss: 1.300214; batch adversarial loss: 0.491002\n",
      "epoch 35; iter: 1200; batch classifier loss: 1.027291; batch adversarial loss: 0.443803\n",
      "epoch 35; iter: 1400; batch classifier loss: 1.928504; batch adversarial loss: 0.554907\n",
      "epoch 35; iter: 1600; batch classifier loss: 1.088176; batch adversarial loss: 0.451209\n",
      "epoch 35; iter: 1800; batch classifier loss: 1.542489; batch adversarial loss: 0.469345\n",
      "epoch 35; iter: 2000; batch classifier loss: 1.928069; batch adversarial loss: 0.435459\n",
      "epoch 35; iter: 2200; batch classifier loss: 1.404702; batch adversarial loss: 0.410831\n",
      "epoch 35; iter: 2400; batch classifier loss: 1.317902; batch adversarial loss: 0.395072\n",
      "epoch 35; iter: 2600; batch classifier loss: 1.644290; batch adversarial loss: 0.543658\n",
      "epoch 36; iter: 0; batch classifier loss: 1.272919; batch adversarial loss: 0.520997\n",
      "epoch 36; iter: 200; batch classifier loss: 1.454398; batch adversarial loss: 0.449424\n",
      "epoch 36; iter: 400; batch classifier loss: 1.566136; batch adversarial loss: 0.489693\n",
      "epoch 36; iter: 600; batch classifier loss: 1.148254; batch adversarial loss: 0.494771\n",
      "epoch 36; iter: 800; batch classifier loss: 2.053019; batch adversarial loss: 0.449184\n",
      "epoch 36; iter: 1000; batch classifier loss: 1.562389; batch adversarial loss: 0.498811\n",
      "epoch 36; iter: 1200; batch classifier loss: 1.569044; batch adversarial loss: 0.424195\n",
      "epoch 36; iter: 1400; batch classifier loss: 1.300549; batch adversarial loss: 0.422797\n",
      "epoch 36; iter: 1600; batch classifier loss: 1.242488; batch adversarial loss: 0.403079\n",
      "epoch 36; iter: 1800; batch classifier loss: 2.012902; batch adversarial loss: 0.426434\n",
      "epoch 36; iter: 2000; batch classifier loss: 1.481799; batch adversarial loss: 0.443911\n",
      "epoch 36; iter: 2200; batch classifier loss: 1.172253; batch adversarial loss: 0.456230\n",
      "epoch 36; iter: 2400; batch classifier loss: 1.353615; batch adversarial loss: 0.473083\n",
      "epoch 36; iter: 2600; batch classifier loss: 1.739111; batch adversarial loss: 0.527648\n",
      "epoch 37; iter: 0; batch classifier loss: 1.597734; batch adversarial loss: 0.501402\n",
      "epoch 37; iter: 200; batch classifier loss: 1.562732; batch adversarial loss: 0.462103\n",
      "epoch 37; iter: 400; batch classifier loss: 1.206844; batch adversarial loss: 0.545433\n",
      "epoch 37; iter: 600; batch classifier loss: 1.265927; batch adversarial loss: 0.418544\n",
      "epoch 37; iter: 800; batch classifier loss: 1.716119; batch adversarial loss: 0.418111\n",
      "epoch 37; iter: 1000; batch classifier loss: 1.365489; batch adversarial loss: 0.544658\n",
      "epoch 37; iter: 1200; batch classifier loss: 1.290575; batch adversarial loss: 0.488096\n",
      "epoch 37; iter: 1400; batch classifier loss: 1.240188; batch adversarial loss: 0.385639\n",
      "epoch 37; iter: 1600; batch classifier loss: 1.052969; batch adversarial loss: 0.442409\n",
      "epoch 37; iter: 1800; batch classifier loss: 1.901358; batch adversarial loss: 0.436937\n",
      "epoch 37; iter: 2000; batch classifier loss: 1.260233; batch adversarial loss: 0.439017\n",
      "epoch 37; iter: 2200; batch classifier loss: 1.872439; batch adversarial loss: 0.529235\n",
      "epoch 37; iter: 2400; batch classifier loss: 1.045277; batch adversarial loss: 0.453786\n",
      "epoch 37; iter: 2600; batch classifier loss: 1.325738; batch adversarial loss: 0.517579\n",
      "epoch 38; iter: 0; batch classifier loss: 1.991395; batch adversarial loss: 0.529121\n",
      "epoch 38; iter: 200; batch classifier loss: 1.368835; batch adversarial loss: 0.448338\n",
      "epoch 38; iter: 400; batch classifier loss: 1.826852; batch adversarial loss: 0.396762\n",
      "epoch 38; iter: 600; batch classifier loss: 1.544229; batch adversarial loss: 0.461079\n",
      "epoch 38; iter: 800; batch classifier loss: 1.665891; batch adversarial loss: 0.420112\n",
      "epoch 38; iter: 1000; batch classifier loss: 1.915989; batch adversarial loss: 0.430128\n",
      "epoch 38; iter: 1200; batch classifier loss: 2.123054; batch adversarial loss: 0.491040\n",
      "epoch 38; iter: 1400; batch classifier loss: 1.726151; batch adversarial loss: 0.484554\n",
      "epoch 38; iter: 1600; batch classifier loss: 1.872894; batch adversarial loss: 0.475777\n",
      "epoch 38; iter: 1800; batch classifier loss: 1.448327; batch adversarial loss: 0.529314\n",
      "epoch 38; iter: 2000; batch classifier loss: 1.716314; batch adversarial loss: 0.516710\n",
      "epoch 38; iter: 2200; batch classifier loss: 1.667381; batch adversarial loss: 0.442265\n",
      "epoch 38; iter: 2400; batch classifier loss: 1.099588; batch adversarial loss: 0.559794\n",
      "epoch 38; iter: 2600; batch classifier loss: 1.391358; batch adversarial loss: 0.414653\n",
      "epoch 39; iter: 0; batch classifier loss: 1.803371; batch adversarial loss: 0.379730\n",
      "epoch 39; iter: 200; batch classifier loss: 1.569275; batch adversarial loss: 0.542016\n",
      "epoch 39; iter: 400; batch classifier loss: 1.660015; batch adversarial loss: 0.524556\n",
      "epoch 39; iter: 600; batch classifier loss: 1.689050; batch adversarial loss: 0.430306\n",
      "epoch 39; iter: 800; batch classifier loss: 1.632093; batch adversarial loss: 0.471801\n",
      "epoch 39; iter: 1000; batch classifier loss: 1.009467; batch adversarial loss: 0.497832\n",
      "epoch 39; iter: 1200; batch classifier loss: 1.997229; batch adversarial loss: 0.503437\n",
      "epoch 39; iter: 1400; batch classifier loss: 1.283861; batch adversarial loss: 0.542636\n",
      "epoch 39; iter: 1600; batch classifier loss: 1.932070; batch adversarial loss: 0.448069\n",
      "epoch 39; iter: 1800; batch classifier loss: 1.206700; batch adversarial loss: 0.493172\n",
      "epoch 39; iter: 2000; batch classifier loss: 1.478062; batch adversarial loss: 0.362272\n",
      "epoch 39; iter: 2200; batch classifier loss: 0.981926; batch adversarial loss: 0.526771\n",
      "epoch 39; iter: 2400; batch classifier loss: 1.796834; batch adversarial loss: 0.522720\n",
      "epoch 39; iter: 2600; batch classifier loss: 1.892357; batch adversarial loss: 0.401657\n",
      "epoch 40; iter: 0; batch classifier loss: 1.382350; batch adversarial loss: 0.451739\n",
      "epoch 40; iter: 200; batch classifier loss: 1.688201; batch adversarial loss: 0.474600\n",
      "epoch 40; iter: 400; batch classifier loss: 1.570953; batch adversarial loss: 0.512362\n",
      "epoch 40; iter: 600; batch classifier loss: 1.469074; batch adversarial loss: 0.427566\n",
      "epoch 40; iter: 800; batch classifier loss: 1.275319; batch adversarial loss: 0.430591\n",
      "epoch 40; iter: 1000; batch classifier loss: 1.290989; batch adversarial loss: 0.457014\n",
      "epoch 40; iter: 1200; batch classifier loss: 1.867138; batch adversarial loss: 0.428079\n",
      "epoch 40; iter: 1400; batch classifier loss: 1.618434; batch adversarial loss: 0.490406\n",
      "epoch 40; iter: 1600; batch classifier loss: 1.226886; batch adversarial loss: 0.433410\n",
      "epoch 40; iter: 1800; batch classifier loss: 1.441594; batch adversarial loss: 0.460998\n",
      "epoch 40; iter: 2000; batch classifier loss: 2.041336; batch adversarial loss: 0.496121\n",
      "epoch 40; iter: 2200; batch classifier loss: 1.735463; batch adversarial loss: 0.492942\n",
      "epoch 40; iter: 2400; batch classifier loss: 1.447840; batch adversarial loss: 0.538669\n",
      "epoch 40; iter: 2600; batch classifier loss: 1.613610; batch adversarial loss: 0.427008\n",
      "epoch 41; iter: 0; batch classifier loss: 1.583235; batch adversarial loss: 0.483866\n",
      "epoch 41; iter: 200; batch classifier loss: 1.072779; batch adversarial loss: 0.371667\n",
      "epoch 41; iter: 400; batch classifier loss: 1.170090; batch adversarial loss: 0.477955\n",
      "epoch 41; iter: 600; batch classifier loss: 1.535811; batch adversarial loss: 0.520343\n",
      "epoch 41; iter: 800; batch classifier loss: 1.313810; batch adversarial loss: 0.448589\n",
      "epoch 41; iter: 1000; batch classifier loss: 1.760352; batch adversarial loss: 0.464142\n",
      "epoch 41; iter: 1200; batch classifier loss: 1.463651; batch adversarial loss: 0.474400\n",
      "epoch 41; iter: 1400; batch classifier loss: 1.649431; batch adversarial loss: 0.453392\n",
      "epoch 41; iter: 1600; batch classifier loss: 1.848652; batch adversarial loss: 0.500160\n",
      "epoch 41; iter: 1800; batch classifier loss: 1.729706; batch adversarial loss: 0.535335\n",
      "epoch 41; iter: 2000; batch classifier loss: 1.141474; batch adversarial loss: 0.443799\n",
      "epoch 41; iter: 2200; batch classifier loss: 1.425878; batch adversarial loss: 0.504357\n",
      "epoch 41; iter: 2400; batch classifier loss: 1.518797; batch adversarial loss: 0.393401\n",
      "epoch 41; iter: 2600; batch classifier loss: 1.247976; batch adversarial loss: 0.565022\n",
      "epoch 42; iter: 0; batch classifier loss: 1.648061; batch adversarial loss: 0.434824\n",
      "epoch 42; iter: 200; batch classifier loss: 1.651228; batch adversarial loss: 0.393285\n",
      "epoch 42; iter: 400; batch classifier loss: 2.293991; batch adversarial loss: 0.488167\n",
      "epoch 42; iter: 600; batch classifier loss: 1.453444; batch adversarial loss: 0.475673\n",
      "epoch 42; iter: 800; batch classifier loss: 1.754990; batch adversarial loss: 0.477746\n",
      "epoch 42; iter: 1000; batch classifier loss: 1.490997; batch adversarial loss: 0.407635\n",
      "epoch 42; iter: 1200; batch classifier loss: 1.574691; batch adversarial loss: 0.565743\n",
      "epoch 42; iter: 1400; batch classifier loss: 1.405166; batch adversarial loss: 0.506164\n",
      "epoch 42; iter: 1600; batch classifier loss: 1.869743; batch adversarial loss: 0.413199\n",
      "epoch 42; iter: 1800; batch classifier loss: 1.488275; batch adversarial loss: 0.439222\n",
      "epoch 42; iter: 2000; batch classifier loss: 1.745875; batch adversarial loss: 0.472752\n",
      "epoch 42; iter: 2200; batch classifier loss: 1.942175; batch adversarial loss: 0.481651\n",
      "epoch 42; iter: 2400; batch classifier loss: 2.055112; batch adversarial loss: 0.415835\n",
      "epoch 42; iter: 2600; batch classifier loss: 1.301717; batch adversarial loss: 0.361330\n",
      "epoch 43; iter: 0; batch classifier loss: 1.485082; batch adversarial loss: 0.387019\n",
      "epoch 43; iter: 200; batch classifier loss: 1.727918; batch adversarial loss: 0.381500\n",
      "epoch 43; iter: 400; batch classifier loss: 1.709665; batch adversarial loss: 0.600787\n",
      "epoch 43; iter: 600; batch classifier loss: 1.345392; batch adversarial loss: 0.425880\n",
      "epoch 43; iter: 800; batch classifier loss: 1.473341; batch adversarial loss: 0.386980\n",
      "epoch 43; iter: 1000; batch classifier loss: 1.201296; batch adversarial loss: 0.478548\n",
      "epoch 43; iter: 1200; batch classifier loss: 1.340554; batch adversarial loss: 0.343131\n",
      "epoch 43; iter: 1400; batch classifier loss: 2.237302; batch adversarial loss: 0.447781\n",
      "epoch 43; iter: 1600; batch classifier loss: 2.028984; batch adversarial loss: 0.458995\n",
      "epoch 43; iter: 1800; batch classifier loss: 1.776718; batch adversarial loss: 0.533499\n",
      "epoch 43; iter: 2000; batch classifier loss: 1.369068; batch adversarial loss: 0.401680\n",
      "epoch 43; iter: 2200; batch classifier loss: 1.875175; batch adversarial loss: 0.534449\n",
      "epoch 43; iter: 2400; batch classifier loss: 1.799227; batch adversarial loss: 0.410241\n",
      "epoch 43; iter: 2600; batch classifier loss: 1.373947; batch adversarial loss: 0.444412\n",
      "epoch 44; iter: 0; batch classifier loss: 1.733954; batch adversarial loss: 0.429252\n",
      "epoch 44; iter: 200; batch classifier loss: 1.244759; batch adversarial loss: 0.479591\n",
      "epoch 44; iter: 400; batch classifier loss: 1.551647; batch adversarial loss: 0.499370\n",
      "epoch 44; iter: 600; batch classifier loss: 2.028080; batch adversarial loss: 0.464268\n",
      "epoch 44; iter: 800; batch classifier loss: 1.566286; batch adversarial loss: 0.541641\n",
      "epoch 44; iter: 1000; batch classifier loss: 1.276171; batch adversarial loss: 0.487297\n",
      "epoch 44; iter: 1200; batch classifier loss: 1.822802; batch adversarial loss: 0.530954\n",
      "epoch 44; iter: 1400; batch classifier loss: 1.609606; batch adversarial loss: 0.498809\n",
      "epoch 44; iter: 1600; batch classifier loss: 2.319658; batch adversarial loss: 0.510880\n",
      "epoch 44; iter: 1800; batch classifier loss: 1.239847; batch adversarial loss: 0.481666\n",
      "epoch 44; iter: 2000; batch classifier loss: 1.416256; batch adversarial loss: 0.375692\n",
      "epoch 44; iter: 2200; batch classifier loss: 1.783852; batch adversarial loss: 0.416892\n",
      "epoch 44; iter: 2400; batch classifier loss: 1.462671; batch adversarial loss: 0.567604\n",
      "epoch 44; iter: 2600; batch classifier loss: 1.445331; batch adversarial loss: 0.430138\n",
      "epoch 45; iter: 0; batch classifier loss: 1.819520; batch adversarial loss: 0.447695\n",
      "epoch 45; iter: 200; batch classifier loss: 1.117601; batch adversarial loss: 0.420063\n",
      "epoch 45; iter: 400; batch classifier loss: 1.451133; batch adversarial loss: 0.466166\n",
      "epoch 45; iter: 600; batch classifier loss: 1.521098; batch adversarial loss: 0.397945\n",
      "epoch 45; iter: 800; batch classifier loss: 1.536332; batch adversarial loss: 0.458573\n",
      "epoch 45; iter: 1000; batch classifier loss: 1.335113; batch adversarial loss: 0.448878\n",
      "epoch 45; iter: 1200; batch classifier loss: 1.434359; batch adversarial loss: 0.420914\n",
      "epoch 45; iter: 1400; batch classifier loss: 1.717352; batch adversarial loss: 0.543611\n",
      "epoch 45; iter: 1600; batch classifier loss: 1.398149; batch adversarial loss: 0.356363\n",
      "epoch 45; iter: 1800; batch classifier loss: 1.635379; batch adversarial loss: 0.421616\n",
      "epoch 45; iter: 2000; batch classifier loss: 1.450172; batch adversarial loss: 0.443078\n",
      "epoch 45; iter: 2200; batch classifier loss: 2.387559; batch adversarial loss: 0.488582\n",
      "epoch 45; iter: 2400; batch classifier loss: 1.354237; batch adversarial loss: 0.571724\n",
      "epoch 45; iter: 2600; batch classifier loss: 1.900891; batch adversarial loss: 0.432344\n",
      "epoch 46; iter: 0; batch classifier loss: 1.591107; batch adversarial loss: 0.548998\n",
      "epoch 46; iter: 200; batch classifier loss: 1.952212; batch adversarial loss: 0.469390\n",
      "epoch 46; iter: 400; batch classifier loss: 2.092227; batch adversarial loss: 0.509721\n",
      "epoch 46; iter: 600; batch classifier loss: 1.253368; batch adversarial loss: 0.430844\n",
      "epoch 46; iter: 800; batch classifier loss: 1.873123; batch adversarial loss: 0.508574\n",
      "epoch 46; iter: 1000; batch classifier loss: 1.538142; batch adversarial loss: 0.561629\n",
      "epoch 46; iter: 1200; batch classifier loss: 2.293234; batch adversarial loss: 0.491578\n",
      "epoch 46; iter: 1400; batch classifier loss: 1.545455; batch adversarial loss: 0.380589\n",
      "epoch 46; iter: 1600; batch classifier loss: 1.399846; batch adversarial loss: 0.490443\n",
      "epoch 46; iter: 1800; batch classifier loss: 1.704108; batch adversarial loss: 0.443960\n",
      "epoch 46; iter: 2000; batch classifier loss: 1.750791; batch adversarial loss: 0.431805\n",
      "epoch 46; iter: 2200; batch classifier loss: 1.340896; batch adversarial loss: 0.367400\n",
      "epoch 46; iter: 2400; batch classifier loss: 1.827742; batch adversarial loss: 0.449071\n",
      "epoch 46; iter: 2600; batch classifier loss: 1.519478; batch adversarial loss: 0.487088\n",
      "epoch 47; iter: 0; batch classifier loss: 1.647347; batch adversarial loss: 0.466264\n",
      "epoch 47; iter: 200; batch classifier loss: 1.920765; batch adversarial loss: 0.418448\n",
      "epoch 47; iter: 400; batch classifier loss: 1.665910; batch adversarial loss: 0.417636\n",
      "epoch 47; iter: 600; batch classifier loss: 1.318966; batch adversarial loss: 0.416419\n",
      "epoch 47; iter: 800; batch classifier loss: 1.455053; batch adversarial loss: 0.442885\n",
      "epoch 47; iter: 1000; batch classifier loss: 1.792799; batch adversarial loss: 0.436128\n",
      "epoch 47; iter: 1200; batch classifier loss: 1.576626; batch adversarial loss: 0.426401\n",
      "epoch 47; iter: 1400; batch classifier loss: 1.620684; batch adversarial loss: 0.449155\n",
      "epoch 47; iter: 1600; batch classifier loss: 1.255928; batch adversarial loss: 0.559284\n",
      "epoch 47; iter: 1800; batch classifier loss: 1.043599; batch adversarial loss: 0.409822\n",
      "epoch 47; iter: 2000; batch classifier loss: 1.258458; batch adversarial loss: 0.544281\n",
      "epoch 47; iter: 2200; batch classifier loss: 1.064889; batch adversarial loss: 0.465145\n",
      "epoch 47; iter: 2400; batch classifier loss: 1.459593; batch adversarial loss: 0.429987\n",
      "epoch 47; iter: 2600; batch classifier loss: 1.790278; batch adversarial loss: 0.444860\n",
      "epoch 48; iter: 0; batch classifier loss: 1.108139; batch adversarial loss: 0.439717\n",
      "epoch 48; iter: 200; batch classifier loss: 1.360143; batch adversarial loss: 0.468727\n",
      "epoch 48; iter: 400; batch classifier loss: 1.877977; batch adversarial loss: 0.510736\n",
      "epoch 48; iter: 600; batch classifier loss: 1.566290; batch adversarial loss: 0.514724\n",
      "epoch 48; iter: 800; batch classifier loss: 1.761033; batch adversarial loss: 0.547641\n",
      "epoch 48; iter: 1000; batch classifier loss: 1.901676; batch adversarial loss: 0.454413\n",
      "epoch 48; iter: 1200; batch classifier loss: 1.418574; batch adversarial loss: 0.476582\n",
      "epoch 48; iter: 1400; batch classifier loss: 1.386786; batch adversarial loss: 0.387803\n",
      "epoch 48; iter: 1600; batch classifier loss: 1.167541; batch adversarial loss: 0.363402\n",
      "epoch 48; iter: 1800; batch classifier loss: 1.165789; batch adversarial loss: 0.464384\n",
      "epoch 48; iter: 2000; batch classifier loss: 1.415556; batch adversarial loss: 0.501493\n",
      "epoch 48; iter: 2200; batch classifier loss: 1.384976; batch adversarial loss: 0.421398\n",
      "epoch 48; iter: 2400; batch classifier loss: 1.838133; batch adversarial loss: 0.394078\n",
      "epoch 48; iter: 2600; batch classifier loss: 1.747313; batch adversarial loss: 0.443956\n",
      "epoch 49; iter: 0; batch classifier loss: 1.846705; batch adversarial loss: 0.491737\n",
      "epoch 49; iter: 200; batch classifier loss: 1.289726; batch adversarial loss: 0.531154\n",
      "epoch 49; iter: 400; batch classifier loss: 1.305390; batch adversarial loss: 0.437896\n",
      "epoch 49; iter: 600; batch classifier loss: 1.770955; batch adversarial loss: 0.484480\n",
      "epoch 49; iter: 800; batch classifier loss: 1.602679; batch adversarial loss: 0.440393\n",
      "epoch 49; iter: 1000; batch classifier loss: 1.444276; batch adversarial loss: 0.430744\n",
      "epoch 49; iter: 1200; batch classifier loss: 1.636036; batch adversarial loss: 0.458731\n",
      "epoch 49; iter: 1400; batch classifier loss: 1.469179; batch adversarial loss: 0.521973\n",
      "epoch 49; iter: 1600; batch classifier loss: 2.117997; batch adversarial loss: 0.501054\n",
      "epoch 49; iter: 1800; batch classifier loss: 1.229195; batch adversarial loss: 0.471842\n",
      "epoch 49; iter: 2000; batch classifier loss: 1.475695; batch adversarial loss: 0.507688\n",
      "epoch 49; iter: 2200; batch classifier loss: 1.437574; batch adversarial loss: 0.496209\n",
      "epoch 49; iter: 2400; batch classifier loss: 1.764649; batch adversarial loss: 0.509573\n",
      "epoch 49; iter: 2600; batch classifier loss: 1.542951; batch adversarial loss: 0.503493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<aif360.algorithms.inprocessing.adversarial_debiasing.AdversarialDebiasing at 0x7fc3e3fd8f50>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debiased_model_ad.fit(train_pp_bld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the adversial model to test data\n",
    "dataset_debiasing_train_ad = debiased_model_ad.predict(train_pp_bld)\n",
    "dataset_debiasing_test_ad = debiased_model_ad.predict(test_pp_bld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics comparisons before and after adversial debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - dataset metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.322358\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.322358\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Model - with debiasing - dataset metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.540159\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.539525\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.742596\n",
      "Test set: Balanced classification accuracy = 0.617089\n",
      "Test set: Disparate impact = 0.361564\n",
      "Test set: Equal opportunity difference = -0.518308\n",
      "Test set: Average odds difference = -0.612265\n",
      "Test set: Theil_index = 0.104408\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Model - with debiasing - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.698420\n",
      "Test set: Balanced classification accuracy = 0.604537\n",
      "Test set: Disparate impact = 0.391340\n",
      "Test set: Equal opportunity difference = -0.465564\n",
      "Test set: Average odds difference = -0.518350\n",
      "Test set: Theil_index = 0.173040\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the dataset from plain model (without debiasing)\n",
    "display(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_test.mean_difference())\n",
    "\n",
    "# Metrics for the dataset from model with debiasing\n",
    "display(Markdown(\"#### Model - with debiasing - dataset metrics\"))\n",
    "metric_dataset_debiasing_train_ad = BinaryLabelDatasetMetric(dataset_debiasing_train_ad, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_train_ad.mean_difference())\n",
    "\n",
    "metric_dataset_debiasing_test_ad = BinaryLabelDatasetMetric(dataset_debiasing_test_ad, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_test_ad.mean_difference())\n",
    "\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
    "classified_metric_nodebiasing_test_ad = ClassificationMetric(test_pp_bld, \n",
    "                                                 dataset_nodebiasing_test_ad,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test_ad.accuracy())\n",
    "TPR_ad = classified_metric_nodebiasing_test_ad.true_positive_rate()\n",
    "TNR_ad = classified_metric_nodebiasing_test_ad.true_negative_rate()\n",
    "bal_acc_nodebiasing_test_ad = 0.5*(TPR_ad+TNR_ad)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test_ad)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test_ad.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test_ad.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test_ad.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test_ad.theil_index())\n",
    "\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Model - with debiasing - classification metrics\"))\n",
    "classified_metric_debiasing_test_ad = ClassificationMetric(test_pp_bld, \n",
    "                                                 dataset_debiasing_test_ad,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test_ad.accuracy())\n",
    "TPR_dad = classified_metric_debiasing_test_ad.true_positive_rate()\n",
    "TNR_dad = classified_metric_debiasing_test_ad.true_negative_rate()\n",
    "bal_acc_debiasing_test_dad = 0.5*(TPR_dad+TNR_dad)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test_dad)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test_ad.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test_ad.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test_ad.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test_ad.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment 2: Based on the metrics score for dataset and classification, we can say that adversial debiasing will not reduce biasness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Meta Fair Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm without debiasing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biased_model = MetaFairClassifier(tau=0, sensitive_attr=\"age\", type=\"fdr\").fit(train_pp_bld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_bias_test_mf = biased_model.predict(test_pp_bld)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Markdown(\"#### Model - without Meta Fair debiasing - classification metrics\"))\n",
    "# classified_metric_bias_test_mf = ClassificationMetric(test_pp_bld, dataset_bias_test_mf,\n",
    "#                                                    unprivileged_groups=unprivileged_groups,\n",
    "#                                                    privileged_groups=privileged_groups)\n",
    "# print(\"Test set: Classification accuracy = {:.3f}\".format(classified_metric_bias_test_mf.accuracy()))\n",
    "# TPR_mf = classified_metric_bias_test_mf.true_positive_rate()\n",
    "# TNR_mf = classified_metric_bias_test_mf.true_negative_rate()\n",
    "# bal_acc_bias_test_mf = 0.5*(TPR_mf+TNR_mf)\n",
    "# print(\"Test set: Balanced classification accuracy = {:.3f}\".format(bal_acc_bias_test_mf))\n",
    "# print(\"Test set: Disparate impact = {:.3f}\".format(classified_metric_bias_test_mf.disparate_impact()))\n",
    "# fdr_mf = classified_metric_bias_test_mf.false_discovery_rate_ratio()\n",
    "# fdr_mf = min(fdr_mf, 1/fdr_mf)\n",
    "# print(\"Test set: False discovery rate ratio = {:.3f}\".format(fdr_mf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn debiased meta classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debiased_model_mf = MetaFairClassifier(tau = 0.7, sensitive_attr=\"age\", type=\"fdr\").fit(train_pp_bld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prejudice Remover\n",
    "Prejudice remover is an in-processing technique that adds a discrimination-aware regularization term to the learning objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/users/ssultana/anaconda3/envs/aif360/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/data2/users/ssultana/anaconda3/envs/aif360/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "# scale_transf_pr = StandardScaler()\n",
    "# train_pp_bld_sc = scale_transf_pr.fit_transform(train_pp_bld)\n",
    "model_pr = PrejudiceRemover(sensitive_attr= 'age', eta=25.0)\n",
    "pr_orig_scaler = StandardScaler()\n",
    "\n",
    "dataset = train_pp_bld.copy()\n",
    "dataset.features = pr_orig_scaler.fit_transform(dataset.features)\n",
    "\n",
    "debiased_model_pr = model_pr.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validating PR model\n",
    "dataset_debias_test_pr = debiased_model_pr.predict(test_pp_bld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.725\n",
      "Test set: Balanced classification accuracy = 0.627701\n",
      "Test set: Disparate impact = 0.000000\n",
      "Test set: Equal opportunity difference = -1.000000\n",
      "Test set: Average odds difference = -1.000000\n",
      "Test set: Theil_index = 0.147542\n"
     ]
    }
   ],
   "source": [
    "classified_metric_debiasing_test_pr = ClassificationMetric(test_pp_bld, dataset_debias_test_pr,\n",
    "                                                   unprivileged_groups=unprivileged_groups,\n",
    "                                                   privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = {:.3f}\".format(classified_metric_debiasing_test_pr.accuracy()))\n",
    "TPR_pr = classified_metric_debiasing_test_pr.true_positive_rate()\n",
    "TNR_pr = classified_metric_debiasing_test_pr.true_negative_rate()\n",
    "bal_acc_debiasing_test_pr = 0.5*(TPR_pr+TNR_pr)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test_pr)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test_pr.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test_pr.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test_pr.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test_pr.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exponentiated Gradient Reduction\n",
    "\n",
    "Exponentiated gradient reduction is an in-processing technique that reduces fair classification to a sequence of cost-sensitive classification problems, returning a randomized classifier with the lowest empirical error subject to fair classification constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LogisticRegression(solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) #need for reproducibility\n",
    "exp_grad_red = ExponentiatedGradientReduction(estimator=estimator, \n",
    "                                              constraints=\"EqualizedOdds\",\n",
    "                                              drop_prot_attr=False)\n",
    "exp_grad_red.fit(train_pp_bld)\n",
    "exp_grad_red_pred = exp_grad_red.predict(test_pp_bld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Accuracy for Exponentiated Gradient Reduction"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7040293453724605\n"
     ]
    }
   ],
   "source": [
    "metric_test_egr = ClassificationMetric(test_pp_bld, \n",
    "                                    exp_grad_red_pred,\n",
    "                                    unprivileged_groups=unprivileged_groups,\n",
    "                                    privileged_groups=privileged_groups)\n",
    "\n",
    "display(Markdown(\"#### Accuracy for Exponentiated Gradient Reduction\"))\n",
    "egr_acc = metric_test_egr.accuracy()\n",
    "print(egr_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Processing Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Odds equalizing post-processing algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_train_pred = train_pp_bld.copy(deepcopy=True)\n",
    "# cost constraint of fnr will optimize generalized false negative rates, that of\n",
    "# fpr will optimize generalized false positive rates, and weighted will optimize\n",
    "# a weighted combination of both\n",
    "cost_constraint = \"fnr\" # \"fnr\", \"fpr\", \"weighted\"\n",
    "#random seed for calibrated equal odds prediction\n",
    "randseed = 12345679 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Learn parameters to equalize odds and apply to create a new dataset\n",
    "cpp = CalibratedEqOddsPostprocessing(privileged_groups = privileged_groups,\n",
    "                                     unprivileged_groups = unprivileged_groups,\n",
    "                                     cost_constraint=cost_constraint,\n",
    "                                     seed=randseed)\n",
    "cpp = cpp.fit(train_pp_bld, dataset_orig_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform test data using the post processing algorithm\n",
    "dataset_orig_test_pred = test_pp_bld.copy(deepcopy=True)\n",
    "dataset_transf_test_pred = cpp.predict(dataset_orig_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Transformed testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in GFPR between unprivileged and privileged groups\n",
      "0.0\n",
      "Difference in GFNR between unprivileged and privileged groups\n",
      "0.0\n",
      "Test set: Classification accuracy of Calibrated Equality of Odds = 0.992\n"
     ]
    }
   ],
   "source": [
    "\"\"\" GFP: Return the generalized number of false positives, :math:`GFP`, the\n",
    "        weighted sum of predicted scores where true labels are 'favorable',\n",
    "        optionally conditioned on protected attributes.\"\"\"\n",
    "cm_transf_test = ClassificationMetric(test_pp_bld, dataset_transf_test_pred,\n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Transformed testing dataset\"))\n",
    "print(\"Difference in GFPR between unprivileged and privileged groups\")\n",
    "print(cm_transf_test.difference(cm_transf_test.generalized_false_positive_rate))\n",
    "print(\"Difference in GFNR between unprivileged and privileged groups\")\n",
    "print(cm_transf_test.difference(cm_transf_test.generalized_false_negative_rate))\n",
    "print(\"Test set: Classification accuracy of Calibrated Equality of Odds = {:.3f}\".format(cm_transf_test.accuracy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reject Option Classification: \n",
    "Reject option classification is a postprocessing technique that gives favorable outcomes to unpriviliged groups and unfavorable outcomes to priviliged groups in a confidence band around the decision boundary with the highest uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric used (should be one of allowed_metrics)\n",
    "metric_name = \"Statistical parity difference\"\n",
    "\n",
    "# Upper and lower bound on the fairness metric used\n",
    "metric_ub = 0.05\n",
    "metric_lb = -0.05\n",
    "        \n",
    "#random seed for calibrated equal odds prediction\n",
    "np.random.seed(1)\n",
    "\n",
    "# Verify metric name\n",
    "allowed_metrics = [\"Statistical parity difference\",\n",
    "                   \"Average odds difference\",\n",
    "                   \"Equal opportunity difference\"]\n",
    "if metric_name not in allowed_metrics:\n",
    "    raise ValueError(\"Metric name should be one of allowed metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
    "                                 privileged_groups=privileged_groups, \n",
    "                                 low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                  num_class_thresh=100, num_ROC_margin=50,\n",
    "                                  metric_name=metric_name,\n",
    "                                  metric_ub=metric_ub, metric_lb=metric_lb)\n",
    "ROC = ROC.fit(train_pp_bld, dataset_orig_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal classification threshold (with fairness constraints) = 0.0100\n",
      "Optimal ROC margin = 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal classification threshold (with fairness constraints) = %.4f\" % ROC.classification_threshold)\n",
    "print(\"Optimal ROC margin = %.4f\" % ROC.ROC_margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('aif360')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a795b581f176658b1d8a94910201b57c09f6b86b50e4f173a00bd65fdbbd33d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
